{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "H3S-oU98oyWT"
      },
      "outputs": [],
      "source": [
        "# --- Data Loading ---\n",
        "import gzip\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "def load_data():\n",
        "    f = gzip.open('/content/mnist.pkl.gz', 'rb')  # adjust path if necessary\n",
        "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
        "    f.close()\n",
        "    return training_data, validation_data, test_data\n",
        "\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = list(zip(training_inputs, training_results))\n",
        "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
        "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
        "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
        "    test_data = list(zip(test_inputs, te_d[1]))\n",
        "    return training_data, validation_data, test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# --- Cost Function ---\n",
        "class CrossEntropyCost:\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a) - (1-y)*np.log(1-a)))\n",
        "\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a - y)\n",
        "\n",
        "\n",
        "# --- Network Class ---\n",
        "class Network:\n",
        "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.default_weight_initializer()\n",
        "        self.cost = cost\n",
        "\n",
        "    def large_weight_initializer(self):\n",
        "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def default_weight_initializer(self):\n",
        "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) / np.sqrt(x)\n",
        "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            lmbda=0.0,\n",
        "            evaluation_data=None,\n",
        "            monitor_evaluation_cost=False,\n",
        "            monitor_evaluation_accuracy=False,\n",
        "            monitor_training_cost=False,\n",
        "            monitor_training_accuracy=False):\n",
        "\n",
        "        if evaluation_data: n_data = len(evaluation_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        evaluation_cost, evaluation_accuracy = [], []\n",
        "        training_cost, training_accuracy = [], []\n",
        "\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta, lmbda, n)\n",
        "\n",
        "            print(f\"Epoch {j} training complete\")\n",
        "\n",
        "            if monitor_training_cost:\n",
        "                cost = self.total_cost(training_data, lmbda)\n",
        "                training_cost.append(cost)\n",
        "                print(f\"Cost on training data: {cost}\")\n",
        "\n",
        "            if monitor_training_accuracy:\n",
        "                accuracy = self.accuracy(training_data, convert=True)\n",
        "                training_accuracy.append(accuracy)\n",
        "                print(f\"Accuracy on training data: {accuracy} / {n}\")\n",
        "\n",
        "            if monitor_evaluation_cost:\n",
        "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                evaluation_cost.append(cost)\n",
        "                print(f\"Cost on evaluation data: {cost}\")\n",
        "\n",
        "            if monitor_evaluation_accuracy:\n",
        "                accuracy = self.accuracy(evaluation_data)\n",
        "                evaluation_accuracy.append(accuracy)\n",
        "                print(f\"Accuracy on evaluation data: {accuracy} / {n_data}\")\n",
        "\n",
        "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "\n",
        "        self.weights = [(1 - eta * (lmbda / n)) * w - (eta / len(mini_batch)) * nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b - (eta / len(mini_batch)) * nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        delta = self.cost.delta(zs[-1], activations[-1], y)\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].T)\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].T, delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].T)\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def accuracy(self, data, convert=False):\n",
        "        if convert:\n",
        "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
        "        else:\n",
        "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
        "        return sum(int(x == y) for (x, y) in results)\n",
        "\n",
        "    def total_cost(self, data, lmbda, convert=False):\n",
        "        cost = 0.0\n",
        "        for x, y in data:\n",
        "            a = self.feedforward(x)\n",
        "            if convert: y = vectorized_result(y)\n",
        "            cost += self.cost.fn(a, y) / len(data)\n",
        "        cost += 0.5 * (lmbda / len(data)) * sum(np.linalg.norm(w)**2 for w in self.weights)\n",
        "        return cost\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n"
      ],
      "metadata": {
        "id": "b0kFgZRJpNrX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3: Overfitting on Small Dataset\n"
      ],
      "metadata": {
        "id": "eV-bOqzAqysJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load data\n",
        "training_data, validation_data, test_data = load_data_wrapper()\n",
        "\n",
        "# Initialize network with 30 hidden neurons\n",
        "net = Network([784, 30, 10], cost=CrossEntropyCost)\n",
        "net.large_weight_initializer()\n",
        "\n",
        "# Train on first 1000 examples\n",
        "net.SGD(training_data[:1000], 400, 10, 0.5,\n",
        "        evaluation_data=test_data,\n",
        "        monitor_evaluation_accuracy=True,\n",
        "        monitor_training_cost=True,\n",
        "        monitor_training_accuracy=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klLEz7Nqqff1",
        "outputId": "aacdea3e-54e4-440f-9d58-36b5f2ff7671"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 1.9161820362261717\n",
            "Accuracy on training data: 651 / 1000\n",
            "Accuracy on evaluation data: 5259 / 10000\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 1.451017926394901\n",
            "Accuracy on training data: 755 / 1000\n",
            "Accuracy on evaluation data: 6368 / 10000\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 1.173382667331476\n",
            "Accuracy on training data: 825 / 1000\n",
            "Accuracy on evaluation data: 7097 / 10000\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 0.9988949276506093\n",
            "Accuracy on training data: 864 / 1000\n",
            "Accuracy on evaluation data: 7273 / 10000\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 0.8567673403871416\n",
            "Accuracy on training data: 877 / 1000\n",
            "Accuracy on evaluation data: 7472 / 10000\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 0.7437641726265786\n",
            "Accuracy on training data: 920 / 1000\n",
            "Accuracy on evaluation data: 7584 / 10000\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 0.6474265216331955\n",
            "Accuracy on training data: 929 / 1000\n",
            "Accuracy on evaluation data: 7739 / 10000\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 0.6034252638094514\n",
            "Accuracy on training data: 930 / 1000\n",
            "Accuracy on evaluation data: 7732 / 10000\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 0.5420420143341356\n",
            "Accuracy on training data: 939 / 1000\n",
            "Accuracy on evaluation data: 7744 / 10000\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 0.5053106439773256\n",
            "Accuracy on training data: 946 / 1000\n",
            "Accuracy on evaluation data: 7795 / 10000\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 0.44544690426495254\n",
            "Accuracy on training data: 957 / 1000\n",
            "Accuracy on evaluation data: 7908 / 10000\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 0.3977533304529599\n",
            "Accuracy on training data: 964 / 1000\n",
            "Accuracy on evaluation data: 7865 / 10000\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 0.3611016344250987\n",
            "Accuracy on training data: 973 / 1000\n",
            "Accuracy on evaluation data: 7903 / 10000\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 0.33495723476138306\n",
            "Accuracy on training data: 972 / 1000\n",
            "Accuracy on evaluation data: 7938 / 10000\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 0.3203821835952007\n",
            "Accuracy on training data: 977 / 1000\n",
            "Accuracy on evaluation data: 7957 / 10000\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 0.286216034795556\n",
            "Accuracy on training data: 977 / 1000\n",
            "Accuracy on evaluation data: 7944 / 10000\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 0.26957860539957895\n",
            "Accuracy on training data: 981 / 1000\n",
            "Accuracy on evaluation data: 7972 / 10000\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 0.25448293811085193\n",
            "Accuracy on training data: 982 / 1000\n",
            "Accuracy on evaluation data: 7971 / 10000\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 0.23128967538408807\n",
            "Accuracy on training data: 986 / 1000\n",
            "Accuracy on evaluation data: 7948 / 10000\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 0.216062446169818\n",
            "Accuracy on training data: 988 / 1000\n",
            "Accuracy on evaluation data: 8024 / 10000\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 0.20396481451441967\n",
            "Accuracy on training data: 988 / 1000\n",
            "Accuracy on evaluation data: 7989 / 10000\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 0.19153433380812124\n",
            "Accuracy on training data: 988 / 1000\n",
            "Accuracy on evaluation data: 8041 / 10000\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 0.17929305492574402\n",
            "Accuracy on training data: 991 / 1000\n",
            "Accuracy on evaluation data: 8004 / 10000\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 0.16682468797374292\n",
            "Accuracy on training data: 992 / 1000\n",
            "Accuracy on evaluation data: 8040 / 10000\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 0.15979874882952233\n",
            "Accuracy on training data: 992 / 1000\n",
            "Accuracy on evaluation data: 8035 / 10000\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 0.15321694722254678\n",
            "Accuracy on training data: 994 / 1000\n",
            "Accuracy on evaluation data: 8047 / 10000\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 0.14139142908310484\n",
            "Accuracy on training data: 993 / 1000\n",
            "Accuracy on evaluation data: 8073 / 10000\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 0.13466084565847827\n",
            "Accuracy on training data: 994 / 1000\n",
            "Accuracy on evaluation data: 8083 / 10000\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 0.12759118195112362\n",
            "Accuracy on training data: 995 / 1000\n",
            "Accuracy on evaluation data: 8085 / 10000\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 0.12385474661657996\n",
            "Accuracy on training data: 995 / 1000\n",
            "Accuracy on evaluation data: 8074 / 10000\n",
            "Epoch 30 training complete\n",
            "Cost on training data: 0.11726852484485097\n",
            "Accuracy on training data: 996 / 1000\n",
            "Accuracy on evaluation data: 8066 / 10000\n",
            "Epoch 31 training complete\n",
            "Cost on training data: 0.11246198787642238\n",
            "Accuracy on training data: 996 / 1000\n",
            "Accuracy on evaluation data: 8108 / 10000\n",
            "Epoch 32 training complete\n",
            "Cost on training data: 0.10742607937208339\n",
            "Accuracy on training data: 996 / 1000\n",
            "Accuracy on evaluation data: 8086 / 10000\n",
            "Epoch 33 training complete\n",
            "Cost on training data: 0.10193959998580965\n",
            "Accuracy on training data: 996 / 1000\n",
            "Accuracy on evaluation data: 8084 / 10000\n",
            "Epoch 34 training complete\n",
            "Cost on training data: 0.09669549541614512\n",
            "Accuracy on training data: 996 / 1000\n",
            "Accuracy on evaluation data: 8094 / 10000\n",
            "Epoch 35 training complete\n",
            "Cost on training data: 0.09298987083939546\n",
            "Accuracy on training data: 996 / 1000\n",
            "Accuracy on evaluation data: 8106 / 10000\n",
            "Epoch 36 training complete\n",
            "Cost on training data: 0.08943076801751881\n",
            "Accuracy on training data: 996 / 1000\n",
            "Accuracy on evaluation data: 8106 / 10000\n",
            "Epoch 37 training complete\n",
            "Cost on training data: 0.08635324676988973\n",
            "Accuracy on training data: 997 / 1000\n",
            "Accuracy on evaluation data: 8101 / 10000\n",
            "Epoch 38 training complete\n",
            "Cost on training data: 0.08702723097650616\n",
            "Accuracy on training data: 998 / 1000\n",
            "Accuracy on evaluation data: 8079 / 10000\n",
            "Epoch 39 training complete\n",
            "Cost on training data: 0.08116830908375858\n",
            "Accuracy on training data: 998 / 1000\n",
            "Accuracy on evaluation data: 8100 / 10000\n",
            "Epoch 40 training complete\n",
            "Cost on training data: 0.07841023188092598\n",
            "Accuracy on training data: 998 / 1000\n",
            "Accuracy on evaluation data: 8114 / 10000\n",
            "Epoch 41 training complete\n",
            "Cost on training data: 0.07598501925377833\n",
            "Accuracy on training data: 998 / 1000\n",
            "Accuracy on evaluation data: 8110 / 10000\n",
            "Epoch 42 training complete\n",
            "Cost on training data: 0.0736781415014485\n",
            "Accuracy on training data: 997 / 1000\n",
            "Accuracy on evaluation data: 8118 / 10000\n",
            "Epoch 43 training complete\n",
            "Cost on training data: 0.07133489162809774\n",
            "Accuracy on training data: 998 / 1000\n",
            "Accuracy on evaluation data: 8119 / 10000\n",
            "Epoch 44 training complete\n",
            "Cost on training data: 0.06936400605372806\n",
            "Accuracy on training data: 998 / 1000\n",
            "Accuracy on evaluation data: 8138 / 10000\n",
            "Epoch 45 training complete\n",
            "Cost on training data: 0.06748057582244696\n",
            "Accuracy on training data: 998 / 1000\n",
            "Accuracy on evaluation data: 8119 / 10000\n",
            "Epoch 46 training complete\n",
            "Cost on training data: 0.06498218171822621\n",
            "Accuracy on training data: 998 / 1000\n",
            "Accuracy on evaluation data: 8125 / 10000\n",
            "Epoch 47 training complete\n",
            "Cost on training data: 0.06381103004793708\n",
            "Accuracy on training data: 998 / 1000\n",
            "Accuracy on evaluation data: 8126 / 10000\n",
            "Epoch 48 training complete\n",
            "Cost on training data: 0.061479700233414813\n",
            "Accuracy on training data: 998 / 1000\n",
            "Accuracy on evaluation data: 8129 / 10000\n",
            "Epoch 49 training complete\n",
            "Cost on training data: 0.059444965925680986\n",
            "Accuracy on training data: 998 / 1000\n",
            "Accuracy on evaluation data: 8126 / 10000\n",
            "Epoch 50 training complete\n",
            "Cost on training data: 0.05793690867071746\n",
            "Accuracy on training data: 998 / 1000\n",
            "Accuracy on evaluation data: 8128 / 10000\n",
            "Epoch 51 training complete\n",
            "Cost on training data: 0.05648780824032729\n",
            "Accuracy on training data: 998 / 1000\n",
            "Accuracy on evaluation data: 8134 / 10000\n",
            "Epoch 52 training complete\n",
            "Cost on training data: 0.05536658528098888\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8124 / 10000\n",
            "Epoch 53 training complete\n",
            "Cost on training data: 0.054367236815501783\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8116 / 10000\n",
            "Epoch 54 training complete\n",
            "Cost on training data: 0.05224463600139193\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8146 / 10000\n",
            "Epoch 55 training complete\n",
            "Cost on training data: 0.05120196257745052\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8136 / 10000\n",
            "Epoch 56 training complete\n",
            "Cost on training data: 0.05033755096574551\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8153 / 10000\n",
            "Epoch 57 training complete\n",
            "Cost on training data: 0.048789811725183055\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8147 / 10000\n",
            "Epoch 58 training complete\n",
            "Cost on training data: 0.04761470787587565\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8160 / 10000\n",
            "Epoch 59 training complete\n",
            "Cost on training data: 0.04657019696190632\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8147 / 10000\n",
            "Epoch 60 training complete\n",
            "Cost on training data: 0.04557400780166889\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8159 / 10000\n",
            "Epoch 61 training complete\n",
            "Cost on training data: 0.04455286225363928\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8154 / 10000\n",
            "Epoch 62 training complete\n",
            "Cost on training data: 0.04357519756663017\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8154 / 10000\n",
            "Epoch 63 training complete\n",
            "Cost on training data: 0.042754651047054774\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8155 / 10000\n",
            "Epoch 64 training complete\n",
            "Cost on training data: 0.04192434935861382\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8157 / 10000\n",
            "Epoch 65 training complete\n",
            "Cost on training data: 0.041055442109601534\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8167 / 10000\n",
            "Epoch 66 training complete\n",
            "Cost on training data: 0.04018140964793058\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8155 / 10000\n",
            "Epoch 67 training complete\n",
            "Cost on training data: 0.03942070459396349\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8163 / 10000\n",
            "Epoch 68 training complete\n",
            "Cost on training data: 0.03856348934016692\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8164 / 10000\n",
            "Epoch 69 training complete\n",
            "Cost on training data: 0.037815633376224164\n",
            "Accuracy on training data: 999 / 1000\n",
            "Accuracy on evaluation data: 8166 / 10000\n",
            "Epoch 70 training complete\n",
            "Cost on training data: 0.03743754487411842\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8167 / 10000\n",
            "Epoch 71 training complete\n",
            "Cost on training data: 0.03670791287588563\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8172 / 10000\n",
            "Epoch 72 training complete\n",
            "Cost on training data: 0.03569591871955962\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8172 / 10000\n",
            "Epoch 73 training complete\n",
            "Cost on training data: 0.034972289255259165\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8171 / 10000\n",
            "Epoch 74 training complete\n",
            "Cost on training data: 0.03432560243805228\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8174 / 10000\n",
            "Epoch 75 training complete\n",
            "Cost on training data: 0.033801698807545376\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8183 / 10000\n",
            "Epoch 76 training complete\n",
            "Cost on training data: 0.03314390162344836\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8175 / 10000\n",
            "Epoch 77 training complete\n",
            "Cost on training data: 0.032633074619398954\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8176 / 10000\n",
            "Epoch 78 training complete\n",
            "Cost on training data: 0.03217431246927002\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8171 / 10000\n",
            "Epoch 79 training complete\n",
            "Cost on training data: 0.03157622740335076\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8181 / 10000\n",
            "Epoch 80 training complete\n",
            "Cost on training data: 0.03108791530769714\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8179 / 10000\n",
            "Epoch 81 training complete\n",
            "Cost on training data: 0.030546338335984694\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8175 / 10000\n",
            "Epoch 82 training complete\n",
            "Cost on training data: 0.030028164847316727\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8177 / 10000\n",
            "Epoch 83 training complete\n",
            "Cost on training data: 0.02955217994188909\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8181 / 10000\n",
            "Epoch 84 training complete\n",
            "Cost on training data: 0.029138726274794552\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8179 / 10000\n",
            "Epoch 85 training complete\n",
            "Cost on training data: 0.02867480169490551\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8178 / 10000\n",
            "Epoch 86 training complete\n",
            "Cost on training data: 0.028241424459614604\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8183 / 10000\n",
            "Epoch 87 training complete\n",
            "Cost on training data: 0.02779191792260762\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8178 / 10000\n",
            "Epoch 88 training complete\n",
            "Cost on training data: 0.027362086548414703\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8183 / 10000\n",
            "Epoch 89 training complete\n",
            "Cost on training data: 0.02697043765238485\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8184 / 10000\n",
            "Epoch 90 training complete\n",
            "Cost on training data: 0.026566059590364642\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8183 / 10000\n",
            "Epoch 91 training complete\n",
            "Cost on training data: 0.026194987649834356\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8185 / 10000\n",
            "Epoch 92 training complete\n",
            "Cost on training data: 0.02584493763103555\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8180 / 10000\n",
            "Epoch 93 training complete\n",
            "Cost on training data: 0.025456255780698513\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8184 / 10000\n",
            "Epoch 94 training complete\n",
            "Cost on training data: 0.025140276808286257\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8179 / 10000\n",
            "Epoch 95 training complete\n",
            "Cost on training data: 0.024791497558512045\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8188 / 10000\n",
            "Epoch 96 training complete\n",
            "Cost on training data: 0.024429273240369302\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8186 / 10000\n",
            "Epoch 97 training complete\n",
            "Cost on training data: 0.02412533660527348\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8186 / 10000\n",
            "Epoch 98 training complete\n",
            "Cost on training data: 0.023814096637369518\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8187 / 10000\n",
            "Epoch 99 training complete\n",
            "Cost on training data: 0.02354633873343821\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8189 / 10000\n",
            "Epoch 100 training complete\n",
            "Cost on training data: 0.02324144609585402\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8189 / 10000\n",
            "Epoch 101 training complete\n",
            "Cost on training data: 0.02290807940262636\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8188 / 10000\n",
            "Epoch 102 training complete\n",
            "Cost on training data: 0.022624824250162084\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8191 / 10000\n",
            "Epoch 103 training complete\n",
            "Cost on training data: 0.02237423092093714\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8189 / 10000\n",
            "Epoch 104 training complete\n",
            "Cost on training data: 0.022071335710303203\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8188 / 10000\n",
            "Epoch 105 training complete\n",
            "Cost on training data: 0.02179989655347394\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 106 training complete\n",
            "Cost on training data: 0.021496006275398487\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8189 / 10000\n",
            "Epoch 107 training complete\n",
            "Cost on training data: 0.021262129023082096\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8188 / 10000\n",
            "Epoch 108 training complete\n",
            "Cost on training data: 0.02096081029385379\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8192 / 10000\n",
            "Epoch 109 training complete\n",
            "Cost on training data: 0.020703630014782082\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8190 / 10000\n",
            "Epoch 110 training complete\n",
            "Cost on training data: 0.020465036469164706\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8186 / 10000\n",
            "Epoch 111 training complete\n",
            "Cost on training data: 0.020191458091514278\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8193 / 10000\n",
            "Epoch 112 training complete\n",
            "Cost on training data: 0.019998741662203546\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8183 / 10000\n",
            "Epoch 113 training complete\n",
            "Cost on training data: 0.019734918076023015\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8193 / 10000\n",
            "Epoch 114 training complete\n",
            "Cost on training data: 0.019460826235153265\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8191 / 10000\n",
            "Epoch 115 training complete\n",
            "Cost on training data: 0.019217789514015832\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8193 / 10000\n",
            "Epoch 116 training complete\n",
            "Cost on training data: 0.018983199754592204\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 117 training complete\n",
            "Cost on training data: 0.018753253962473456\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8189 / 10000\n",
            "Epoch 118 training complete\n",
            "Cost on training data: 0.01856936871287704\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 119 training complete\n",
            "Cost on training data: 0.018302361461691747\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8192 / 10000\n",
            "Epoch 120 training complete\n",
            "Cost on training data: 0.018105480893469347\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8194 / 10000\n",
            "Epoch 121 training complete\n",
            "Cost on training data: 0.01788906097272135\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 122 training complete\n",
            "Cost on training data: 0.017680813084336416\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 123 training complete\n",
            "Cost on training data: 0.01747816503349572\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8192 / 10000\n",
            "Epoch 124 training complete\n",
            "Cost on training data: 0.017264320651737632\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8189 / 10000\n",
            "Epoch 125 training complete\n",
            "Cost on training data: 0.01706632146523699\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 126 training complete\n",
            "Cost on training data: 0.016875126537043754\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8194 / 10000\n",
            "Epoch 127 training complete\n",
            "Cost on training data: 0.016692042132676308\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8191 / 10000\n",
            "Epoch 128 training complete\n",
            "Cost on training data: 0.01651147247776163\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8199 / 10000\n",
            "Epoch 129 training complete\n",
            "Cost on training data: 0.016310746949840197\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8199 / 10000\n",
            "Epoch 130 training complete\n",
            "Cost on training data: 0.016112361310733396\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 131 training complete\n",
            "Cost on training data: 0.015921070867666295\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 132 training complete\n",
            "Cost on training data: 0.015711021290301504\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8194 / 10000\n",
            "Epoch 133 training complete\n",
            "Cost on training data: 0.015505214941508656\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8193 / 10000\n",
            "Epoch 134 training complete\n",
            "Cost on training data: 0.01532012740877518\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 135 training complete\n",
            "Cost on training data: 0.015148960861733726\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 136 training complete\n",
            "Cost on training data: 0.01498781769941701\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 137 training complete\n",
            "Cost on training data: 0.014839111562080563\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 138 training complete\n",
            "Cost on training data: 0.014678343630104592\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 139 training complete\n",
            "Cost on training data: 0.014525556352789296\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 140 training complete\n",
            "Cost on training data: 0.014380717823169952\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 141 training complete\n",
            "Cost on training data: 0.014261398772340174\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8193 / 10000\n",
            "Epoch 142 training complete\n",
            "Cost on training data: 0.014109059634773458\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 143 training complete\n",
            "Cost on training data: 0.013982834939021511\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 144 training complete\n",
            "Cost on training data: 0.013849588087997829\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8193 / 10000\n",
            "Epoch 145 training complete\n",
            "Cost on training data: 0.013701370539075794\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8193 / 10000\n",
            "Epoch 146 training complete\n",
            "Cost on training data: 0.013573429191201783\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 147 training complete\n",
            "Cost on training data: 0.01344866460288\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8193 / 10000\n",
            "Epoch 148 training complete\n",
            "Cost on training data: 0.013325498039407482\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8190 / 10000\n",
            "Epoch 149 training complete\n",
            "Cost on training data: 0.013206087461658111\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 150 training complete\n",
            "Cost on training data: 0.013095096162772782\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 151 training complete\n",
            "Cost on training data: 0.012987910978113255\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8192 / 10000\n",
            "Epoch 152 training complete\n",
            "Cost on training data: 0.012869610295091325\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8190 / 10000\n",
            "Epoch 153 training complete\n",
            "Cost on training data: 0.012759458661242612\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8190 / 10000\n",
            "Epoch 154 training complete\n",
            "Cost on training data: 0.012651724392913384\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 155 training complete\n",
            "Cost on training data: 0.012547652315861663\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 156 training complete\n",
            "Cost on training data: 0.01245019907909969\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 157 training complete\n",
            "Cost on training data: 0.01234863022656329\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8194 / 10000\n",
            "Epoch 158 training complete\n",
            "Cost on training data: 0.01224477628598606\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 159 training complete\n",
            "Cost on training data: 0.012147463848492874\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8192 / 10000\n",
            "Epoch 160 training complete\n",
            "Cost on training data: 0.01205223515490755\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8193 / 10000\n",
            "Epoch 161 training complete\n",
            "Cost on training data: 0.01197018278479611\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8184 / 10000\n",
            "Epoch 162 training complete\n",
            "Cost on training data: 0.01188815700368673\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8194 / 10000\n",
            "Epoch 163 training complete\n",
            "Cost on training data: 0.01178250226994366\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 164 training complete\n",
            "Cost on training data: 0.011685353868556883\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 165 training complete\n",
            "Cost on training data: 0.011595563376570195\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8194 / 10000\n",
            "Epoch 166 training complete\n",
            "Cost on training data: 0.011518755557464986\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 167 training complete\n",
            "Cost on training data: 0.011429833622034811\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8194 / 10000\n",
            "Epoch 168 training complete\n",
            "Cost on training data: 0.01133675632204841\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8194 / 10000\n",
            "Epoch 169 training complete\n",
            "Cost on training data: 0.011256320951133048\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 170 training complete\n",
            "Cost on training data: 0.011178493325425868\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8194 / 10000\n",
            "Epoch 171 training complete\n",
            "Cost on training data: 0.011092004689268863\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 172 training complete\n",
            "Cost on training data: 0.011013598774968695\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8198 / 10000\n",
            "Epoch 173 training complete\n",
            "Cost on training data: 0.010933259833762561\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 174 training complete\n",
            "Cost on training data: 0.010856740862805357\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8201 / 10000\n",
            "Epoch 175 training complete\n",
            "Cost on training data: 0.010786017180406202\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 176 training complete\n",
            "Cost on training data: 0.010705505233659714\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 177 training complete\n",
            "Cost on training data: 0.01063797088804263\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 178 training complete\n",
            "Cost on training data: 0.010558541752300437\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 179 training complete\n",
            "Cost on training data: 0.010485904797885585\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8198 / 10000\n",
            "Epoch 180 training complete\n",
            "Cost on training data: 0.010418149574097993\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 181 training complete\n",
            "Cost on training data: 0.010372193886553341\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8190 / 10000\n",
            "Epoch 182 training complete\n",
            "Cost on training data: 0.010278159060368663\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8199 / 10000\n",
            "Epoch 183 training complete\n",
            "Cost on training data: 0.010208860954146621\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8199 / 10000\n",
            "Epoch 184 training complete\n",
            "Cost on training data: 0.010140688990892887\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8198 / 10000\n",
            "Epoch 185 training complete\n",
            "Cost on training data: 0.010075215554740778\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 186 training complete\n",
            "Cost on training data: 0.010013886837945135\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8201 / 10000\n",
            "Epoch 187 training complete\n",
            "Cost on training data: 0.009944308299781539\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8198 / 10000\n",
            "Epoch 188 training complete\n",
            "Cost on training data: 0.009881694472788029\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 189 training complete\n",
            "Cost on training data: 0.009817253847471128\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 190 training complete\n",
            "Cost on training data: 0.009755605665997709\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8194 / 10000\n",
            "Epoch 191 training complete\n",
            "Cost on training data: 0.009694741085292793\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8199 / 10000\n",
            "Epoch 192 training complete\n",
            "Cost on training data: 0.009632483086161674\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8199 / 10000\n",
            "Epoch 193 training complete\n",
            "Cost on training data: 0.009574660652954215\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 194 training complete\n",
            "Cost on training data: 0.009513498177960322\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 195 training complete\n",
            "Cost on training data: 0.009455315000322501\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 196 training complete\n",
            "Cost on training data: 0.009395083448544256\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 197 training complete\n",
            "Cost on training data: 0.009338776943226303\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8198 / 10000\n",
            "Epoch 198 training complete\n",
            "Cost on training data: 0.009305170493211742\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8191 / 10000\n",
            "Epoch 199 training complete\n",
            "Cost on training data: 0.00922852174324816\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 200 training complete\n",
            "Cost on training data: 0.009177841457123775\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8193 / 10000\n",
            "Epoch 201 training complete\n",
            "Cost on training data: 0.009123477013100829\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 202 training complete\n",
            "Cost on training data: 0.009061441285500025\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8193 / 10000\n",
            "Epoch 203 training complete\n",
            "Cost on training data: 0.009006310567048441\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 204 training complete\n",
            "Cost on training data: 0.008951969259200545\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 205 training complete\n",
            "Cost on training data: 0.008898511079695668\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8195 / 10000\n",
            "Epoch 206 training complete\n",
            "Cost on training data: 0.008850175708678604\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8198 / 10000\n",
            "Epoch 207 training complete\n",
            "Cost on training data: 0.008794061424406354\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 208 training complete\n",
            "Cost on training data: 0.008744607371844518\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 209 training complete\n",
            "Cost on training data: 0.008692446465293272\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8196 / 10000\n",
            "Epoch 210 training complete\n",
            "Cost on training data: 0.008643889182648924\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 211 training complete\n",
            "Cost on training data: 0.008593494298684877\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 212 training complete\n",
            "Cost on training data: 0.00854694518432956\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8198 / 10000\n",
            "Epoch 213 training complete\n",
            "Cost on training data: 0.008493533317362408\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8198 / 10000\n",
            "Epoch 214 training complete\n",
            "Cost on training data: 0.008445485131322804\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 215 training complete\n",
            "Cost on training data: 0.008397693445210642\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 216 training complete\n",
            "Cost on training data: 0.0083505496044251\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 217 training complete\n",
            "Cost on training data: 0.008301630488797035\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 218 training complete\n",
            "Cost on training data: 0.008255682747954154\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8192 / 10000\n",
            "Epoch 219 training complete\n",
            "Cost on training data: 0.008209342877140242\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8192 / 10000\n",
            "Epoch 220 training complete\n",
            "Cost on training data: 0.008164693932317037\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 221 training complete\n",
            "Cost on training data: 0.008119914723580953\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8193 / 10000\n",
            "Epoch 222 training complete\n",
            "Cost on training data: 0.008075184401243175\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8197 / 10000\n",
            "Epoch 223 training complete\n",
            "Cost on training data: 0.008030307981612006\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 224 training complete\n",
            "Cost on training data: 0.007985471709988172\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8199 / 10000\n",
            "Epoch 225 training complete\n",
            "Cost on training data: 0.007942887517048188\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 226 training complete\n",
            "Cost on training data: 0.007899773625432104\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 227 training complete\n",
            "Cost on training data: 0.007857758660617754\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 228 training complete\n",
            "Cost on training data: 0.007815216206462142\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8198 / 10000\n",
            "Epoch 229 training complete\n",
            "Cost on training data: 0.007776844140545788\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8198 / 10000\n",
            "Epoch 230 training complete\n",
            "Cost on training data: 0.0077423274250074245\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 231 training complete\n",
            "Cost on training data: 0.007699710613073892\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8198 / 10000\n",
            "Epoch 232 training complete\n",
            "Cost on training data: 0.007652863882152705\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8198 / 10000\n",
            "Epoch 233 training complete\n",
            "Cost on training data: 0.007613551250715081\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 234 training complete\n",
            "Cost on training data: 0.007574978400859972\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8199 / 10000\n",
            "Epoch 235 training complete\n",
            "Cost on training data: 0.007537000169906921\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 236 training complete\n",
            "Cost on training data: 0.007497658806899837\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8199 / 10000\n",
            "Epoch 237 training complete\n",
            "Cost on training data: 0.007460882799728524\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 238 training complete\n",
            "Cost on training data: 0.007425128231005597\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 239 training complete\n",
            "Cost on training data: 0.007385885128179468\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8198 / 10000\n",
            "Epoch 240 training complete\n",
            "Cost on training data: 0.007349975612804946\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8200 / 10000\n",
            "Epoch 241 training complete\n",
            "Cost on training data: 0.0073131401836965945\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 242 training complete\n",
            "Cost on training data: 0.007276752168573986\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 243 training complete\n",
            "Cost on training data: 0.007242498395843805\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 244 training complete\n",
            "Cost on training data: 0.007206500054338843\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 245 training complete\n",
            "Cost on training data: 0.007173089614814217\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 246 training complete\n",
            "Cost on training data: 0.007136986606612905\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 247 training complete\n",
            "Cost on training data: 0.007102426912413435\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8201 / 10000\n",
            "Epoch 248 training complete\n",
            "Cost on training data: 0.0070697276137163985\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 249 training complete\n",
            "Cost on training data: 0.007035694483412859\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 250 training complete\n",
            "Cost on training data: 0.0070020571825583685\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 251 training complete\n",
            "Cost on training data: 0.006969495217084261\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 252 training complete\n",
            "Cost on training data: 0.006936486158517295\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 253 training complete\n",
            "Cost on training data: 0.006904217108445739\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8201 / 10000\n",
            "Epoch 254 training complete\n",
            "Cost on training data: 0.00687237717798646\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8201 / 10000\n",
            "Epoch 255 training complete\n",
            "Cost on training data: 0.00684145285113612\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 256 training complete\n",
            "Cost on training data: 0.006809030158502687\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8201 / 10000\n",
            "Epoch 257 training complete\n",
            "Cost on training data: 0.00677855311972923\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 258 training complete\n",
            "Cost on training data: 0.006748118505653425\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 259 training complete\n",
            "Cost on training data: 0.006716829530744676\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 260 training complete\n",
            "Cost on training data: 0.006687068592361557\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 261 training complete\n",
            "Cost on training data: 0.0066568814790248084\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 262 training complete\n",
            "Cost on training data: 0.0066267944393486\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 263 training complete\n",
            "Cost on training data: 0.006597834635481722\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 264 training complete\n",
            "Cost on training data: 0.006570797846856313\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 265 training complete\n",
            "Cost on training data: 0.006538862234439492\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 266 training complete\n",
            "Cost on training data: 0.006509856973086046\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 267 training complete\n",
            "Cost on training data: 0.0064817388763105735\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 268 training complete\n",
            "Cost on training data: 0.006453153935039508\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 269 training complete\n",
            "Cost on training data: 0.006425086550379677\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 270 training complete\n",
            "Cost on training data: 0.00639801289125471\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 271 training complete\n",
            "Cost on training data: 0.006370432506071775\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 272 training complete\n",
            "Cost on training data: 0.006344928568219975\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 273 training complete\n",
            "Cost on training data: 0.006315115458495787\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 274 training complete\n",
            "Cost on training data: 0.006288277113647007\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 275 training complete\n",
            "Cost on training data: 0.006261375186264531\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8207 / 10000\n",
            "Epoch 276 training complete\n",
            "Cost on training data: 0.006235949262796828\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 277 training complete\n",
            "Cost on training data: 0.006208588845377523\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 278 training complete\n",
            "Cost on training data: 0.006182466508757815\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 279 training complete\n",
            "Cost on training data: 0.006157724418467639\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 280 training complete\n",
            "Cost on training data: 0.006131835183613007\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 281 training complete\n",
            "Cost on training data: 0.006106484740325447\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 282 training complete\n",
            "Cost on training data: 0.00608002637420485\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8206 / 10000\n",
            "Epoch 283 training complete\n",
            "Cost on training data: 0.006055150539633937\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 284 training complete\n",
            "Cost on training data: 0.006030744914006422\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 285 training complete\n",
            "Cost on training data: 0.006005356815804681\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 286 training complete\n",
            "Cost on training data: 0.0059815474918202965\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8207 / 10000\n",
            "Epoch 287 training complete\n",
            "Cost on training data: 0.0059563786160860945\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8208 / 10000\n",
            "Epoch 288 training complete\n",
            "Cost on training data: 0.005932209203800986\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 289 training complete\n",
            "Cost on training data: 0.005907838183945409\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 290 training complete\n",
            "Cost on training data: 0.0058842505450400675\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 291 training complete\n",
            "Cost on training data: 0.005859843285521336\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 292 training complete\n",
            "Cost on training data: 0.0058364093200851995\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 293 training complete\n",
            "Cost on training data: 0.005812746885317053\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 294 training complete\n",
            "Cost on training data: 0.0057898068836295755\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 295 training complete\n",
            "Cost on training data: 0.005767445417549336\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8206 / 10000\n",
            "Epoch 296 training complete\n",
            "Cost on training data: 0.005743518842300221\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 297 training complete\n",
            "Cost on training data: 0.005720456525631282\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 298 training complete\n",
            "Cost on training data: 0.005698723737464183\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 299 training complete\n",
            "Cost on training data: 0.0056762583418141395\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 300 training complete\n",
            "Cost on training data: 0.005653897199327525\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 301 training complete\n",
            "Cost on training data: 0.005630235087540331\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8206 / 10000\n",
            "Epoch 302 training complete\n",
            "Cost on training data: 0.005607874731349095\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 303 training complete\n",
            "Cost on training data: 0.005585938312317082\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 304 training complete\n",
            "Cost on training data: 0.005565613329320214\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 305 training complete\n",
            "Cost on training data: 0.005541933956654076\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 306 training complete\n",
            "Cost on training data: 0.005520641024840412\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8206 / 10000\n",
            "Epoch 307 training complete\n",
            "Cost on training data: 0.0054985051269886166\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 308 training complete\n",
            "Cost on training data: 0.005479478975485988\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8206 / 10000\n",
            "Epoch 309 training complete\n",
            "Cost on training data: 0.005454732688403054\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 310 training complete\n",
            "Cost on training data: 0.005433326254931169\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 311 training complete\n",
            "Cost on training data: 0.005412003213366873\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8202 / 10000\n",
            "Epoch 312 training complete\n",
            "Cost on training data: 0.005390065836508451\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 313 training complete\n",
            "Cost on training data: 0.0053686117044971105\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8207 / 10000\n",
            "Epoch 314 training complete\n",
            "Cost on training data: 0.005346847473570504\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8204 / 10000\n",
            "Epoch 315 training complete\n",
            "Cost on training data: 0.005326566795290727\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8203 / 10000\n",
            "Epoch 316 training complete\n",
            "Cost on training data: 0.005304155891788895\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 317 training complete\n",
            "Cost on training data: 0.0052826472253598485\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 318 training complete\n",
            "Cost on training data: 0.0052605634498580625\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8207 / 10000\n",
            "Epoch 319 training complete\n",
            "Cost on training data: 0.0052390654525985335\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8208 / 10000\n",
            "Epoch 320 training complete\n",
            "Cost on training data: 0.005217333082296659\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8206 / 10000\n",
            "Epoch 321 training complete\n",
            "Cost on training data: 0.005195714823809681\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8205 / 10000\n",
            "Epoch 322 training complete\n",
            "Cost on training data: 0.005174084592797595\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8208 / 10000\n",
            "Epoch 323 training complete\n",
            "Cost on training data: 0.0051523243272966245\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8207 / 10000\n",
            "Epoch 324 training complete\n",
            "Cost on training data: 0.005131124428191397\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8208 / 10000\n",
            "Epoch 325 training complete\n",
            "Cost on training data: 0.005108970907702921\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8209 / 10000\n",
            "Epoch 326 training complete\n",
            "Cost on training data: 0.005087976000002941\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8211 / 10000\n",
            "Epoch 327 training complete\n",
            "Cost on training data: 0.0050652602833821956\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8211 / 10000\n",
            "Epoch 328 training complete\n",
            "Cost on training data: 0.005043967134056425\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8213 / 10000\n",
            "Epoch 329 training complete\n",
            "Cost on training data: 0.005023214309740786\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8211 / 10000\n",
            "Epoch 330 training complete\n",
            "Cost on training data: 0.005001976548865219\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8211 / 10000\n",
            "Epoch 331 training complete\n",
            "Cost on training data: 0.004981013808261443\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8212 / 10000\n",
            "Epoch 332 training complete\n",
            "Cost on training data: 0.004960717402513568\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8212 / 10000\n",
            "Epoch 333 training complete\n",
            "Cost on training data: 0.004941963871384926\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8211 / 10000\n",
            "Epoch 334 training complete\n",
            "Cost on training data: 0.004920459429530762\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8214 / 10000\n",
            "Epoch 335 training complete\n",
            "Cost on training data: 0.004901759333916081\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8212 / 10000\n",
            "Epoch 336 training complete\n",
            "Cost on training data: 0.0048812699721783055\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8213 / 10000\n",
            "Epoch 337 training complete\n",
            "Cost on training data: 0.004862067291088413\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8211 / 10000\n",
            "Epoch 338 training complete\n",
            "Cost on training data: 0.004843229304154937\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8211 / 10000\n",
            "Epoch 339 training complete\n",
            "Cost on training data: 0.004824252993840638\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8214 / 10000\n",
            "Epoch 340 training complete\n",
            "Cost on training data: 0.004805837110075987\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8213 / 10000\n",
            "Epoch 341 training complete\n",
            "Cost on training data: 0.00478729726603634\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8214 / 10000\n",
            "Epoch 342 training complete\n",
            "Cost on training data: 0.004769303358809054\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8214 / 10000\n",
            "Epoch 343 training complete\n",
            "Cost on training data: 0.004751419791552972\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8214 / 10000\n",
            "Epoch 344 training complete\n",
            "Cost on training data: 0.004733583568129665\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8213 / 10000\n",
            "Epoch 345 training complete\n",
            "Cost on training data: 0.0047159451530217195\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8213 / 10000\n",
            "Epoch 346 training complete\n",
            "Cost on training data: 0.004698274652879195\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 347 training complete\n",
            "Cost on training data: 0.004681079763016091\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8217 / 10000\n",
            "Epoch 348 training complete\n",
            "Cost on training data: 0.004663907218850477\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8218 / 10000\n",
            "Epoch 349 training complete\n",
            "Cost on training data: 0.004646929047516724\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 350 training complete\n",
            "Cost on training data: 0.0046302121734004414\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 351 training complete\n",
            "Cost on training data: 0.004613432778836316\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 352 training complete\n",
            "Cost on training data: 0.004597048875781222\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8219 / 10000\n",
            "Epoch 353 training complete\n",
            "Cost on training data: 0.004581423422877473\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8217 / 10000\n",
            "Epoch 354 training complete\n",
            "Cost on training data: 0.004564104714746344\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8218 / 10000\n",
            "Epoch 355 training complete\n",
            "Cost on training data: 0.004547776327289191\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8219 / 10000\n",
            "Epoch 356 training complete\n",
            "Cost on training data: 0.004531706616575266\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8218 / 10000\n",
            "Epoch 357 training complete\n",
            "Cost on training data: 0.004515765588148142\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 358 training complete\n",
            "Cost on training data: 0.004499943793703048\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8217 / 10000\n",
            "Epoch 359 training complete\n",
            "Cost on training data: 0.004484311802352103\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8216 / 10000\n",
            "Epoch 360 training complete\n",
            "Cost on training data: 0.004468706495849413\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 361 training complete\n",
            "Cost on training data: 0.004453240222398173\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8216 / 10000\n",
            "Epoch 362 training complete\n",
            "Cost on training data: 0.004438232498111458\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 363 training complete\n",
            "Cost on training data: 0.0044227986069462315\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 364 training complete\n",
            "Cost on training data: 0.004407434118610254\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8216 / 10000\n",
            "Epoch 365 training complete\n",
            "Cost on training data: 0.00439241000023391\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8217 / 10000\n",
            "Epoch 366 training complete\n",
            "Cost on training data: 0.004377390649225742\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8217 / 10000\n",
            "Epoch 367 training complete\n",
            "Cost on training data: 0.004362558867095417\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 368 training complete\n",
            "Cost on training data: 0.004348208111246156\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 369 training complete\n",
            "Cost on training data: 0.004333351513815025\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 370 training complete\n",
            "Cost on training data: 0.004319726410348993\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8217 / 10000\n",
            "Epoch 371 training complete\n",
            "Cost on training data: 0.004305023602127728\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 372 training complete\n",
            "Cost on training data: 0.004290110443764529\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8217 / 10000\n",
            "Epoch 373 training complete\n",
            "Cost on training data: 0.004275671731984722\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 374 training complete\n",
            "Cost on training data: 0.004261490418164142\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8219 / 10000\n",
            "Epoch 375 training complete\n",
            "Cost on training data: 0.004247366504757626\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8217 / 10000\n",
            "Epoch 376 training complete\n",
            "Cost on training data: 0.00423335527221789\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8217 / 10000\n",
            "Epoch 377 training complete\n",
            "Cost on training data: 0.004219490083109391\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8218 / 10000\n",
            "Epoch 378 training complete\n",
            "Cost on training data: 0.004205668000242625\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8219 / 10000\n",
            "Epoch 379 training complete\n",
            "Cost on training data: 0.004192012978046249\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8216 / 10000\n",
            "Epoch 380 training complete\n",
            "Cost on training data: 0.0041784978609759865\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8218 / 10000\n",
            "Epoch 381 training complete\n",
            "Cost on training data: 0.004165179364854805\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8217 / 10000\n",
            "Epoch 382 training complete\n",
            "Cost on training data: 0.0041516029408328325\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8216 / 10000\n",
            "Epoch 383 training complete\n",
            "Cost on training data: 0.004138402434448656\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8217 / 10000\n",
            "Epoch 384 training complete\n",
            "Cost on training data: 0.004124721222565639\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8216 / 10000\n",
            "Epoch 385 training complete\n",
            "Cost on training data: 0.004111737712333742\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 386 training complete\n",
            "Cost on training data: 0.004098755857595346\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8215 / 10000\n",
            "Epoch 387 training complete\n",
            "Cost on training data: 0.004085433861197219\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8217 / 10000\n",
            "Epoch 388 training complete\n",
            "Cost on training data: 0.0040724195794112445\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8219 / 10000\n",
            "Epoch 389 training complete\n",
            "Cost on training data: 0.004059582278894604\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8216 / 10000\n",
            "Epoch 390 training complete\n",
            "Cost on training data: 0.004047101894343415\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8219 / 10000\n",
            "Epoch 391 training complete\n",
            "Cost on training data: 0.004034399908405423\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8221 / 10000\n",
            "Epoch 392 training complete\n",
            "Cost on training data: 0.00402159759527241\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8219 / 10000\n",
            "Epoch 393 training complete\n",
            "Cost on training data: 0.0040089405784502555\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8219 / 10000\n",
            "Epoch 394 training complete\n",
            "Cost on training data: 0.003996638472903983\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8220 / 10000\n",
            "Epoch 395 training complete\n",
            "Cost on training data: 0.003984034247396709\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8219 / 10000\n",
            "Epoch 396 training complete\n",
            "Cost on training data: 0.003971886930403427\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8220 / 10000\n",
            "Epoch 397 training complete\n",
            "Cost on training data: 0.0039594086200926085\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8218 / 10000\n",
            "Epoch 398 training complete\n",
            "Cost on training data: 0.003947219316963371\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8219 / 10000\n",
            "Epoch 399 training complete\n",
            "Cost on training data: 0.0039353396331847906\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Accuracy on evaluation data: 8219 / 10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([],\n",
              " [5259,\n",
              "  6368,\n",
              "  7097,\n",
              "  7273,\n",
              "  7472,\n",
              "  7584,\n",
              "  7739,\n",
              "  7732,\n",
              "  7744,\n",
              "  7795,\n",
              "  7908,\n",
              "  7865,\n",
              "  7903,\n",
              "  7938,\n",
              "  7957,\n",
              "  7944,\n",
              "  7972,\n",
              "  7971,\n",
              "  7948,\n",
              "  8024,\n",
              "  7989,\n",
              "  8041,\n",
              "  8004,\n",
              "  8040,\n",
              "  8035,\n",
              "  8047,\n",
              "  8073,\n",
              "  8083,\n",
              "  8085,\n",
              "  8074,\n",
              "  8066,\n",
              "  8108,\n",
              "  8086,\n",
              "  8084,\n",
              "  8094,\n",
              "  8106,\n",
              "  8106,\n",
              "  8101,\n",
              "  8079,\n",
              "  8100,\n",
              "  8114,\n",
              "  8110,\n",
              "  8118,\n",
              "  8119,\n",
              "  8138,\n",
              "  8119,\n",
              "  8125,\n",
              "  8126,\n",
              "  8129,\n",
              "  8126,\n",
              "  8128,\n",
              "  8134,\n",
              "  8124,\n",
              "  8116,\n",
              "  8146,\n",
              "  8136,\n",
              "  8153,\n",
              "  8147,\n",
              "  8160,\n",
              "  8147,\n",
              "  8159,\n",
              "  8154,\n",
              "  8154,\n",
              "  8155,\n",
              "  8157,\n",
              "  8167,\n",
              "  8155,\n",
              "  8163,\n",
              "  8164,\n",
              "  8166,\n",
              "  8167,\n",
              "  8172,\n",
              "  8172,\n",
              "  8171,\n",
              "  8174,\n",
              "  8183,\n",
              "  8175,\n",
              "  8176,\n",
              "  8171,\n",
              "  8181,\n",
              "  8179,\n",
              "  8175,\n",
              "  8177,\n",
              "  8181,\n",
              "  8179,\n",
              "  8178,\n",
              "  8183,\n",
              "  8178,\n",
              "  8183,\n",
              "  8184,\n",
              "  8183,\n",
              "  8185,\n",
              "  8180,\n",
              "  8184,\n",
              "  8179,\n",
              "  8188,\n",
              "  8186,\n",
              "  8186,\n",
              "  8187,\n",
              "  8189,\n",
              "  8189,\n",
              "  8188,\n",
              "  8191,\n",
              "  8189,\n",
              "  8188,\n",
              "  8197,\n",
              "  8189,\n",
              "  8188,\n",
              "  8192,\n",
              "  8190,\n",
              "  8186,\n",
              "  8193,\n",
              "  8183,\n",
              "  8193,\n",
              "  8191,\n",
              "  8193,\n",
              "  8195,\n",
              "  8189,\n",
              "  8195,\n",
              "  8192,\n",
              "  8194,\n",
              "  8197,\n",
              "  8196,\n",
              "  8192,\n",
              "  8189,\n",
              "  8195,\n",
              "  8194,\n",
              "  8191,\n",
              "  8199,\n",
              "  8199,\n",
              "  8200,\n",
              "  8195,\n",
              "  8194,\n",
              "  8193,\n",
              "  8196,\n",
              "  8197,\n",
              "  8196,\n",
              "  8196,\n",
              "  8203,\n",
              "  8196,\n",
              "  8196,\n",
              "  8193,\n",
              "  8196,\n",
              "  8196,\n",
              "  8193,\n",
              "  8193,\n",
              "  8195,\n",
              "  8193,\n",
              "  8190,\n",
              "  8196,\n",
              "  8197,\n",
              "  8192,\n",
              "  8190,\n",
              "  8190,\n",
              "  8195,\n",
              "  8195,\n",
              "  8196,\n",
              "  8194,\n",
              "  8195,\n",
              "  8192,\n",
              "  8193,\n",
              "  8184,\n",
              "  8194,\n",
              "  8195,\n",
              "  8197,\n",
              "  8194,\n",
              "  8197,\n",
              "  8194,\n",
              "  8194,\n",
              "  8197,\n",
              "  8194,\n",
              "  8197,\n",
              "  8198,\n",
              "  8200,\n",
              "  8201,\n",
              "  8195,\n",
              "  8197,\n",
              "  8200,\n",
              "  8196,\n",
              "  8198,\n",
              "  8195,\n",
              "  8190,\n",
              "  8199,\n",
              "  8199,\n",
              "  8198,\n",
              "  8200,\n",
              "  8201,\n",
              "  8198,\n",
              "  8200,\n",
              "  8195,\n",
              "  8194,\n",
              "  8199,\n",
              "  8199,\n",
              "  8196,\n",
              "  8197,\n",
              "  8200,\n",
              "  8195,\n",
              "  8198,\n",
              "  8191,\n",
              "  8200,\n",
              "  8193,\n",
              "  8202,\n",
              "  8193,\n",
              "  8195,\n",
              "  8195,\n",
              "  8195,\n",
              "  8198,\n",
              "  8196,\n",
              "  8196,\n",
              "  8196,\n",
              "  8197,\n",
              "  8197,\n",
              "  8198,\n",
              "  8198,\n",
              "  8200,\n",
              "  8197,\n",
              "  8197,\n",
              "  8197,\n",
              "  8192,\n",
              "  8192,\n",
              "  8197,\n",
              "  8193,\n",
              "  8197,\n",
              "  8202,\n",
              "  8199,\n",
              "  8200,\n",
              "  8202,\n",
              "  8200,\n",
              "  8198,\n",
              "  8198,\n",
              "  8200,\n",
              "  8198,\n",
              "  8198,\n",
              "  8202,\n",
              "  8199,\n",
              "  8200,\n",
              "  8199,\n",
              "  8200,\n",
              "  8202,\n",
              "  8198,\n",
              "  8200,\n",
              "  8204,\n",
              "  8205,\n",
              "  8203,\n",
              "  8203,\n",
              "  8203,\n",
              "  8203,\n",
              "  8201,\n",
              "  8202,\n",
              "  8202,\n",
              "  8202,\n",
              "  8203,\n",
              "  8203,\n",
              "  8201,\n",
              "  8201,\n",
              "  8204,\n",
              "  8201,\n",
              "  8203,\n",
              "  8202,\n",
              "  8203,\n",
              "  8204,\n",
              "  8204,\n",
              "  8204,\n",
              "  8202,\n",
              "  8202,\n",
              "  8205,\n",
              "  8204,\n",
              "  8203,\n",
              "  8203,\n",
              "  8205,\n",
              "  8205,\n",
              "  8204,\n",
              "  8202,\n",
              "  8205,\n",
              "  8205,\n",
              "  8207,\n",
              "  8202,\n",
              "  8205,\n",
              "  8203,\n",
              "  8202,\n",
              "  8204,\n",
              "  8205,\n",
              "  8206,\n",
              "  8205,\n",
              "  8204,\n",
              "  8203,\n",
              "  8207,\n",
              "  8208,\n",
              "  8204,\n",
              "  8204,\n",
              "  8204,\n",
              "  8203,\n",
              "  8203,\n",
              "  8205,\n",
              "  8204,\n",
              "  8206,\n",
              "  8205,\n",
              "  8204,\n",
              "  8204,\n",
              "  8205,\n",
              "  8204,\n",
              "  8206,\n",
              "  8205,\n",
              "  8205,\n",
              "  8203,\n",
              "  8204,\n",
              "  8206,\n",
              "  8203,\n",
              "  8206,\n",
              "  8204,\n",
              "  8204,\n",
              "  8202,\n",
              "  8204,\n",
              "  8207,\n",
              "  8204,\n",
              "  8203,\n",
              "  8205,\n",
              "  8205,\n",
              "  8207,\n",
              "  8208,\n",
              "  8206,\n",
              "  8205,\n",
              "  8208,\n",
              "  8207,\n",
              "  8208,\n",
              "  8209,\n",
              "  8211,\n",
              "  8211,\n",
              "  8213,\n",
              "  8211,\n",
              "  8211,\n",
              "  8212,\n",
              "  8212,\n",
              "  8211,\n",
              "  8214,\n",
              "  8212,\n",
              "  8213,\n",
              "  8211,\n",
              "  8211,\n",
              "  8214,\n",
              "  8213,\n",
              "  8214,\n",
              "  8214,\n",
              "  8214,\n",
              "  8213,\n",
              "  8213,\n",
              "  8215,\n",
              "  8217,\n",
              "  8218,\n",
              "  8215,\n",
              "  8215,\n",
              "  8215,\n",
              "  8219,\n",
              "  8217,\n",
              "  8218,\n",
              "  8219,\n",
              "  8218,\n",
              "  8215,\n",
              "  8217,\n",
              "  8216,\n",
              "  8215,\n",
              "  8216,\n",
              "  8215,\n",
              "  8215,\n",
              "  8216,\n",
              "  8217,\n",
              "  8217,\n",
              "  8215,\n",
              "  8215,\n",
              "  8215,\n",
              "  8217,\n",
              "  8215,\n",
              "  8217,\n",
              "  8215,\n",
              "  8219,\n",
              "  8217,\n",
              "  8217,\n",
              "  8218,\n",
              "  8219,\n",
              "  8216,\n",
              "  8218,\n",
              "  8217,\n",
              "  8216,\n",
              "  8217,\n",
              "  8216,\n",
              "  8215,\n",
              "  8215,\n",
              "  8217,\n",
              "  8219,\n",
              "  8216,\n",
              "  8219,\n",
              "  8221,\n",
              "  8219,\n",
              "  8219,\n",
              "  8220,\n",
              "  8219,\n",
              "  8220,\n",
              "  8218,\n",
              "  8219,\n",
              "  8219],\n",
              " [np.float64(1.9161820362261717),\n",
              "  np.float64(1.451017926394901),\n",
              "  np.float64(1.173382667331476),\n",
              "  np.float64(0.9988949276506093),\n",
              "  np.float64(0.8567673403871416),\n",
              "  np.float64(0.7437641726265786),\n",
              "  np.float64(0.6474265216331955),\n",
              "  np.float64(0.6034252638094514),\n",
              "  np.float64(0.5420420143341356),\n",
              "  np.float64(0.5053106439773256),\n",
              "  np.float64(0.44544690426495254),\n",
              "  np.float64(0.3977533304529599),\n",
              "  np.float64(0.3611016344250987),\n",
              "  np.float64(0.33495723476138306),\n",
              "  np.float64(0.3203821835952007),\n",
              "  np.float64(0.286216034795556),\n",
              "  np.float64(0.26957860539957895),\n",
              "  np.float64(0.25448293811085193),\n",
              "  np.float64(0.23128967538408807),\n",
              "  np.float64(0.216062446169818),\n",
              "  np.float64(0.20396481451441967),\n",
              "  np.float64(0.19153433380812124),\n",
              "  np.float64(0.17929305492574402),\n",
              "  np.float64(0.16682468797374292),\n",
              "  np.float64(0.15979874882952233),\n",
              "  np.float64(0.15321694722254678),\n",
              "  np.float64(0.14139142908310484),\n",
              "  np.float64(0.13466084565847827),\n",
              "  np.float64(0.12759118195112362),\n",
              "  np.float64(0.12385474661657996),\n",
              "  np.float64(0.11726852484485097),\n",
              "  np.float64(0.11246198787642238),\n",
              "  np.float64(0.10742607937208339),\n",
              "  np.float64(0.10193959998580965),\n",
              "  np.float64(0.09669549541614512),\n",
              "  np.float64(0.09298987083939546),\n",
              "  np.float64(0.08943076801751881),\n",
              "  np.float64(0.08635324676988973),\n",
              "  np.float64(0.08702723097650616),\n",
              "  np.float64(0.08116830908375858),\n",
              "  np.float64(0.07841023188092598),\n",
              "  np.float64(0.07598501925377833),\n",
              "  np.float64(0.0736781415014485),\n",
              "  np.float64(0.07133489162809774),\n",
              "  np.float64(0.06936400605372806),\n",
              "  np.float64(0.06748057582244696),\n",
              "  np.float64(0.06498218171822621),\n",
              "  np.float64(0.06381103004793708),\n",
              "  np.float64(0.061479700233414813),\n",
              "  np.float64(0.059444965925680986),\n",
              "  np.float64(0.05793690867071746),\n",
              "  np.float64(0.05648780824032729),\n",
              "  np.float64(0.05536658528098888),\n",
              "  np.float64(0.054367236815501783),\n",
              "  np.float64(0.05224463600139193),\n",
              "  np.float64(0.05120196257745052),\n",
              "  np.float64(0.05033755096574551),\n",
              "  np.float64(0.048789811725183055),\n",
              "  np.float64(0.04761470787587565),\n",
              "  np.float64(0.04657019696190632),\n",
              "  np.float64(0.04557400780166889),\n",
              "  np.float64(0.04455286225363928),\n",
              "  np.float64(0.04357519756663017),\n",
              "  np.float64(0.042754651047054774),\n",
              "  np.float64(0.04192434935861382),\n",
              "  np.float64(0.041055442109601534),\n",
              "  np.float64(0.04018140964793058),\n",
              "  np.float64(0.03942070459396349),\n",
              "  np.float64(0.03856348934016692),\n",
              "  np.float64(0.037815633376224164),\n",
              "  np.float64(0.03743754487411842),\n",
              "  np.float64(0.03670791287588563),\n",
              "  np.float64(0.03569591871955962),\n",
              "  np.float64(0.034972289255259165),\n",
              "  np.float64(0.03432560243805228),\n",
              "  np.float64(0.033801698807545376),\n",
              "  np.float64(0.03314390162344836),\n",
              "  np.float64(0.032633074619398954),\n",
              "  np.float64(0.03217431246927002),\n",
              "  np.float64(0.03157622740335076),\n",
              "  np.float64(0.03108791530769714),\n",
              "  np.float64(0.030546338335984694),\n",
              "  np.float64(0.030028164847316727),\n",
              "  np.float64(0.02955217994188909),\n",
              "  np.float64(0.029138726274794552),\n",
              "  np.float64(0.02867480169490551),\n",
              "  np.float64(0.028241424459614604),\n",
              "  np.float64(0.02779191792260762),\n",
              "  np.float64(0.027362086548414703),\n",
              "  np.float64(0.02697043765238485),\n",
              "  np.float64(0.026566059590364642),\n",
              "  np.float64(0.026194987649834356),\n",
              "  np.float64(0.02584493763103555),\n",
              "  np.float64(0.025456255780698513),\n",
              "  np.float64(0.025140276808286257),\n",
              "  np.float64(0.024791497558512045),\n",
              "  np.float64(0.024429273240369302),\n",
              "  np.float64(0.02412533660527348),\n",
              "  np.float64(0.023814096637369518),\n",
              "  np.float64(0.02354633873343821),\n",
              "  np.float64(0.02324144609585402),\n",
              "  np.float64(0.02290807940262636),\n",
              "  np.float64(0.022624824250162084),\n",
              "  np.float64(0.02237423092093714),\n",
              "  np.float64(0.022071335710303203),\n",
              "  np.float64(0.02179989655347394),\n",
              "  np.float64(0.021496006275398487),\n",
              "  np.float64(0.021262129023082096),\n",
              "  np.float64(0.02096081029385379),\n",
              "  np.float64(0.020703630014782082),\n",
              "  np.float64(0.020465036469164706),\n",
              "  np.float64(0.020191458091514278),\n",
              "  np.float64(0.019998741662203546),\n",
              "  np.float64(0.019734918076023015),\n",
              "  np.float64(0.019460826235153265),\n",
              "  np.float64(0.019217789514015832),\n",
              "  np.float64(0.018983199754592204),\n",
              "  np.float64(0.018753253962473456),\n",
              "  np.float64(0.01856936871287704),\n",
              "  np.float64(0.018302361461691747),\n",
              "  np.float64(0.018105480893469347),\n",
              "  np.float64(0.01788906097272135),\n",
              "  np.float64(0.017680813084336416),\n",
              "  np.float64(0.01747816503349572),\n",
              "  np.float64(0.017264320651737632),\n",
              "  np.float64(0.01706632146523699),\n",
              "  np.float64(0.016875126537043754),\n",
              "  np.float64(0.016692042132676308),\n",
              "  np.float64(0.01651147247776163),\n",
              "  np.float64(0.016310746949840197),\n",
              "  np.float64(0.016112361310733396),\n",
              "  np.float64(0.015921070867666295),\n",
              "  np.float64(0.015711021290301504),\n",
              "  np.float64(0.015505214941508656),\n",
              "  np.float64(0.01532012740877518),\n",
              "  np.float64(0.015148960861733726),\n",
              "  np.float64(0.01498781769941701),\n",
              "  np.float64(0.014839111562080563),\n",
              "  np.float64(0.014678343630104592),\n",
              "  np.float64(0.014525556352789296),\n",
              "  np.float64(0.014380717823169952),\n",
              "  np.float64(0.014261398772340174),\n",
              "  np.float64(0.014109059634773458),\n",
              "  np.float64(0.013982834939021511),\n",
              "  np.float64(0.013849588087997829),\n",
              "  np.float64(0.013701370539075794),\n",
              "  np.float64(0.013573429191201783),\n",
              "  np.float64(0.01344866460288),\n",
              "  np.float64(0.013325498039407482),\n",
              "  np.float64(0.013206087461658111),\n",
              "  np.float64(0.013095096162772782),\n",
              "  np.float64(0.012987910978113255),\n",
              "  np.float64(0.012869610295091325),\n",
              "  np.float64(0.012759458661242612),\n",
              "  np.float64(0.012651724392913384),\n",
              "  np.float64(0.012547652315861663),\n",
              "  np.float64(0.01245019907909969),\n",
              "  np.float64(0.01234863022656329),\n",
              "  np.float64(0.01224477628598606),\n",
              "  np.float64(0.012147463848492874),\n",
              "  np.float64(0.01205223515490755),\n",
              "  np.float64(0.01197018278479611),\n",
              "  np.float64(0.01188815700368673),\n",
              "  np.float64(0.01178250226994366),\n",
              "  np.float64(0.011685353868556883),\n",
              "  np.float64(0.011595563376570195),\n",
              "  np.float64(0.011518755557464986),\n",
              "  np.float64(0.011429833622034811),\n",
              "  np.float64(0.01133675632204841),\n",
              "  np.float64(0.011256320951133048),\n",
              "  np.float64(0.011178493325425868),\n",
              "  np.float64(0.011092004689268863),\n",
              "  np.float64(0.011013598774968695),\n",
              "  np.float64(0.010933259833762561),\n",
              "  np.float64(0.010856740862805357),\n",
              "  np.float64(0.010786017180406202),\n",
              "  np.float64(0.010705505233659714),\n",
              "  np.float64(0.01063797088804263),\n",
              "  np.float64(0.010558541752300437),\n",
              "  np.float64(0.010485904797885585),\n",
              "  np.float64(0.010418149574097993),\n",
              "  np.float64(0.010372193886553341),\n",
              "  np.float64(0.010278159060368663),\n",
              "  np.float64(0.010208860954146621),\n",
              "  np.float64(0.010140688990892887),\n",
              "  np.float64(0.010075215554740778),\n",
              "  np.float64(0.010013886837945135),\n",
              "  np.float64(0.009944308299781539),\n",
              "  np.float64(0.009881694472788029),\n",
              "  np.float64(0.009817253847471128),\n",
              "  np.float64(0.009755605665997709),\n",
              "  np.float64(0.009694741085292793),\n",
              "  np.float64(0.009632483086161674),\n",
              "  np.float64(0.009574660652954215),\n",
              "  np.float64(0.009513498177960322),\n",
              "  np.float64(0.009455315000322501),\n",
              "  np.float64(0.009395083448544256),\n",
              "  np.float64(0.009338776943226303),\n",
              "  np.float64(0.009305170493211742),\n",
              "  np.float64(0.00922852174324816),\n",
              "  np.float64(0.009177841457123775),\n",
              "  np.float64(0.009123477013100829),\n",
              "  np.float64(0.009061441285500025),\n",
              "  np.float64(0.009006310567048441),\n",
              "  np.float64(0.008951969259200545),\n",
              "  np.float64(0.008898511079695668),\n",
              "  np.float64(0.008850175708678604),\n",
              "  np.float64(0.008794061424406354),\n",
              "  np.float64(0.008744607371844518),\n",
              "  np.float64(0.008692446465293272),\n",
              "  np.float64(0.008643889182648924),\n",
              "  np.float64(0.008593494298684877),\n",
              "  np.float64(0.00854694518432956),\n",
              "  np.float64(0.008493533317362408),\n",
              "  np.float64(0.008445485131322804),\n",
              "  np.float64(0.008397693445210642),\n",
              "  np.float64(0.0083505496044251),\n",
              "  np.float64(0.008301630488797035),\n",
              "  np.float64(0.008255682747954154),\n",
              "  np.float64(0.008209342877140242),\n",
              "  np.float64(0.008164693932317037),\n",
              "  np.float64(0.008119914723580953),\n",
              "  np.float64(0.008075184401243175),\n",
              "  np.float64(0.008030307981612006),\n",
              "  np.float64(0.007985471709988172),\n",
              "  np.float64(0.007942887517048188),\n",
              "  np.float64(0.007899773625432104),\n",
              "  np.float64(0.007857758660617754),\n",
              "  np.float64(0.007815216206462142),\n",
              "  np.float64(0.007776844140545788),\n",
              "  np.float64(0.0077423274250074245),\n",
              "  np.float64(0.007699710613073892),\n",
              "  np.float64(0.007652863882152705),\n",
              "  np.float64(0.007613551250715081),\n",
              "  np.float64(0.007574978400859972),\n",
              "  np.float64(0.007537000169906921),\n",
              "  np.float64(0.007497658806899837),\n",
              "  np.float64(0.007460882799728524),\n",
              "  np.float64(0.007425128231005597),\n",
              "  np.float64(0.007385885128179468),\n",
              "  np.float64(0.007349975612804946),\n",
              "  np.float64(0.0073131401836965945),\n",
              "  np.float64(0.007276752168573986),\n",
              "  np.float64(0.007242498395843805),\n",
              "  np.float64(0.007206500054338843),\n",
              "  np.float64(0.007173089614814217),\n",
              "  np.float64(0.007136986606612905),\n",
              "  np.float64(0.007102426912413435),\n",
              "  np.float64(0.0070697276137163985),\n",
              "  np.float64(0.007035694483412859),\n",
              "  np.float64(0.0070020571825583685),\n",
              "  np.float64(0.006969495217084261),\n",
              "  np.float64(0.006936486158517295),\n",
              "  np.float64(0.006904217108445739),\n",
              "  np.float64(0.00687237717798646),\n",
              "  np.float64(0.00684145285113612),\n",
              "  np.float64(0.006809030158502687),\n",
              "  np.float64(0.00677855311972923),\n",
              "  np.float64(0.006748118505653425),\n",
              "  np.float64(0.006716829530744676),\n",
              "  np.float64(0.006687068592361557),\n",
              "  np.float64(0.0066568814790248084),\n",
              "  np.float64(0.0066267944393486),\n",
              "  np.float64(0.006597834635481722),\n",
              "  np.float64(0.006570797846856313),\n",
              "  np.float64(0.006538862234439492),\n",
              "  np.float64(0.006509856973086046),\n",
              "  np.float64(0.0064817388763105735),\n",
              "  np.float64(0.006453153935039508),\n",
              "  np.float64(0.006425086550379677),\n",
              "  np.float64(0.00639801289125471),\n",
              "  np.float64(0.006370432506071775),\n",
              "  np.float64(0.006344928568219975),\n",
              "  np.float64(0.006315115458495787),\n",
              "  np.float64(0.006288277113647007),\n",
              "  np.float64(0.006261375186264531),\n",
              "  np.float64(0.006235949262796828),\n",
              "  np.float64(0.006208588845377523),\n",
              "  np.float64(0.006182466508757815),\n",
              "  np.float64(0.006157724418467639),\n",
              "  np.float64(0.006131835183613007),\n",
              "  np.float64(0.006106484740325447),\n",
              "  np.float64(0.00608002637420485),\n",
              "  np.float64(0.006055150539633937),\n",
              "  np.float64(0.006030744914006422),\n",
              "  np.float64(0.006005356815804681),\n",
              "  np.float64(0.0059815474918202965),\n",
              "  np.float64(0.0059563786160860945),\n",
              "  np.float64(0.005932209203800986),\n",
              "  np.float64(0.005907838183945409),\n",
              "  np.float64(0.0058842505450400675),\n",
              "  np.float64(0.005859843285521336),\n",
              "  np.float64(0.0058364093200851995),\n",
              "  np.float64(0.005812746885317053),\n",
              "  np.float64(0.0057898068836295755),\n",
              "  np.float64(0.005767445417549336),\n",
              "  np.float64(0.005743518842300221),\n",
              "  np.float64(0.005720456525631282),\n",
              "  np.float64(0.005698723737464183),\n",
              "  np.float64(0.0056762583418141395),\n",
              "  np.float64(0.005653897199327525),\n",
              "  np.float64(0.005630235087540331),\n",
              "  np.float64(0.005607874731349095),\n",
              "  np.float64(0.005585938312317082),\n",
              "  np.float64(0.005565613329320214),\n",
              "  np.float64(0.005541933956654076),\n",
              "  np.float64(0.005520641024840412),\n",
              "  np.float64(0.0054985051269886166),\n",
              "  np.float64(0.005479478975485988),\n",
              "  np.float64(0.005454732688403054),\n",
              "  np.float64(0.005433326254931169),\n",
              "  np.float64(0.005412003213366873),\n",
              "  np.float64(0.005390065836508451),\n",
              "  np.float64(0.0053686117044971105),\n",
              "  np.float64(0.005346847473570504),\n",
              "  np.float64(0.005326566795290727),\n",
              "  np.float64(0.005304155891788895),\n",
              "  np.float64(0.0052826472253598485),\n",
              "  np.float64(0.0052605634498580625),\n",
              "  np.float64(0.0052390654525985335),\n",
              "  np.float64(0.005217333082296659),\n",
              "  np.float64(0.005195714823809681),\n",
              "  np.float64(0.005174084592797595),\n",
              "  np.float64(0.0051523243272966245),\n",
              "  np.float64(0.005131124428191397),\n",
              "  np.float64(0.005108970907702921),\n",
              "  np.float64(0.005087976000002941),\n",
              "  np.float64(0.0050652602833821956),\n",
              "  np.float64(0.005043967134056425),\n",
              "  np.float64(0.005023214309740786),\n",
              "  np.float64(0.005001976548865219),\n",
              "  np.float64(0.004981013808261443),\n",
              "  np.float64(0.004960717402513568),\n",
              "  np.float64(0.004941963871384926),\n",
              "  np.float64(0.004920459429530762),\n",
              "  np.float64(0.004901759333916081),\n",
              "  np.float64(0.0048812699721783055),\n",
              "  np.float64(0.004862067291088413),\n",
              "  np.float64(0.004843229304154937),\n",
              "  np.float64(0.004824252993840638),\n",
              "  np.float64(0.004805837110075987),\n",
              "  np.float64(0.00478729726603634),\n",
              "  np.float64(0.004769303358809054),\n",
              "  np.float64(0.004751419791552972),\n",
              "  np.float64(0.004733583568129665),\n",
              "  np.float64(0.0047159451530217195),\n",
              "  np.float64(0.004698274652879195),\n",
              "  np.float64(0.004681079763016091),\n",
              "  np.float64(0.004663907218850477),\n",
              "  np.float64(0.004646929047516724),\n",
              "  np.float64(0.0046302121734004414),\n",
              "  np.float64(0.004613432778836316),\n",
              "  np.float64(0.004597048875781222),\n",
              "  np.float64(0.004581423422877473),\n",
              "  np.float64(0.004564104714746344),\n",
              "  np.float64(0.004547776327289191),\n",
              "  np.float64(0.004531706616575266),\n",
              "  np.float64(0.004515765588148142),\n",
              "  np.float64(0.004499943793703048),\n",
              "  np.float64(0.004484311802352103),\n",
              "  np.float64(0.004468706495849413),\n",
              "  np.float64(0.004453240222398173),\n",
              "  np.float64(0.004438232498111458),\n",
              "  np.float64(0.0044227986069462315),\n",
              "  np.float64(0.004407434118610254),\n",
              "  np.float64(0.00439241000023391),\n",
              "  np.float64(0.004377390649225742),\n",
              "  np.float64(0.004362558867095417),\n",
              "  np.float64(0.004348208111246156),\n",
              "  np.float64(0.004333351513815025),\n",
              "  np.float64(0.004319726410348993),\n",
              "  np.float64(0.004305023602127728),\n",
              "  np.float64(0.004290110443764529),\n",
              "  np.float64(0.004275671731984722),\n",
              "  np.float64(0.004261490418164142),\n",
              "  np.float64(0.004247366504757626),\n",
              "  np.float64(0.00423335527221789),\n",
              "  np.float64(0.004219490083109391),\n",
              "  np.float64(0.004205668000242625),\n",
              "  np.float64(0.004192012978046249),\n",
              "  np.float64(0.0041784978609759865),\n",
              "  np.float64(0.004165179364854805),\n",
              "  np.float64(0.0041516029408328325),\n",
              "  np.float64(0.004138402434448656),\n",
              "  np.float64(0.004124721222565639),\n",
              "  np.float64(0.004111737712333742),\n",
              "  np.float64(0.004098755857595346),\n",
              "  np.float64(0.004085433861197219),\n",
              "  np.float64(0.0040724195794112445),\n",
              "  np.float64(0.004059582278894604),\n",
              "  np.float64(0.004047101894343415),\n",
              "  np.float64(0.004034399908405423),\n",
              "  np.float64(0.00402159759527241),\n",
              "  np.float64(0.0040089405784502555),\n",
              "  np.float64(0.003996638472903983),\n",
              "  np.float64(0.003984034247396709),\n",
              "  np.float64(0.003971886930403427),\n",
              "  np.float64(0.0039594086200926085),\n",
              "  np.float64(0.003947219316963371),\n",
              "  np.float64(0.0039353396331847906)],\n",
              " [651,\n",
              "  755,\n",
              "  825,\n",
              "  864,\n",
              "  877,\n",
              "  920,\n",
              "  929,\n",
              "  930,\n",
              "  939,\n",
              "  946,\n",
              "  957,\n",
              "  964,\n",
              "  973,\n",
              "  972,\n",
              "  977,\n",
              "  977,\n",
              "  981,\n",
              "  982,\n",
              "  986,\n",
              "  988,\n",
              "  988,\n",
              "  988,\n",
              "  991,\n",
              "  992,\n",
              "  992,\n",
              "  994,\n",
              "  993,\n",
              "  994,\n",
              "  995,\n",
              "  995,\n",
              "  996,\n",
              "  996,\n",
              "  996,\n",
              "  996,\n",
              "  996,\n",
              "  996,\n",
              "  996,\n",
              "  997,\n",
              "  998,\n",
              "  998,\n",
              "  998,\n",
              "  998,\n",
              "  997,\n",
              "  998,\n",
              "  998,\n",
              "  998,\n",
              "  998,\n",
              "  998,\n",
              "  998,\n",
              "  998,\n",
              "  998,\n",
              "  998,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 4: Using L2 Regularization"
      ],
      "metadata": {
        "id": "qWwNB9DevKm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load data\n",
        "training_data, validation_data, test_data = load_data_wrapper()\n",
        "\n",
        "# Initialize network\n",
        "net = Network([784, 30, 10], cost=CrossEntropyCost)\n",
        "net.large_weight_initializer()\n",
        "\n",
        "# Train with L2 regularization on 1000 examples\n",
        "net.SGD(training_data[:1000], 400, 10, 0.5,\n",
        "        evaluation_data=test_data,\n",
        "        lmbda=0.1,\n",
        "        monitor_evaluation_cost=True,\n",
        "        monitor_evaluation_accuracy=True,\n",
        "        monitor_training_cost=True,\n",
        "        monitor_training_accuracy=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1no35uOvK9S",
        "outputId": "19cf5fa3-a3f4-43a9-e70b-6ccb8d25c1f9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 2.8953866641040014\n",
            "Accuracy on training data: 689 / 1000\n",
            "Cost on evaluation data: 2.23381313794047\n",
            "Accuracy on evaluation data: 5791 / 10000\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 2.4922795980388854\n",
            "Accuracy on training data: 781 / 1000\n",
            "Cost on evaluation data: 1.8578071766480555\n",
            "Accuracy on evaluation data: 6688 / 10000\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 2.1619762221974304\n",
            "Accuracy on training data: 848 / 1000\n",
            "Cost on evaluation data: 1.639313396100369\n",
            "Accuracy on evaluation data: 7131 / 10000\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 1.970081800136849\n",
            "Accuracy on training data: 881 / 1000\n",
            "Cost on evaluation data: 1.5096697678438278\n",
            "Accuracy on evaluation data: 7503 / 10000\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 1.8894258463009241\n",
            "Accuracy on training data: 899 / 1000\n",
            "Cost on evaluation data: 1.4870589068329607\n",
            "Accuracy on evaluation data: 7571 / 10000\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 1.7757738480417256\n",
            "Accuracy on training data: 914 / 1000\n",
            "Cost on evaluation data: 1.4311532854817384\n",
            "Accuracy on evaluation data: 7656 / 10000\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 1.7167127969120508\n",
            "Accuracy on training data: 928 / 1000\n",
            "Cost on evaluation data: 1.42436012666953\n",
            "Accuracy on evaluation data: 7735 / 10000\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 1.612746052058902\n",
            "Accuracy on training data: 945 / 1000\n",
            "Cost on evaluation data: 1.3170151458303538\n",
            "Accuracy on evaluation data: 7991 / 10000\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 1.587489337582136\n",
            "Accuracy on training data: 953 / 1000\n",
            "Cost on evaluation data: 1.3683020768708563\n",
            "Accuracy on evaluation data: 7886 / 10000\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 1.4891893170593409\n",
            "Accuracy on training data: 962 / 1000\n",
            "Cost on evaluation data: 1.3049757569838412\n",
            "Accuracy on evaluation data: 7973 / 10000\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 1.4468596871280575\n",
            "Accuracy on training data: 960 / 1000\n",
            "Cost on evaluation data: 1.2951656321224811\n",
            "Accuracy on evaluation data: 8040 / 10000\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 1.4020809763627835\n",
            "Accuracy on training data: 973 / 1000\n",
            "Cost on evaluation data: 1.2950842331027985\n",
            "Accuracy on evaluation data: 8049 / 10000\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 1.3915461161834075\n",
            "Accuracy on training data: 969 / 1000\n",
            "Cost on evaluation data: 1.3218352380285252\n",
            "Accuracy on evaluation data: 8014 / 10000\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 1.3327333964793393\n",
            "Accuracy on training data: 979 / 1000\n",
            "Cost on evaluation data: 1.257383076836807\n",
            "Accuracy on evaluation data: 8150 / 10000\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 1.2975474465743582\n",
            "Accuracy on training data: 981 / 1000\n",
            "Cost on evaluation data: 1.257967751666464\n",
            "Accuracy on evaluation data: 8169 / 10000\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 1.2722061043374315\n",
            "Accuracy on training data: 982 / 1000\n",
            "Cost on evaluation data: 1.2379148815197436\n",
            "Accuracy on evaluation data: 8199 / 10000\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 1.244678319920999\n",
            "Accuracy on training data: 986 / 1000\n",
            "Cost on evaluation data: 1.243457634629964\n",
            "Accuracy on evaluation data: 8185 / 10000\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 1.2202467172033646\n",
            "Accuracy on training data: 989 / 1000\n",
            "Cost on evaluation data: 1.233259475079607\n",
            "Accuracy on evaluation data: 8211 / 10000\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 1.201370851175033\n",
            "Accuracy on training data: 990 / 1000\n",
            "Cost on evaluation data: 1.256568703881618\n",
            "Accuracy on evaluation data: 8171 / 10000\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 1.1831046792817421\n",
            "Accuracy on training data: 993 / 1000\n",
            "Cost on evaluation data: 1.269423278896301\n",
            "Accuracy on evaluation data: 8186 / 10000\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 1.1660863718308905\n",
            "Accuracy on training data: 992 / 1000\n",
            "Cost on evaluation data: 1.2477405216453608\n",
            "Accuracy on evaluation data: 8239 / 10000\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 1.1480681805592567\n",
            "Accuracy on training data: 996 / 1000\n",
            "Cost on evaluation data: 1.251242321703052\n",
            "Accuracy on evaluation data: 8242 / 10000\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 1.1292540464561043\n",
            "Accuracy on training data: 994 / 1000\n",
            "Cost on evaluation data: 1.2345774477007871\n",
            "Accuracy on evaluation data: 8244 / 10000\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 1.1120620110022292\n",
            "Accuracy on training data: 996 / 1000\n",
            "Cost on evaluation data: 1.2442849592080831\n",
            "Accuracy on evaluation data: 8271 / 10000\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 1.0976202272440376\n",
            "Accuracy on training data: 996 / 1000\n",
            "Cost on evaluation data: 1.2375750891637558\n",
            "Accuracy on evaluation data: 8243 / 10000\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 1.0821428597991998\n",
            "Accuracy on training data: 996 / 1000\n",
            "Cost on evaluation data: 1.2343621866485242\n",
            "Accuracy on evaluation data: 8269 / 10000\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 1.0694519158063187\n",
            "Accuracy on training data: 996 / 1000\n",
            "Cost on evaluation data: 1.2324581426868653\n",
            "Accuracy on evaluation data: 8290 / 10000\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 1.0565032708377886\n",
            "Accuracy on training data: 997 / 1000\n",
            "Cost on evaluation data: 1.249356905463297\n",
            "Accuracy on evaluation data: 8265 / 10000\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 1.0447427895251276\n",
            "Accuracy on training data: 997 / 1000\n",
            "Cost on evaluation data: 1.2344422092740273\n",
            "Accuracy on evaluation data: 8304 / 10000\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 1.029675772775532\n",
            "Accuracy on training data: 998 / 1000\n",
            "Cost on evaluation data: 1.2306143647076562\n",
            "Accuracy on evaluation data: 8306 / 10000\n",
            "Epoch 30 training complete\n",
            "Cost on training data: 1.0185447207957388\n",
            "Accuracy on training data: 997 / 1000\n",
            "Cost on evaluation data: 1.2382703442046212\n",
            "Accuracy on evaluation data: 8292 / 10000\n",
            "Epoch 31 training complete\n",
            "Cost on training data: 1.006074712548049\n",
            "Accuracy on training data: 998 / 1000\n",
            "Cost on evaluation data: 1.2441946400044148\n",
            "Accuracy on evaluation data: 8292 / 10000\n",
            "Epoch 32 training complete\n",
            "Cost on training data: 0.9949598187162297\n",
            "Accuracy on training data: 998 / 1000\n",
            "Cost on evaluation data: 1.2264666000995015\n",
            "Accuracy on evaluation data: 8327 / 10000\n",
            "Epoch 33 training complete\n",
            "Cost on training data: 0.9831159374837539\n",
            "Accuracy on training data: 998 / 1000\n",
            "Cost on evaluation data: 1.2308077554708436\n",
            "Accuracy on evaluation data: 8326 / 10000\n",
            "Epoch 34 training complete\n",
            "Cost on training data: 0.9726771591569425\n",
            "Accuracy on training data: 998 / 1000\n",
            "Cost on evaluation data: 1.2305615285455267\n",
            "Accuracy on evaluation data: 8327 / 10000\n",
            "Epoch 35 training complete\n",
            "Cost on training data: 0.9625003482351202\n",
            "Accuracy on training data: 999 / 1000\n",
            "Cost on evaluation data: 1.230452385313968\n",
            "Accuracy on evaluation data: 8325 / 10000\n",
            "Epoch 36 training complete\n",
            "Cost on training data: 0.9525127364166042\n",
            "Accuracy on training data: 999 / 1000\n",
            "Cost on evaluation data: 1.2331251771360556\n",
            "Accuracy on evaluation data: 8305 / 10000\n",
            "Epoch 37 training complete\n",
            "Cost on training data: 0.941925762664297\n",
            "Accuracy on training data: 999 / 1000\n",
            "Cost on evaluation data: 1.2354171523933766\n",
            "Accuracy on evaluation data: 8335 / 10000\n",
            "Epoch 38 training complete\n",
            "Cost on training data: 0.9322107819868515\n",
            "Accuracy on training data: 999 / 1000\n",
            "Cost on evaluation data: 1.231265731268845\n",
            "Accuracy on evaluation data: 8334 / 10000\n",
            "Epoch 39 training complete\n",
            "Cost on training data: 0.9224050765502984\n",
            "Accuracy on training data: 999 / 1000\n",
            "Cost on evaluation data: 1.2309276108407512\n",
            "Accuracy on evaluation data: 8324 / 10000\n",
            "Epoch 40 training complete\n",
            "Cost on training data: 0.9130212514483664\n",
            "Accuracy on training data: 999 / 1000\n",
            "Cost on evaluation data: 1.227241988820512\n",
            "Accuracy on evaluation data: 8335 / 10000\n",
            "Epoch 41 training complete\n",
            "Cost on training data: 0.9043530032504251\n",
            "Accuracy on training data: 999 / 1000\n",
            "Cost on evaluation data: 1.2322084399437554\n",
            "Accuracy on evaluation data: 8339 / 10000\n",
            "Epoch 42 training complete\n",
            "Cost on training data: 0.8941349533905794\n",
            "Accuracy on training data: 999 / 1000\n",
            "Cost on evaluation data: 1.2181879859107172\n",
            "Accuracy on evaluation data: 8361 / 10000\n",
            "Epoch 43 training complete\n",
            "Cost on training data: 0.8857395968903103\n",
            "Accuracy on training data: 999 / 1000\n",
            "Cost on evaluation data: 1.2132009092906888\n",
            "Accuracy on evaluation data: 8354 / 10000\n",
            "Epoch 44 training complete\n",
            "Cost on training data: 0.876358356967299\n",
            "Accuracy on training data: 999 / 1000\n",
            "Cost on evaluation data: 1.2168981384158661\n",
            "Accuracy on evaluation data: 8362 / 10000\n",
            "Epoch 45 training complete\n",
            "Cost on training data: 0.8680782118866132\n",
            "Accuracy on training data: 999 / 1000\n",
            "Cost on evaluation data: 1.2109649564401836\n",
            "Accuracy on evaluation data: 8378 / 10000\n",
            "Epoch 46 training complete\n",
            "Cost on training data: 0.8602531708369168\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.226369656661059\n",
            "Accuracy on evaluation data: 8356 / 10000\n",
            "Epoch 47 training complete\n",
            "Cost on training data: 0.8511864942715922\n",
            "Accuracy on training data: 999 / 1000\n",
            "Cost on evaluation data: 1.213712965512063\n",
            "Accuracy on evaluation data: 8366 / 10000\n",
            "Epoch 48 training complete\n",
            "Cost on training data: 0.84315435635119\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.2112544121168471\n",
            "Accuracy on evaluation data: 8395 / 10000\n",
            "Epoch 49 training complete\n",
            "Cost on training data: 0.8357657621364689\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.215982131902206\n",
            "Accuracy on evaluation data: 8376 / 10000\n",
            "Epoch 50 training complete\n",
            "Cost on training data: 0.8275871290114507\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.2059067450164096\n",
            "Accuracy on evaluation data: 8406 / 10000\n",
            "Epoch 51 training complete\n",
            "Cost on training data: 0.8200678835240804\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.2110932137022228\n",
            "Accuracy on evaluation data: 8392 / 10000\n",
            "Epoch 52 training complete\n",
            "Cost on training data: 0.8119760594653428\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.2019190505234467\n",
            "Accuracy on evaluation data: 8410 / 10000\n",
            "Epoch 53 training complete\n",
            "Cost on training data: 0.8044806539309369\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.2081167511851154\n",
            "Accuracy on evaluation data: 8408 / 10000\n",
            "Epoch 54 training complete\n",
            "Cost on training data: 0.7968924656048118\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.2033845537094892\n",
            "Accuracy on evaluation data: 8401 / 10000\n",
            "Epoch 55 training complete\n",
            "Cost on training data: 0.7895217999659317\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.2025617536249225\n",
            "Accuracy on evaluation data: 8403 / 10000\n",
            "Epoch 56 training complete\n",
            "Cost on training data: 0.7822315809644625\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1944282734872012\n",
            "Accuracy on evaluation data: 8426 / 10000\n",
            "Epoch 57 training complete\n",
            "Cost on training data: 0.7752288767634681\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.193983414320263\n",
            "Accuracy on evaluation data: 8439 / 10000\n",
            "Epoch 58 training complete\n",
            "Cost on training data: 0.7681811724172749\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1888701681387215\n",
            "Accuracy on evaluation data: 8443 / 10000\n",
            "Epoch 59 training complete\n",
            "Cost on training data: 0.761366092748753\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1876107547629375\n",
            "Accuracy on evaluation data: 8439 / 10000\n",
            "Epoch 60 training complete\n",
            "Cost on training data: 0.7543436401844064\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.191187743665512\n",
            "Accuracy on evaluation data: 8423 / 10000\n",
            "Epoch 61 training complete\n",
            "Cost on training data: 0.7475271440657031\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1888012658977414\n",
            "Accuracy on evaluation data: 8448 / 10000\n",
            "Epoch 62 training complete\n",
            "Cost on training data: 0.7409142600501277\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1861700086649904\n",
            "Accuracy on evaluation data: 8440 / 10000\n",
            "Epoch 63 training complete\n",
            "Cost on training data: 0.7345238102571497\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1879371633456794\n",
            "Accuracy on evaluation data: 8429 / 10000\n",
            "Epoch 64 training complete\n",
            "Cost on training data: 0.7279861834301439\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1794179172539268\n",
            "Accuracy on evaluation data: 8459 / 10000\n",
            "Epoch 65 training complete\n",
            "Cost on training data: 0.7214063212213135\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.184221014757604\n",
            "Accuracy on evaluation data: 8442 / 10000\n",
            "Epoch 66 training complete\n",
            "Cost on training data: 0.7152469280027501\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1833841200788622\n",
            "Accuracy on evaluation data: 8441 / 10000\n",
            "Epoch 67 training complete\n",
            "Cost on training data: 0.7088139463567456\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1717681957707042\n",
            "Accuracy on evaluation data: 8454 / 10000\n",
            "Epoch 68 training complete\n",
            "Cost on training data: 0.7025684288466086\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1791330046765336\n",
            "Accuracy on evaluation data: 8439 / 10000\n",
            "Epoch 69 training complete\n",
            "Cost on training data: 0.6964123982593315\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1704496866149097\n",
            "Accuracy on evaluation data: 8462 / 10000\n",
            "Epoch 70 training complete\n",
            "Cost on training data: 0.6901711691488606\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1708505968947007\n",
            "Accuracy on evaluation data: 8454 / 10000\n",
            "Epoch 71 training complete\n",
            "Cost on training data: 0.6842086298661838\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1709910460019908\n",
            "Accuracy on evaluation data: 8452 / 10000\n",
            "Epoch 72 training complete\n",
            "Cost on training data: 0.6783614964510328\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.169593122364095\n",
            "Accuracy on evaluation data: 8450 / 10000\n",
            "Epoch 73 training complete\n",
            "Cost on training data: 0.6723020908628643\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1701126866437608\n",
            "Accuracy on evaluation data: 8453 / 10000\n",
            "Epoch 74 training complete\n",
            "Cost on training data: 0.6664292874281913\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.164710178205944\n",
            "Accuracy on evaluation data: 8450 / 10000\n",
            "Epoch 75 training complete\n",
            "Cost on training data: 0.6608231151690457\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1586395498032533\n",
            "Accuracy on evaluation data: 8465 / 10000\n",
            "Epoch 76 training complete\n",
            "Cost on training data: 0.6550662450623262\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1558404472823323\n",
            "Accuracy on evaluation data: 8467 / 10000\n",
            "Epoch 77 training complete\n",
            "Cost on training data: 0.649330608692605\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1601791759928375\n",
            "Accuracy on evaluation data: 8470 / 10000\n",
            "Epoch 78 training complete\n",
            "Cost on training data: 0.6437095200537232\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1615750564886698\n",
            "Accuracy on evaluation data: 8473 / 10000\n",
            "Epoch 79 training complete\n",
            "Cost on training data: 0.6381620926463144\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1597549782669618\n",
            "Accuracy on evaluation data: 8474 / 10000\n",
            "Epoch 80 training complete\n",
            "Cost on training data: 0.6327169255625497\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1490130705562083\n",
            "Accuracy on evaluation data: 8480 / 10000\n",
            "Epoch 81 training complete\n",
            "Cost on training data: 0.6272816969186982\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1509402709297032\n",
            "Accuracy on evaluation data: 8480 / 10000\n",
            "Epoch 82 training complete\n",
            "Cost on training data: 0.6219948221528354\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1460278877196968\n",
            "Accuracy on evaluation data: 8481 / 10000\n",
            "Epoch 83 training complete\n",
            "Cost on training data: 0.6165195897525153\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1506338953310478\n",
            "Accuracy on evaluation data: 8483 / 10000\n",
            "Epoch 84 training complete\n",
            "Cost on training data: 0.6115908761200228\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1493334954853915\n",
            "Accuracy on evaluation data: 8482 / 10000\n",
            "Epoch 85 training complete\n",
            "Cost on training data: 0.6062867763377636\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1526245165397377\n",
            "Accuracy on evaluation data: 8482 / 10000\n",
            "Epoch 86 training complete\n",
            "Cost on training data: 0.600924087864093\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1432386800640988\n",
            "Accuracy on evaluation data: 8495 / 10000\n",
            "Epoch 87 training complete\n",
            "Cost on training data: 0.5958996538104421\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1405902198361195\n",
            "Accuracy on evaluation data: 8488 / 10000\n",
            "Epoch 88 training complete\n",
            "Cost on training data: 0.5909290016168103\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1429183419717595\n",
            "Accuracy on evaluation data: 8496 / 10000\n",
            "Epoch 89 training complete\n",
            "Cost on training data: 0.5859286202220314\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.134951565845928\n",
            "Accuracy on evaluation data: 8498 / 10000\n",
            "Epoch 90 training complete\n",
            "Cost on training data: 0.5811317598650122\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1336716812025016\n",
            "Accuracy on evaluation data: 8494 / 10000\n",
            "Epoch 91 training complete\n",
            "Cost on training data: 0.5761159480117922\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1284278141533348\n",
            "Accuracy on evaluation data: 8510 / 10000\n",
            "Epoch 92 training complete\n",
            "Cost on training data: 0.571368948388\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.136089706883153\n",
            "Accuracy on evaluation data: 8498 / 10000\n",
            "Epoch 93 training complete\n",
            "Cost on training data: 0.5666285000074224\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1297793559125417\n",
            "Accuracy on evaluation data: 8507 / 10000\n",
            "Epoch 94 training complete\n",
            "Cost on training data: 0.5617647964489523\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1310218309315925\n",
            "Accuracy on evaluation data: 8505 / 10000\n",
            "Epoch 95 training complete\n",
            "Cost on training data: 0.5571519398944765\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1299268877445061\n",
            "Accuracy on evaluation data: 8502 / 10000\n",
            "Epoch 96 training complete\n",
            "Cost on training data: 0.5525638844992286\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1267671489859339\n",
            "Accuracy on evaluation data: 8518 / 10000\n",
            "Epoch 97 training complete\n",
            "Cost on training data: 0.5479407528900918\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1223136103682247\n",
            "Accuracy on evaluation data: 8505 / 10000\n",
            "Epoch 98 training complete\n",
            "Cost on training data: 0.543507495993037\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1198097278908856\n",
            "Accuracy on evaluation data: 8519 / 10000\n",
            "Epoch 99 training complete\n",
            "Cost on training data: 0.539014003619169\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1210190684050283\n",
            "Accuracy on evaluation data: 8521 / 10000\n",
            "Epoch 100 training complete\n",
            "Cost on training data: 0.5347205715423975\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1247322000784499\n",
            "Accuracy on evaluation data: 8500 / 10000\n",
            "Epoch 101 training complete\n",
            "Cost on training data: 0.5301275572568821\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1115044249266814\n",
            "Accuracy on evaluation data: 8518 / 10000\n",
            "Epoch 102 training complete\n",
            "Cost on training data: 0.525860292562485\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1170218955021272\n",
            "Accuracy on evaluation data: 8524 / 10000\n",
            "Epoch 103 training complete\n",
            "Cost on training data: 0.5214907746502984\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1082498864844381\n",
            "Accuracy on evaluation data: 8527 / 10000\n",
            "Epoch 104 training complete\n",
            "Cost on training data: 0.5172405712349242\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1086553433925694\n",
            "Accuracy on evaluation data: 8526 / 10000\n",
            "Epoch 105 training complete\n",
            "Cost on training data: 0.5130647050088408\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1104000303253303\n",
            "Accuracy on evaluation data: 8523 / 10000\n",
            "Epoch 106 training complete\n",
            "Cost on training data: 0.5088971133248387\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.10862920589249\n",
            "Accuracy on evaluation data: 8520 / 10000\n",
            "Epoch 107 training complete\n",
            "Cost on training data: 0.5047852283037267\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.1027310945638373\n",
            "Accuracy on evaluation data: 8526 / 10000\n",
            "Epoch 108 training complete\n",
            "Cost on training data: 0.500604425940707\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0996563606168024\n",
            "Accuracy on evaluation data: 8540 / 10000\n",
            "Epoch 109 training complete\n",
            "Cost on training data: 0.4966253792434686\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0958739265192583\n",
            "Accuracy on evaluation data: 8536 / 10000\n",
            "Epoch 110 training complete\n",
            "Cost on training data: 0.4925968061915725\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0956706663947644\n",
            "Accuracy on evaluation data: 8533 / 10000\n",
            "Epoch 111 training complete\n",
            "Cost on training data: 0.4885863514705011\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0948275001118917\n",
            "Accuracy on evaluation data: 8538 / 10000\n",
            "Epoch 112 training complete\n",
            "Cost on training data: 0.48461123029108943\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0944678481037924\n",
            "Accuracy on evaluation data: 8533 / 10000\n",
            "Epoch 113 training complete\n",
            "Cost on training data: 0.480724447890328\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0916562493379423\n",
            "Accuracy on evaluation data: 8536 / 10000\n",
            "Epoch 114 training complete\n",
            "Cost on training data: 0.4769394503863549\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0906490973348244\n",
            "Accuracy on evaluation data: 8546 / 10000\n",
            "Epoch 115 training complete\n",
            "Cost on training data: 0.47312063525173687\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0932005328471894\n",
            "Accuracy on evaluation data: 8543 / 10000\n",
            "Epoch 116 training complete\n",
            "Cost on training data: 0.46941700902433864\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0856811410163885\n",
            "Accuracy on evaluation data: 8540 / 10000\n",
            "Epoch 117 training complete\n",
            "Cost on training data: 0.465619545009392\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0797978868866425\n",
            "Accuracy on evaluation data: 8544 / 10000\n",
            "Epoch 118 training complete\n",
            "Cost on training data: 0.4619320917226021\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0866932680089958\n",
            "Accuracy on evaluation data: 8539 / 10000\n",
            "Epoch 119 training complete\n",
            "Cost on training data: 0.4582756164413213\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.076515302580667\n",
            "Accuracy on evaluation data: 8551 / 10000\n",
            "Epoch 120 training complete\n",
            "Cost on training data: 0.45459438532671553\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0752142484052474\n",
            "Accuracy on evaluation data: 8561 / 10000\n",
            "Epoch 121 training complete\n",
            "Cost on training data: 0.4511285145524544\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0801232064649176\n",
            "Accuracy on evaluation data: 8549 / 10000\n",
            "Epoch 122 training complete\n",
            "Cost on training data: 0.44747304023125456\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0755342521147715\n",
            "Accuracy on evaluation data: 8554 / 10000\n",
            "Epoch 123 training complete\n",
            "Cost on training data: 0.4440889413612969\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0676871546426485\n",
            "Accuracy on evaluation data: 8553 / 10000\n",
            "Epoch 124 training complete\n",
            "Cost on training data: 0.4405456361476836\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0761269423164685\n",
            "Accuracy on evaluation data: 8545 / 10000\n",
            "Epoch 125 training complete\n",
            "Cost on training data: 0.4370602530529241\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0685751199624671\n",
            "Accuracy on evaluation data: 8574 / 10000\n",
            "Epoch 126 training complete\n",
            "Cost on training data: 0.43369599469057357\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0735501786981345\n",
            "Accuracy on evaluation data: 8550 / 10000\n",
            "Epoch 127 training complete\n",
            "Cost on training data: 0.4302965015069004\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0643903645291566\n",
            "Accuracy on evaluation data: 8563 / 10000\n",
            "Epoch 128 training complete\n",
            "Cost on training data: 0.4269507681647259\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0636064492151494\n",
            "Accuracy on evaluation data: 8556 / 10000\n",
            "Epoch 129 training complete\n",
            "Cost on training data: 0.4238265000552506\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0633690423950541\n",
            "Accuracy on evaluation data: 8575 / 10000\n",
            "Epoch 130 training complete\n",
            "Cost on training data: 0.4204597967497097\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0664562623218943\n",
            "Accuracy on evaluation data: 8570 / 10000\n",
            "Epoch 131 training complete\n",
            "Cost on training data: 0.4170590385735745\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0600797733588634\n",
            "Accuracy on evaluation data: 8572 / 10000\n",
            "Epoch 132 training complete\n",
            "Cost on training data: 0.4140070207315619\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0550897414847886\n",
            "Accuracy on evaluation data: 8567 / 10000\n",
            "Epoch 133 training complete\n",
            "Cost on training data: 0.4107872065764531\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0513069193064248\n",
            "Accuracy on evaluation data: 8583 / 10000\n",
            "Epoch 134 training complete\n",
            "Cost on training data: 0.4077688268300945\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0580538995233195\n",
            "Accuracy on evaluation data: 8556 / 10000\n",
            "Epoch 135 training complete\n",
            "Cost on training data: 0.4045345680196393\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.054879467575505\n",
            "Accuracy on evaluation data: 8573 / 10000\n",
            "Epoch 136 training complete\n",
            "Cost on training data: 0.4014822156454012\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0445005603513926\n",
            "Accuracy on evaluation data: 8582 / 10000\n",
            "Epoch 137 training complete\n",
            "Cost on training data: 0.3983431123561391\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0491484491563865\n",
            "Accuracy on evaluation data: 8567 / 10000\n",
            "Epoch 138 training complete\n",
            "Cost on training data: 0.39534568925196545\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0526132779993977\n",
            "Accuracy on evaluation data: 8564 / 10000\n",
            "Epoch 139 training complete\n",
            "Cost on training data: 0.3925585801810093\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0466473763293\n",
            "Accuracy on evaluation data: 8580 / 10000\n",
            "Epoch 140 training complete\n",
            "Cost on training data: 0.38932889308940216\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.041888104387314\n",
            "Accuracy on evaluation data: 8581 / 10000\n",
            "Epoch 141 training complete\n",
            "Cost on training data: 0.3864245361382763\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.041208237538272\n",
            "Accuracy on evaluation data: 8570 / 10000\n",
            "Epoch 142 training complete\n",
            "Cost on training data: 0.38352201844085043\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0394582226041023\n",
            "Accuracy on evaluation data: 8576 / 10000\n",
            "Epoch 143 training complete\n",
            "Cost on training data: 0.38084426427829554\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.040400453705019\n",
            "Accuracy on evaluation data: 8574 / 10000\n",
            "Epoch 144 training complete\n",
            "Cost on training data: 0.3778002167026503\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.036591579030419\n",
            "Accuracy on evaluation data: 8587 / 10000\n",
            "Epoch 145 training complete\n",
            "Cost on training data: 0.3750011295700585\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0354573729160639\n",
            "Accuracy on evaluation data: 8585 / 10000\n",
            "Epoch 146 training complete\n",
            "Cost on training data: 0.3723309940522808\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0320102822287522\n",
            "Accuracy on evaluation data: 8588 / 10000\n",
            "Epoch 147 training complete\n",
            "Cost on training data: 0.36958783089156433\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0342252585595149\n",
            "Accuracy on evaluation data: 8598 / 10000\n",
            "Epoch 148 training complete\n",
            "Cost on training data: 0.3667131112201757\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0316930723438378\n",
            "Accuracy on evaluation data: 8595 / 10000\n",
            "Epoch 149 training complete\n",
            "Cost on training data: 0.3639675926076163\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0307080351956857\n",
            "Accuracy on evaluation data: 8597 / 10000\n",
            "Epoch 150 training complete\n",
            "Cost on training data: 0.361261603927798\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.024511200742775\n",
            "Accuracy on evaluation data: 8599 / 10000\n",
            "Epoch 151 training complete\n",
            "Cost on training data: 0.35859856705009246\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0273302935492181\n",
            "Accuracy on evaluation data: 8586 / 10000\n",
            "Epoch 152 training complete\n",
            "Cost on training data: 0.3559977063831139\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0256251883234804\n",
            "Accuracy on evaluation data: 8587 / 10000\n",
            "Epoch 153 training complete\n",
            "Cost on training data: 0.3534517356575809\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0273283045299915\n",
            "Accuracy on evaluation data: 8602 / 10000\n",
            "Epoch 154 training complete\n",
            "Cost on training data: 0.3508627769016752\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0260316343169018\n",
            "Accuracy on evaluation data: 8584 / 10000\n",
            "Epoch 155 training complete\n",
            "Cost on training data: 0.348322384560445\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0208336954536532\n",
            "Accuracy on evaluation data: 8606 / 10000\n",
            "Epoch 156 training complete\n",
            "Cost on training data: 0.34583268391464667\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0236237485395197\n",
            "Accuracy on evaluation data: 8591 / 10000\n",
            "Epoch 157 training complete\n",
            "Cost on training data: 0.34324145668635214\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0163666068126074\n",
            "Accuracy on evaluation data: 8605 / 10000\n",
            "Epoch 158 training complete\n",
            "Cost on training data: 0.34081161309782504\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0229591063552612\n",
            "Accuracy on evaluation data: 8597 / 10000\n",
            "Epoch 159 training complete\n",
            "Cost on training data: 0.33835728955699024\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.015982804870418\n",
            "Accuracy on evaluation data: 8581 / 10000\n",
            "Epoch 160 training complete\n",
            "Cost on training data: 0.3359738867755381\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0149074921328267\n",
            "Accuracy on evaluation data: 8605 / 10000\n",
            "Epoch 161 training complete\n",
            "Cost on training data: 0.3335583575956708\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0129520560177143\n",
            "Accuracy on evaluation data: 8598 / 10000\n",
            "Epoch 162 training complete\n",
            "Cost on training data: 0.33099467199821403\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0093673200110276\n",
            "Accuracy on evaluation data: 8613 / 10000\n",
            "Epoch 163 training complete\n",
            "Cost on training data: 0.32880524981694265\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0134672316662687\n",
            "Accuracy on evaluation data: 8605 / 10000\n",
            "Epoch 164 training complete\n",
            "Cost on training data: 0.32633271572783\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0077418617233964\n",
            "Accuracy on evaluation data: 8614 / 10000\n",
            "Epoch 165 training complete\n",
            "Cost on training data: 0.32408301648580845\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0046468297262234\n",
            "Accuracy on evaluation data: 8613 / 10000\n",
            "Epoch 166 training complete\n",
            "Cost on training data: 0.3217857196940037\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0037619612215842\n",
            "Accuracy on evaluation data: 8617 / 10000\n",
            "Epoch 167 training complete\n",
            "Cost on training data: 0.31948225021237636\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0037139418898917\n",
            "Accuracy on evaluation data: 8620 / 10000\n",
            "Epoch 168 training complete\n",
            "Cost on training data: 0.31719796563289554\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0020904328547702\n",
            "Accuracy on evaluation data: 8619 / 10000\n",
            "Epoch 169 training complete\n",
            "Cost on training data: 0.3150347032642422\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 1.0029727675606759\n",
            "Accuracy on evaluation data: 8624 / 10000\n",
            "Epoch 170 training complete\n",
            "Cost on training data: 0.3126964605057805\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9983961736603311\n",
            "Accuracy on evaluation data: 8626 / 10000\n",
            "Epoch 171 training complete\n",
            "Cost on training data: 0.31053281268813726\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9984482236234222\n",
            "Accuracy on evaluation data: 8627 / 10000\n",
            "Epoch 172 training complete\n",
            "Cost on training data: 0.30852295231757315\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9980504615226646\n",
            "Accuracy on evaluation data: 8631 / 10000\n",
            "Epoch 173 training complete\n",
            "Cost on training data: 0.3064196052270224\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9966898475485679\n",
            "Accuracy on evaluation data: 8634 / 10000\n",
            "Epoch 174 training complete\n",
            "Cost on training data: 0.30405802105584356\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9933194018758037\n",
            "Accuracy on evaluation data: 8628 / 10000\n",
            "Epoch 175 training complete\n",
            "Cost on training data: 0.3023623556947479\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9958601677500079\n",
            "Accuracy on evaluation data: 8627 / 10000\n",
            "Epoch 176 training complete\n",
            "Cost on training data: 0.2999679219808967\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9912423556600048\n",
            "Accuracy on evaluation data: 8639 / 10000\n",
            "Epoch 177 training complete\n",
            "Cost on training data: 0.29796470446991574\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9912313529917537\n",
            "Accuracy on evaluation data: 8638 / 10000\n",
            "Epoch 178 training complete\n",
            "Cost on training data: 0.2957611548168559\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9876153745394035\n",
            "Accuracy on evaluation data: 8632 / 10000\n",
            "Epoch 179 training complete\n",
            "Cost on training data: 0.29378409168662856\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9852464550113817\n",
            "Accuracy on evaluation data: 8642 / 10000\n",
            "Epoch 180 training complete\n",
            "Cost on training data: 0.29182689246553106\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9851494526839423\n",
            "Accuracy on evaluation data: 8646 / 10000\n",
            "Epoch 181 training complete\n",
            "Cost on training data: 0.28976679704325203\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9854745516813888\n",
            "Accuracy on evaluation data: 8651 / 10000\n",
            "Epoch 182 training complete\n",
            "Cost on training data: 0.2879534973809882\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9899832889379796\n",
            "Accuracy on evaluation data: 8625 / 10000\n",
            "Epoch 183 training complete\n",
            "Cost on training data: 0.28587190444301136\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9827780140468239\n",
            "Accuracy on evaluation data: 8635 / 10000\n",
            "Epoch 184 training complete\n",
            "Cost on training data: 0.2838623459055237\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9772972336616752\n",
            "Accuracy on evaluation data: 8643 / 10000\n",
            "Epoch 185 training complete\n",
            "Cost on training data: 0.28225624832406193\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9795260150485857\n",
            "Accuracy on evaluation data: 8656 / 10000\n",
            "Epoch 186 training complete\n",
            "Cost on training data: 0.2801312476181282\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9812624255396147\n",
            "Accuracy on evaluation data: 8635 / 10000\n",
            "Epoch 187 training complete\n",
            "Cost on training data: 0.278274193273612\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.977602386637912\n",
            "Accuracy on evaluation data: 8647 / 10000\n",
            "Epoch 188 training complete\n",
            "Cost on training data: 0.2764420086712167\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9754867083067064\n",
            "Accuracy on evaluation data: 8652 / 10000\n",
            "Epoch 189 training complete\n",
            "Cost on training data: 0.27448829485897047\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9739359796296361\n",
            "Accuracy on evaluation data: 8651 / 10000\n",
            "Epoch 190 training complete\n",
            "Cost on training data: 0.2726872678525152\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9733897849669881\n",
            "Accuracy on evaluation data: 8649 / 10000\n",
            "Epoch 191 training complete\n",
            "Cost on training data: 0.2709002970571852\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9714967245596274\n",
            "Accuracy on evaluation data: 8651 / 10000\n",
            "Epoch 192 training complete\n",
            "Cost on training data: 0.2693291031789611\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9687464490430199\n",
            "Accuracy on evaluation data: 8659 / 10000\n",
            "Epoch 193 training complete\n",
            "Cost on training data: 0.26742209204369427\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9675207135123466\n",
            "Accuracy on evaluation data: 8667 / 10000\n",
            "Epoch 194 training complete\n",
            "Cost on training data: 0.26572867489148855\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9724547406996101\n",
            "Accuracy on evaluation data: 8648 / 10000\n",
            "Epoch 195 training complete\n",
            "Cost on training data: 0.26393886936363276\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9682665950602847\n",
            "Accuracy on evaluation data: 8653 / 10000\n",
            "Epoch 196 training complete\n",
            "Cost on training data: 0.2622163253577332\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9634415868662933\n",
            "Accuracy on evaluation data: 8675 / 10000\n",
            "Epoch 197 training complete\n",
            "Cost on training data: 0.2606605106341803\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9666053131735165\n",
            "Accuracy on evaluation data: 8647 / 10000\n",
            "Epoch 198 training complete\n",
            "Cost on training data: 0.25899769460972744\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9649353527652191\n",
            "Accuracy on evaluation data: 8652 / 10000\n",
            "Epoch 199 training complete\n",
            "Cost on training data: 0.25720694908236436\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9641641535323175\n",
            "Accuracy on evaluation data: 8664 / 10000\n",
            "Epoch 200 training complete\n",
            "Cost on training data: 0.2555168616475448\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9638092685749537\n",
            "Accuracy on evaluation data: 8661 / 10000\n",
            "Epoch 201 training complete\n",
            "Cost on training data: 0.25387921020858767\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9607341085924664\n",
            "Accuracy on evaluation data: 8669 / 10000\n",
            "Epoch 202 training complete\n",
            "Cost on training data: 0.2524666693644133\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9647232644890867\n",
            "Accuracy on evaluation data: 8651 / 10000\n",
            "Epoch 203 training complete\n",
            "Cost on training data: 0.2510680937687893\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9695885760042101\n",
            "Accuracy on evaluation data: 8635 / 10000\n",
            "Epoch 204 training complete\n",
            "Cost on training data: 0.24910307442199006\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9590732871643047\n",
            "Accuracy on evaluation data: 8680 / 10000\n",
            "Epoch 205 training complete\n",
            "Cost on training data: 0.247488365140716\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9568687736587742\n",
            "Accuracy on evaluation data: 8671 / 10000\n",
            "Epoch 206 training complete\n",
            "Cost on training data: 0.24602966839016635\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9507523323869675\n",
            "Accuracy on evaluation data: 8685 / 10000\n",
            "Epoch 207 training complete\n",
            "Cost on training data: 0.24443905967084026\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9564131724048358\n",
            "Accuracy on evaluation data: 8673 / 10000\n",
            "Epoch 208 training complete\n",
            "Cost on training data: 0.2428842477941346\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9561492094792736\n",
            "Accuracy on evaluation data: 8674 / 10000\n",
            "Epoch 209 training complete\n",
            "Cost on training data: 0.24149864829321055\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.952786134800699\n",
            "Accuracy on evaluation data: 8676 / 10000\n",
            "Epoch 210 training complete\n",
            "Cost on training data: 0.24003181090818057\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9554712889349509\n",
            "Accuracy on evaluation data: 8667 / 10000\n",
            "Epoch 211 training complete\n",
            "Cost on training data: 0.23844733550222247\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.954315240839431\n",
            "Accuracy on evaluation data: 8676 / 10000\n",
            "Epoch 212 training complete\n",
            "Cost on training data: 0.2369816314351822\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9520476440380687\n",
            "Accuracy on evaluation data: 8672 / 10000\n",
            "Epoch 213 training complete\n",
            "Cost on training data: 0.23556746014499413\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9451105873629928\n",
            "Accuracy on evaluation data: 8689 / 10000\n",
            "Epoch 214 training complete\n",
            "Cost on training data: 0.23425000056336692\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9438314852795101\n",
            "Accuracy on evaluation data: 8679 / 10000\n",
            "Epoch 215 training complete\n",
            "Cost on training data: 0.2329269676893306\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9525833700864114\n",
            "Accuracy on evaluation data: 8667 / 10000\n",
            "Epoch 216 training complete\n",
            "Cost on training data: 0.23120627907216837\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9465171579437679\n",
            "Accuracy on evaluation data: 8691 / 10000\n",
            "Epoch 217 training complete\n",
            "Cost on training data: 0.22986437132790646\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.942595150595273\n",
            "Accuracy on evaluation data: 8690 / 10000\n",
            "Epoch 218 training complete\n",
            "Cost on training data: 0.22846062112091023\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9448468390038404\n",
            "Accuracy on evaluation data: 8682 / 10000\n",
            "Epoch 219 training complete\n",
            "Cost on training data: 0.22715013091031855\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9447236825703795\n",
            "Accuracy on evaluation data: 8674 / 10000\n",
            "Epoch 220 training complete\n",
            "Cost on training data: 0.22579461101110082\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9416219230213395\n",
            "Accuracy on evaluation data: 8684 / 10000\n",
            "Epoch 221 training complete\n",
            "Cost on training data: 0.22450615892207587\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9396566318331718\n",
            "Accuracy on evaluation data: 8690 / 10000\n",
            "Epoch 222 training complete\n",
            "Cost on training data: 0.22315715239558542\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9421117126278145\n",
            "Accuracy on evaluation data: 8684 / 10000\n",
            "Epoch 223 training complete\n",
            "Cost on training data: 0.2218680887284607\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9444952970845528\n",
            "Accuracy on evaluation data: 8682 / 10000\n",
            "Epoch 224 training complete\n",
            "Cost on training data: 0.22058074975126696\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9347761052416744\n",
            "Accuracy on evaluation data: 8698 / 10000\n",
            "Epoch 225 training complete\n",
            "Cost on training data: 0.21927649348701583\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9391529207458551\n",
            "Accuracy on evaluation data: 8685 / 10000\n",
            "Epoch 226 training complete\n",
            "Cost on training data: 0.2179584876897378\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9367633418734309\n",
            "Accuracy on evaluation data: 8698 / 10000\n",
            "Epoch 227 training complete\n",
            "Cost on training data: 0.21678505027401054\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9355534170164717\n",
            "Accuracy on evaluation data: 8684 / 10000\n",
            "Epoch 228 training complete\n",
            "Cost on training data: 0.21560596481550798\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9368090293675461\n",
            "Accuracy on evaluation data: 8682 / 10000\n",
            "Epoch 229 training complete\n",
            "Cost on training data: 0.2143159034190677\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9338300192652639\n",
            "Accuracy on evaluation data: 8692 / 10000\n",
            "Epoch 230 training complete\n",
            "Cost on training data: 0.21310156108383937\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9368415045062056\n",
            "Accuracy on evaluation data: 8687 / 10000\n",
            "Epoch 231 training complete\n",
            "Cost on training data: 0.21181415042268972\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9324877515214856\n",
            "Accuracy on evaluation data: 8694 / 10000\n",
            "Epoch 232 training complete\n",
            "Cost on training data: 0.2107209469672005\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9324871326039087\n",
            "Accuracy on evaluation data: 8691 / 10000\n",
            "Epoch 233 training complete\n",
            "Cost on training data: 0.2094106545086832\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.932045391253745\n",
            "Accuracy on evaluation data: 8700 / 10000\n",
            "Epoch 234 training complete\n",
            "Cost on training data: 0.20830126723879652\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9286058654560793\n",
            "Accuracy on evaluation data: 8693 / 10000\n",
            "Epoch 235 training complete\n",
            "Cost on training data: 0.20735162228524945\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.936668588862639\n",
            "Accuracy on evaluation data: 8675 / 10000\n",
            "Epoch 236 training complete\n",
            "Cost on training data: 0.2060904101423051\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.930730332107528\n",
            "Accuracy on evaluation data: 8688 / 10000\n",
            "Epoch 237 training complete\n",
            "Cost on training data: 0.20476384082069152\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9229533905711206\n",
            "Accuracy on evaluation data: 8703 / 10000\n",
            "Epoch 238 training complete\n",
            "Cost on training data: 0.20384004026728453\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9272419093105022\n",
            "Accuracy on evaluation data: 8704 / 10000\n",
            "Epoch 239 training complete\n",
            "Cost on training data: 0.20257047024934072\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9268978141037126\n",
            "Accuracy on evaluation data: 8704 / 10000\n",
            "Epoch 240 training complete\n",
            "Cost on training data: 0.20149972554306553\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9223109530926343\n",
            "Accuracy on evaluation data: 8692 / 10000\n",
            "Epoch 241 training complete\n",
            "Cost on training data: 0.20036846908559894\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.922776249066881\n",
            "Accuracy on evaluation data: 8696 / 10000\n",
            "Epoch 242 training complete\n",
            "Cost on training data: 0.19960621601537784\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9239849106282955\n",
            "Accuracy on evaluation data: 8704 / 10000\n",
            "Epoch 243 training complete\n",
            "Cost on training data: 0.19818111118771226\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9217481278049661\n",
            "Accuracy on evaluation data: 8712 / 10000\n",
            "Epoch 244 training complete\n",
            "Cost on training data: 0.19722728702008757\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9235829723129406\n",
            "Accuracy on evaluation data: 8713 / 10000\n",
            "Epoch 245 training complete\n",
            "Cost on training data: 0.19618707528171572\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9217208400641957\n",
            "Accuracy on evaluation data: 8708 / 10000\n",
            "Epoch 246 training complete\n",
            "Cost on training data: 0.19509104252358378\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9225537566919365\n",
            "Accuracy on evaluation data: 8700 / 10000\n",
            "Epoch 247 training complete\n",
            "Cost on training data: 0.19410312567323582\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9200765889948038\n",
            "Accuracy on evaluation data: 8705 / 10000\n",
            "Epoch 248 training complete\n",
            "Cost on training data: 0.19309525291998234\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9152444102074252\n",
            "Accuracy on evaluation data: 8713 / 10000\n",
            "Epoch 249 training complete\n",
            "Cost on training data: 0.1921139586178192\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9197894298231919\n",
            "Accuracy on evaluation data: 8701 / 10000\n",
            "Epoch 250 training complete\n",
            "Cost on training data: 0.19111117110283238\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9186624874586558\n",
            "Accuracy on evaluation data: 8714 / 10000\n",
            "Epoch 251 training complete\n",
            "Cost on training data: 0.1899998588426611\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9208779952246396\n",
            "Accuracy on evaluation data: 8714 / 10000\n",
            "Epoch 252 training complete\n",
            "Cost on training data: 0.18909519758260854\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9139532036053704\n",
            "Accuracy on evaluation data: 8709 / 10000\n",
            "Epoch 253 training complete\n",
            "Cost on training data: 0.1881494985341675\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9148077175305772\n",
            "Accuracy on evaluation data: 8717 / 10000\n",
            "Epoch 254 training complete\n",
            "Cost on training data: 0.18731962344611378\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9209301566029329\n",
            "Accuracy on evaluation data: 8701 / 10000\n",
            "Epoch 255 training complete\n",
            "Cost on training data: 0.1862563565509251\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9157554638943763\n",
            "Accuracy on evaluation data: 8705 / 10000\n",
            "Epoch 256 training complete\n",
            "Cost on training data: 0.1853702911627573\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.916555197108018\n",
            "Accuracy on evaluation data: 8706 / 10000\n",
            "Epoch 257 training complete\n",
            "Cost on training data: 0.18434745542766223\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9130837367132254\n",
            "Accuracy on evaluation data: 8717 / 10000\n",
            "Epoch 258 training complete\n",
            "Cost on training data: 0.183373311075825\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9113925684587308\n",
            "Accuracy on evaluation data: 8721 / 10000\n",
            "Epoch 259 training complete\n",
            "Cost on training data: 0.18263297972251527\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9093169688906771\n",
            "Accuracy on evaluation data: 8719 / 10000\n",
            "Epoch 260 training complete\n",
            "Cost on training data: 0.1816670713571194\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9088854855064433\n",
            "Accuracy on evaluation data: 8725 / 10000\n",
            "Epoch 261 training complete\n",
            "Cost on training data: 0.18102637366026422\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9153847957137105\n",
            "Accuracy on evaluation data: 8700 / 10000\n",
            "Epoch 262 training complete\n",
            "Cost on training data: 0.18003899920990726\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9176218231167736\n",
            "Accuracy on evaluation data: 8696 / 10000\n",
            "Epoch 263 training complete\n",
            "Cost on training data: 0.17893545386631005\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9153777253847887\n",
            "Accuracy on evaluation data: 8711 / 10000\n",
            "Epoch 264 training complete\n",
            "Cost on training data: 0.17803800835695086\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9058832602462504\n",
            "Accuracy on evaluation data: 8731 / 10000\n",
            "Epoch 265 training complete\n",
            "Cost on training data: 0.17724292571562006\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9090608989584996\n",
            "Accuracy on evaluation data: 8708 / 10000\n",
            "Epoch 266 training complete\n",
            "Cost on training data: 0.17638555353850627\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9075780805246967\n",
            "Accuracy on evaluation data: 8713 / 10000\n",
            "Epoch 267 training complete\n",
            "Cost on training data: 0.17552490813473035\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9097259410466463\n",
            "Accuracy on evaluation data: 8710 / 10000\n",
            "Epoch 268 training complete\n",
            "Cost on training data: 0.1746431354240359\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9040219702442139\n",
            "Accuracy on evaluation data: 8719 / 10000\n",
            "Epoch 269 training complete\n",
            "Cost on training data: 0.17395874181646967\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9040339172292926\n",
            "Accuracy on evaluation data: 8712 / 10000\n",
            "Epoch 270 training complete\n",
            "Cost on training data: 0.1731586279630932\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9067426198622827\n",
            "Accuracy on evaluation data: 8721 / 10000\n",
            "Epoch 271 training complete\n",
            "Cost on training data: 0.17220596080228118\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9021100691281603\n",
            "Accuracy on evaluation data: 8729 / 10000\n",
            "Epoch 272 training complete\n",
            "Cost on training data: 0.17146012490666374\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9012033885444908\n",
            "Accuracy on evaluation data: 8734 / 10000\n",
            "Epoch 273 training complete\n",
            "Cost on training data: 0.17074425049979083\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9106029572643762\n",
            "Accuracy on evaluation data: 8711 / 10000\n",
            "Epoch 274 training complete\n",
            "Cost on training data: 0.16986747593682888\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8960755043584023\n",
            "Accuracy on evaluation data: 8732 / 10000\n",
            "Epoch 275 training complete\n",
            "Cost on training data: 0.1693415713592\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9037739936746654\n",
            "Accuracy on evaluation data: 8715 / 10000\n",
            "Epoch 276 training complete\n",
            "Cost on training data: 0.1682543117648992\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9011978966329464\n",
            "Accuracy on evaluation data: 8725 / 10000\n",
            "Epoch 277 training complete\n",
            "Cost on training data: 0.1676521326296229\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9060788615733627\n",
            "Accuracy on evaluation data: 8728 / 10000\n",
            "Epoch 278 training complete\n",
            "Cost on training data: 0.16679025088594954\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9023423978537171\n",
            "Accuracy on evaluation data: 8724 / 10000\n",
            "Epoch 279 training complete\n",
            "Cost on training data: 0.16618340688657873\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8998259496482847\n",
            "Accuracy on evaluation data: 8741 / 10000\n",
            "Epoch 280 training complete\n",
            "Cost on training data: 0.16547488618641462\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.897266111227457\n",
            "Accuracy on evaluation data: 8718 / 10000\n",
            "Epoch 281 training complete\n",
            "Cost on training data: 0.16479353786406675\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9066706765356962\n",
            "Accuracy on evaluation data: 8725 / 10000\n",
            "Epoch 282 training complete\n",
            "Cost on training data: 0.16393897403958727\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8926838057813254\n",
            "Accuracy on evaluation data: 8740 / 10000\n",
            "Epoch 283 training complete\n",
            "Cost on training data: 0.16317068219200734\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9009454536842314\n",
            "Accuracy on evaluation data: 8718 / 10000\n",
            "Epoch 284 training complete\n",
            "Cost on training data: 0.16247045404312552\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8957796595993239\n",
            "Accuracy on evaluation data: 8742 / 10000\n",
            "Epoch 285 training complete\n",
            "Cost on training data: 0.16184721714632191\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8997191519756288\n",
            "Accuracy on evaluation data: 8745 / 10000\n",
            "Epoch 286 training complete\n",
            "Cost on training data: 0.1611014454064717\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8923445295529425\n",
            "Accuracy on evaluation data: 8735 / 10000\n",
            "Epoch 287 training complete\n",
            "Cost on training data: 0.1603705501664093\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8950432939234149\n",
            "Accuracy on evaluation data: 8738 / 10000\n",
            "Epoch 288 training complete\n",
            "Cost on training data: 0.15957017230071685\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8929655042292008\n",
            "Accuracy on evaluation data: 8725 / 10000\n",
            "Epoch 289 training complete\n",
            "Cost on training data: 0.15899652869629038\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8969849118586438\n",
            "Accuracy on evaluation data: 8732 / 10000\n",
            "Epoch 290 training complete\n",
            "Cost on training data: 0.15827431095880581\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8901799875690688\n",
            "Accuracy on evaluation data: 8730 / 10000\n",
            "Epoch 291 training complete\n",
            "Cost on training data: 0.15771844882790104\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.887504652850806\n",
            "Accuracy on evaluation data: 8755 / 10000\n",
            "Epoch 292 training complete\n",
            "Cost on training data: 0.15714057181140872\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.9000609761327453\n",
            "Accuracy on evaluation data: 8723 / 10000\n",
            "Epoch 293 training complete\n",
            "Cost on training data: 0.15635788638922118\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8885516727445999\n",
            "Accuracy on evaluation data: 8745 / 10000\n",
            "Epoch 294 training complete\n",
            "Cost on training data: 0.15578808676125983\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8938940423564085\n",
            "Accuracy on evaluation data: 8724 / 10000\n",
            "Epoch 295 training complete\n",
            "Cost on training data: 0.15504110869591697\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8935941211347652\n",
            "Accuracy on evaluation data: 8735 / 10000\n",
            "Epoch 296 training complete\n",
            "Cost on training data: 0.15446760894345218\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8967627265333417\n",
            "Accuracy on evaluation data: 8734 / 10000\n",
            "Epoch 297 training complete\n",
            "Cost on training data: 0.1539187913523163\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8903488959019646\n",
            "Accuracy on evaluation data: 8734 / 10000\n",
            "Epoch 298 training complete\n",
            "Cost on training data: 0.15317458380346863\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8904512048841041\n",
            "Accuracy on evaluation data: 8734 / 10000\n",
            "Epoch 299 training complete\n",
            "Cost on training data: 0.15249511996120754\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8928622548662912\n",
            "Accuracy on evaluation data: 8731 / 10000\n",
            "Epoch 300 training complete\n",
            "Cost on training data: 0.15197376255709838\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8905449983334723\n",
            "Accuracy on evaluation data: 8741 / 10000\n",
            "Epoch 301 training complete\n",
            "Cost on training data: 0.1514035025527707\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8863922223593935\n",
            "Accuracy on evaluation data: 8739 / 10000\n",
            "Epoch 302 training complete\n",
            "Cost on training data: 0.15078898822798853\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8882790885842334\n",
            "Accuracy on evaluation data: 8744 / 10000\n",
            "Epoch 303 training complete\n",
            "Cost on training data: 0.15015663282691896\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8889727813120496\n",
            "Accuracy on evaluation data: 8739 / 10000\n",
            "Epoch 304 training complete\n",
            "Cost on training data: 0.14959297167608976\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8868456897070973\n",
            "Accuracy on evaluation data: 8729 / 10000\n",
            "Epoch 305 training complete\n",
            "Cost on training data: 0.14904104461510365\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.889817295548033\n",
            "Accuracy on evaluation data: 8743 / 10000\n",
            "Epoch 306 training complete\n",
            "Cost on training data: 0.14846072539597982\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8877566832220003\n",
            "Accuracy on evaluation data: 8746 / 10000\n",
            "Epoch 307 training complete\n",
            "Cost on training data: 0.14788130933142052\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8904449267856758\n",
            "Accuracy on evaluation data: 8733 / 10000\n",
            "Epoch 308 training complete\n",
            "Cost on training data: 0.1473613770428636\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8835416749826585\n",
            "Accuracy on evaluation data: 8747 / 10000\n",
            "Epoch 309 training complete\n",
            "Cost on training data: 0.1467681823012253\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8843010216987335\n",
            "Accuracy on evaluation data: 8741 / 10000\n",
            "Epoch 310 training complete\n",
            "Cost on training data: 0.14621272630607138\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8875806086811292\n",
            "Accuracy on evaluation data: 8743 / 10000\n",
            "Epoch 311 training complete\n",
            "Cost on training data: 0.14570945769157584\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8872866050969048\n",
            "Accuracy on evaluation data: 8752 / 10000\n",
            "Epoch 312 training complete\n",
            "Cost on training data: 0.14530525592030852\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.884518886425309\n",
            "Accuracy on evaluation data: 8756 / 10000\n",
            "Epoch 313 training complete\n",
            "Cost on training data: 0.14450084650698497\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8848996557018436\n",
            "Accuracy on evaluation data: 8754 / 10000\n",
            "Epoch 314 training complete\n",
            "Cost on training data: 0.14404784438245832\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8873884212063083\n",
            "Accuracy on evaluation data: 8744 / 10000\n",
            "Epoch 315 training complete\n",
            "Cost on training data: 0.1436904628741349\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8861815322556235\n",
            "Accuracy on evaluation data: 8755 / 10000\n",
            "Epoch 316 training complete\n",
            "Cost on training data: 0.1429888259091831\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.888038626806599\n",
            "Accuracy on evaluation data: 8740 / 10000\n",
            "Epoch 317 training complete\n",
            "Cost on training data: 0.14258487668585915\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.887455529857132\n",
            "Accuracy on evaluation data: 8748 / 10000\n",
            "Epoch 318 training complete\n",
            "Cost on training data: 0.14210575506563802\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8856751026045152\n",
            "Accuracy on evaluation data: 8758 / 10000\n",
            "Epoch 319 training complete\n",
            "Cost on training data: 0.14148760743629696\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8836085133513047\n",
            "Accuracy on evaluation data: 8730 / 10000\n",
            "Epoch 320 training complete\n",
            "Cost on training data: 0.1409217416533782\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8852995089200784\n",
            "Accuracy on evaluation data: 8745 / 10000\n",
            "Epoch 321 training complete\n",
            "Cost on training data: 0.14063774563436976\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8800458882305999\n",
            "Accuracy on evaluation data: 8762 / 10000\n",
            "Epoch 322 training complete\n",
            "Cost on training data: 0.13999776429841881\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8810525904646841\n",
            "Accuracy on evaluation data: 8755 / 10000\n",
            "Epoch 323 training complete\n",
            "Cost on training data: 0.13955784369604635\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8827436241404913\n",
            "Accuracy on evaluation data: 8761 / 10000\n",
            "Epoch 324 training complete\n",
            "Cost on training data: 0.13916726960002254\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8821323758515287\n",
            "Accuracy on evaluation data: 8741 / 10000\n",
            "Epoch 325 training complete\n",
            "Cost on training data: 0.1384830198648262\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.878947996071029\n",
            "Accuracy on evaluation data: 8752 / 10000\n",
            "Epoch 326 training complete\n",
            "Cost on training data: 0.13807924267995728\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8840907090933265\n",
            "Accuracy on evaluation data: 8748 / 10000\n",
            "Epoch 327 training complete\n",
            "Cost on training data: 0.13753234941333412\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8809627248672378\n",
            "Accuracy on evaluation data: 8757 / 10000\n",
            "Epoch 328 training complete\n",
            "Cost on training data: 0.13714345392455046\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8743790550173449\n",
            "Accuracy on evaluation data: 8763 / 10000\n",
            "Epoch 329 training complete\n",
            "Cost on training data: 0.1365821930936538\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8780292625230506\n",
            "Accuracy on evaluation data: 8753 / 10000\n",
            "Epoch 330 training complete\n",
            "Cost on training data: 0.13650611849242428\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8767703144068512\n",
            "Accuracy on evaluation data: 8768 / 10000\n",
            "Epoch 331 training complete\n",
            "Cost on training data: 0.13602439712227998\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8880339947754528\n",
            "Accuracy on evaluation data: 8738 / 10000\n",
            "Epoch 332 training complete\n",
            "Cost on training data: 0.13534342648897937\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8761722121367611\n",
            "Accuracy on evaluation data: 8756 / 10000\n",
            "Epoch 333 training complete\n",
            "Cost on training data: 0.13492021904318013\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.881160983709882\n",
            "Accuracy on evaluation data: 8755 / 10000\n",
            "Epoch 334 training complete\n",
            "Cost on training data: 0.13445905527825117\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8787979103254909\n",
            "Accuracy on evaluation data: 8746 / 10000\n",
            "Epoch 335 training complete\n",
            "Cost on training data: 0.13419008964860973\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8797475111781463\n",
            "Accuracy on evaluation data: 8743 / 10000\n",
            "Epoch 336 training complete\n",
            "Cost on training data: 0.133595966407324\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8807546330300018\n",
            "Accuracy on evaluation data: 8759 / 10000\n",
            "Epoch 337 training complete\n",
            "Cost on training data: 0.133286248915948\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8801649562964035\n",
            "Accuracy on evaluation data: 8763 / 10000\n",
            "Epoch 338 training complete\n",
            "Cost on training data: 0.1328930383505031\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8778032191661741\n",
            "Accuracy on evaluation data: 8759 / 10000\n",
            "Epoch 339 training complete\n",
            "Cost on training data: 0.13238377702147947\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.874323416032101\n",
            "Accuracy on evaluation data: 8763 / 10000\n",
            "Epoch 340 training complete\n",
            "Cost on training data: 0.13200130104335064\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.881849410557475\n",
            "Accuracy on evaluation data: 8753 / 10000\n",
            "Epoch 341 training complete\n",
            "Cost on training data: 0.1317691096190335\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8878350898792754\n",
            "Accuracy on evaluation data: 8756 / 10000\n",
            "Epoch 342 training complete\n",
            "Cost on training data: 0.1313062488518219\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8849624201844091\n",
            "Accuracy on evaluation data: 8749 / 10000\n",
            "Epoch 343 training complete\n",
            "Cost on training data: 0.1305919569529912\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8772263571888262\n",
            "Accuracy on evaluation data: 8748 / 10000\n",
            "Epoch 344 training complete\n",
            "Cost on training data: 0.1303450990054995\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8826914790040006\n",
            "Accuracy on evaluation data: 8751 / 10000\n",
            "Epoch 345 training complete\n",
            "Cost on training data: 0.13017110181395902\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8777354410361909\n",
            "Accuracy on evaluation data: 8746 / 10000\n",
            "Epoch 346 training complete\n",
            "Cost on training data: 0.12958206795589902\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8716791955030383\n",
            "Accuracy on evaluation data: 8766 / 10000\n",
            "Epoch 347 training complete\n",
            "Cost on training data: 0.12923499961142795\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8660216657022154\n",
            "Accuracy on evaluation data: 8774 / 10000\n",
            "Epoch 348 training complete\n",
            "Cost on training data: 0.1286614593712025\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8762717343739943\n",
            "Accuracy on evaluation data: 8756 / 10000\n",
            "Epoch 349 training complete\n",
            "Cost on training data: 0.12840392001672427\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.871495440558483\n",
            "Accuracy on evaluation data: 8760 / 10000\n",
            "Epoch 350 training complete\n",
            "Cost on training data: 0.12802908909372052\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8759471306538724\n",
            "Accuracy on evaluation data: 8762 / 10000\n",
            "Epoch 351 training complete\n",
            "Cost on training data: 0.12764319700250296\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8753556243121077\n",
            "Accuracy on evaluation data: 8750 / 10000\n",
            "Epoch 352 training complete\n",
            "Cost on training data: 0.12731440867552737\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8676565266462263\n",
            "Accuracy on evaluation data: 8776 / 10000\n",
            "Epoch 353 training complete\n",
            "Cost on training data: 0.1269286981774702\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.872297148941598\n",
            "Accuracy on evaluation data: 8762 / 10000\n",
            "Epoch 354 training complete\n",
            "Cost on training data: 0.126664811460222\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8785688412518107\n",
            "Accuracy on evaluation data: 8752 / 10000\n",
            "Epoch 355 training complete\n",
            "Cost on training data: 0.1261416310091203\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8696796044370793\n",
            "Accuracy on evaluation data: 8757 / 10000\n",
            "Epoch 356 training complete\n",
            "Cost on training data: 0.12584376090067198\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8731186774412398\n",
            "Accuracy on evaluation data: 8763 / 10000\n",
            "Epoch 357 training complete\n",
            "Cost on training data: 0.12547050051091135\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8699003219548253\n",
            "Accuracy on evaluation data: 8767 / 10000\n",
            "Epoch 358 training complete\n",
            "Cost on training data: 0.12500243992470006\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.873786469046789\n",
            "Accuracy on evaluation data: 8763 / 10000\n",
            "Epoch 359 training complete\n",
            "Cost on training data: 0.12481653280729418\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8799914870929781\n",
            "Accuracy on evaluation data: 8753 / 10000\n",
            "Epoch 360 training complete\n",
            "Cost on training data: 0.12441124883461827\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8804488667336351\n",
            "Accuracy on evaluation data: 8758 / 10000\n",
            "Epoch 361 training complete\n",
            "Cost on training data: 0.12425166617339078\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8660253804971724\n",
            "Accuracy on evaluation data: 8757 / 10000\n",
            "Epoch 362 training complete\n",
            "Cost on training data: 0.12371988912701948\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8697656431482822\n",
            "Accuracy on evaluation data: 8770 / 10000\n",
            "Epoch 363 training complete\n",
            "Cost on training data: 0.12342494583138576\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8695743907122104\n",
            "Accuracy on evaluation data: 8763 / 10000\n",
            "Epoch 364 training complete\n",
            "Cost on training data: 0.12305648915161826\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8741939715153293\n",
            "Accuracy on evaluation data: 8756 / 10000\n",
            "Epoch 365 training complete\n",
            "Cost on training data: 0.12280993828390915\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8720882103826171\n",
            "Accuracy on evaluation data: 8769 / 10000\n",
            "Epoch 366 training complete\n",
            "Cost on training data: 0.12250275175593191\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8790648649668835\n",
            "Accuracy on evaluation data: 8750 / 10000\n",
            "Epoch 367 training complete\n",
            "Cost on training data: 0.1220809014471368\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8685395585584702\n",
            "Accuracy on evaluation data: 8767 / 10000\n",
            "Epoch 368 training complete\n",
            "Cost on training data: 0.12191867442529838\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8708248192781415\n",
            "Accuracy on evaluation data: 8762 / 10000\n",
            "Epoch 369 training complete\n",
            "Cost on training data: 0.12155036677322173\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8762439491615728\n",
            "Accuracy on evaluation data: 8748 / 10000\n",
            "Epoch 370 training complete\n",
            "Cost on training data: 0.12112334517630874\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8712621589028465\n",
            "Accuracy on evaluation data: 8766 / 10000\n",
            "Epoch 371 training complete\n",
            "Cost on training data: 0.12080859133299672\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8726817382420112\n",
            "Accuracy on evaluation data: 8757 / 10000\n",
            "Epoch 372 training complete\n",
            "Cost on training data: 0.12056184762099578\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.869026521756376\n",
            "Accuracy on evaluation data: 8758 / 10000\n",
            "Epoch 373 training complete\n",
            "Cost on training data: 0.12019686454694711\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8792570807543136\n",
            "Accuracy on evaluation data: 8753 / 10000\n",
            "Epoch 374 training complete\n",
            "Cost on training data: 0.12004947977229463\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8673503741365367\n",
            "Accuracy on evaluation data: 8759 / 10000\n",
            "Epoch 375 training complete\n",
            "Cost on training data: 0.11956530562774197\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8690035254085654\n",
            "Accuracy on evaluation data: 8754 / 10000\n",
            "Epoch 376 training complete\n",
            "Cost on training data: 0.11962488561042144\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8723680462954649\n",
            "Accuracy on evaluation data: 8755 / 10000\n",
            "Epoch 377 training complete\n",
            "Cost on training data: 0.11897183651571769\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8622487258945718\n",
            "Accuracy on evaluation data: 8768 / 10000\n",
            "Epoch 378 training complete\n",
            "Cost on training data: 0.1188103249908498\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8627320009868769\n",
            "Accuracy on evaluation data: 8772 / 10000\n",
            "Epoch 379 training complete\n",
            "Cost on training data: 0.11837554802040431\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8673700628447971\n",
            "Accuracy on evaluation data: 8760 / 10000\n",
            "Epoch 380 training complete\n",
            "Cost on training data: 0.11810257287532662\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8652274711816027\n",
            "Accuracy on evaluation data: 8765 / 10000\n",
            "Epoch 381 training complete\n",
            "Cost on training data: 0.11794277026153316\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8661198519537379\n",
            "Accuracy on evaluation data: 8767 / 10000\n",
            "Epoch 382 training complete\n",
            "Cost on training data: 0.11756183690136782\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8656932824377225\n",
            "Accuracy on evaluation data: 8772 / 10000\n",
            "Epoch 383 training complete\n",
            "Cost on training data: 0.11742372499492787\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8716763346657407\n",
            "Accuracy on evaluation data: 8768 / 10000\n",
            "Epoch 384 training complete\n",
            "Cost on training data: 0.11709198244151678\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8706149362922291\n",
            "Accuracy on evaluation data: 8761 / 10000\n",
            "Epoch 385 training complete\n",
            "Cost on training data: 0.11676530290504777\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8714489028519743\n",
            "Accuracy on evaluation data: 8754 / 10000\n",
            "Epoch 386 training complete\n",
            "Cost on training data: 0.11646139948919121\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8700629027183296\n",
            "Accuracy on evaluation data: 8761 / 10000\n",
            "Epoch 387 training complete\n",
            "Cost on training data: 0.11615172904435606\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8654534963629152\n",
            "Accuracy on evaluation data: 8765 / 10000\n",
            "Epoch 388 training complete\n",
            "Cost on training data: 0.11596824637856618\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8642376353649103\n",
            "Accuracy on evaluation data: 8778 / 10000\n",
            "Epoch 389 training complete\n",
            "Cost on training data: 0.11564881513276175\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8627179199628516\n",
            "Accuracy on evaluation data: 8778 / 10000\n",
            "Epoch 390 training complete\n",
            "Cost on training data: 0.11544102146238532\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8641339869929316\n",
            "Accuracy on evaluation data: 8770 / 10000\n",
            "Epoch 391 training complete\n",
            "Cost on training data: 0.11508397664579054\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8660175015392962\n",
            "Accuracy on evaluation data: 8766 / 10000\n",
            "Epoch 392 training complete\n",
            "Cost on training data: 0.11488024212269121\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8695973459917651\n",
            "Accuracy on evaluation data: 8763 / 10000\n",
            "Epoch 393 training complete\n",
            "Cost on training data: 0.11466869915302236\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8677736558675561\n",
            "Accuracy on evaluation data: 8750 / 10000\n",
            "Epoch 394 training complete\n",
            "Cost on training data: 0.11433591063041842\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8631894466117618\n",
            "Accuracy on evaluation data: 8775 / 10000\n",
            "Epoch 395 training complete\n",
            "Cost on training data: 0.11411593006876543\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8627297817892551\n",
            "Accuracy on evaluation data: 8776 / 10000\n",
            "Epoch 396 training complete\n",
            "Cost on training data: 0.11395159819262317\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8676002053382836\n",
            "Accuracy on evaluation data: 8756 / 10000\n",
            "Epoch 397 training complete\n",
            "Cost on training data: 0.11363176487528329\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8664044365762389\n",
            "Accuracy on evaluation data: 8756 / 10000\n",
            "Epoch 398 training complete\n",
            "Cost on training data: 0.11347625460700184\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.867745810316793\n",
            "Accuracy on evaluation data: 8765 / 10000\n",
            "Epoch 399 training complete\n",
            "Cost on training data: 0.11346989760277112\n",
            "Accuracy on training data: 1000 / 1000\n",
            "Cost on evaluation data: 0.8718483208298586\n",
            "Accuracy on evaluation data: 8751 / 10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([np.float64(2.23381313794047),\n",
              "  np.float64(1.8578071766480555),\n",
              "  np.float64(1.639313396100369),\n",
              "  np.float64(1.5096697678438278),\n",
              "  np.float64(1.4870589068329607),\n",
              "  np.float64(1.4311532854817384),\n",
              "  np.float64(1.42436012666953),\n",
              "  np.float64(1.3170151458303538),\n",
              "  np.float64(1.3683020768708563),\n",
              "  np.float64(1.3049757569838412),\n",
              "  np.float64(1.2951656321224811),\n",
              "  np.float64(1.2950842331027985),\n",
              "  np.float64(1.3218352380285252),\n",
              "  np.float64(1.257383076836807),\n",
              "  np.float64(1.257967751666464),\n",
              "  np.float64(1.2379148815197436),\n",
              "  np.float64(1.243457634629964),\n",
              "  np.float64(1.233259475079607),\n",
              "  np.float64(1.256568703881618),\n",
              "  np.float64(1.269423278896301),\n",
              "  np.float64(1.2477405216453608),\n",
              "  np.float64(1.251242321703052),\n",
              "  np.float64(1.2345774477007871),\n",
              "  np.float64(1.2442849592080831),\n",
              "  np.float64(1.2375750891637558),\n",
              "  np.float64(1.2343621866485242),\n",
              "  np.float64(1.2324581426868653),\n",
              "  np.float64(1.249356905463297),\n",
              "  np.float64(1.2344422092740273),\n",
              "  np.float64(1.2306143647076562),\n",
              "  np.float64(1.2382703442046212),\n",
              "  np.float64(1.2441946400044148),\n",
              "  np.float64(1.2264666000995015),\n",
              "  np.float64(1.2308077554708436),\n",
              "  np.float64(1.2305615285455267),\n",
              "  np.float64(1.230452385313968),\n",
              "  np.float64(1.2331251771360556),\n",
              "  np.float64(1.2354171523933766),\n",
              "  np.float64(1.231265731268845),\n",
              "  np.float64(1.2309276108407512),\n",
              "  np.float64(1.227241988820512),\n",
              "  np.float64(1.2322084399437554),\n",
              "  np.float64(1.2181879859107172),\n",
              "  np.float64(1.2132009092906888),\n",
              "  np.float64(1.2168981384158661),\n",
              "  np.float64(1.2109649564401836),\n",
              "  np.float64(1.226369656661059),\n",
              "  np.float64(1.213712965512063),\n",
              "  np.float64(1.2112544121168471),\n",
              "  np.float64(1.215982131902206),\n",
              "  np.float64(1.2059067450164096),\n",
              "  np.float64(1.2110932137022228),\n",
              "  np.float64(1.2019190505234467),\n",
              "  np.float64(1.2081167511851154),\n",
              "  np.float64(1.2033845537094892),\n",
              "  np.float64(1.2025617536249225),\n",
              "  np.float64(1.1944282734872012),\n",
              "  np.float64(1.193983414320263),\n",
              "  np.float64(1.1888701681387215),\n",
              "  np.float64(1.1876107547629375),\n",
              "  np.float64(1.191187743665512),\n",
              "  np.float64(1.1888012658977414),\n",
              "  np.float64(1.1861700086649904),\n",
              "  np.float64(1.1879371633456794),\n",
              "  np.float64(1.1794179172539268),\n",
              "  np.float64(1.184221014757604),\n",
              "  np.float64(1.1833841200788622),\n",
              "  np.float64(1.1717681957707042),\n",
              "  np.float64(1.1791330046765336),\n",
              "  np.float64(1.1704496866149097),\n",
              "  np.float64(1.1708505968947007),\n",
              "  np.float64(1.1709910460019908),\n",
              "  np.float64(1.169593122364095),\n",
              "  np.float64(1.1701126866437608),\n",
              "  np.float64(1.164710178205944),\n",
              "  np.float64(1.1586395498032533),\n",
              "  np.float64(1.1558404472823323),\n",
              "  np.float64(1.1601791759928375),\n",
              "  np.float64(1.1615750564886698),\n",
              "  np.float64(1.1597549782669618),\n",
              "  np.float64(1.1490130705562083),\n",
              "  np.float64(1.1509402709297032),\n",
              "  np.float64(1.1460278877196968),\n",
              "  np.float64(1.1506338953310478),\n",
              "  np.float64(1.1493334954853915),\n",
              "  np.float64(1.1526245165397377),\n",
              "  np.float64(1.1432386800640988),\n",
              "  np.float64(1.1405902198361195),\n",
              "  np.float64(1.1429183419717595),\n",
              "  np.float64(1.134951565845928),\n",
              "  np.float64(1.1336716812025016),\n",
              "  np.float64(1.1284278141533348),\n",
              "  np.float64(1.136089706883153),\n",
              "  np.float64(1.1297793559125417),\n",
              "  np.float64(1.1310218309315925),\n",
              "  np.float64(1.1299268877445061),\n",
              "  np.float64(1.1267671489859339),\n",
              "  np.float64(1.1223136103682247),\n",
              "  np.float64(1.1198097278908856),\n",
              "  np.float64(1.1210190684050283),\n",
              "  np.float64(1.1247322000784499),\n",
              "  np.float64(1.1115044249266814),\n",
              "  np.float64(1.1170218955021272),\n",
              "  np.float64(1.1082498864844381),\n",
              "  np.float64(1.1086553433925694),\n",
              "  np.float64(1.1104000303253303),\n",
              "  np.float64(1.10862920589249),\n",
              "  np.float64(1.1027310945638373),\n",
              "  np.float64(1.0996563606168024),\n",
              "  np.float64(1.0958739265192583),\n",
              "  np.float64(1.0956706663947644),\n",
              "  np.float64(1.0948275001118917),\n",
              "  np.float64(1.0944678481037924),\n",
              "  np.float64(1.0916562493379423),\n",
              "  np.float64(1.0906490973348244),\n",
              "  np.float64(1.0932005328471894),\n",
              "  np.float64(1.0856811410163885),\n",
              "  np.float64(1.0797978868866425),\n",
              "  np.float64(1.0866932680089958),\n",
              "  np.float64(1.076515302580667),\n",
              "  np.float64(1.0752142484052474),\n",
              "  np.float64(1.0801232064649176),\n",
              "  np.float64(1.0755342521147715),\n",
              "  np.float64(1.0676871546426485),\n",
              "  np.float64(1.0761269423164685),\n",
              "  np.float64(1.0685751199624671),\n",
              "  np.float64(1.0735501786981345),\n",
              "  np.float64(1.0643903645291566),\n",
              "  np.float64(1.0636064492151494),\n",
              "  np.float64(1.0633690423950541),\n",
              "  np.float64(1.0664562623218943),\n",
              "  np.float64(1.0600797733588634),\n",
              "  np.float64(1.0550897414847886),\n",
              "  np.float64(1.0513069193064248),\n",
              "  np.float64(1.0580538995233195),\n",
              "  np.float64(1.054879467575505),\n",
              "  np.float64(1.0445005603513926),\n",
              "  np.float64(1.0491484491563865),\n",
              "  np.float64(1.0526132779993977),\n",
              "  np.float64(1.0466473763293),\n",
              "  np.float64(1.041888104387314),\n",
              "  np.float64(1.041208237538272),\n",
              "  np.float64(1.0394582226041023),\n",
              "  np.float64(1.040400453705019),\n",
              "  np.float64(1.036591579030419),\n",
              "  np.float64(1.0354573729160639),\n",
              "  np.float64(1.0320102822287522),\n",
              "  np.float64(1.0342252585595149),\n",
              "  np.float64(1.0316930723438378),\n",
              "  np.float64(1.0307080351956857),\n",
              "  np.float64(1.024511200742775),\n",
              "  np.float64(1.0273302935492181),\n",
              "  np.float64(1.0256251883234804),\n",
              "  np.float64(1.0273283045299915),\n",
              "  np.float64(1.0260316343169018),\n",
              "  np.float64(1.0208336954536532),\n",
              "  np.float64(1.0236237485395197),\n",
              "  np.float64(1.0163666068126074),\n",
              "  np.float64(1.0229591063552612),\n",
              "  np.float64(1.015982804870418),\n",
              "  np.float64(1.0149074921328267),\n",
              "  np.float64(1.0129520560177143),\n",
              "  np.float64(1.0093673200110276),\n",
              "  np.float64(1.0134672316662687),\n",
              "  np.float64(1.0077418617233964),\n",
              "  np.float64(1.0046468297262234),\n",
              "  np.float64(1.0037619612215842),\n",
              "  np.float64(1.0037139418898917),\n",
              "  np.float64(1.0020904328547702),\n",
              "  np.float64(1.0029727675606759),\n",
              "  np.float64(0.9983961736603311),\n",
              "  np.float64(0.9984482236234222),\n",
              "  np.float64(0.9980504615226646),\n",
              "  np.float64(0.9966898475485679),\n",
              "  np.float64(0.9933194018758037),\n",
              "  np.float64(0.9958601677500079),\n",
              "  np.float64(0.9912423556600048),\n",
              "  np.float64(0.9912313529917537),\n",
              "  np.float64(0.9876153745394035),\n",
              "  np.float64(0.9852464550113817),\n",
              "  np.float64(0.9851494526839423),\n",
              "  np.float64(0.9854745516813888),\n",
              "  np.float64(0.9899832889379796),\n",
              "  np.float64(0.9827780140468239),\n",
              "  np.float64(0.9772972336616752),\n",
              "  np.float64(0.9795260150485857),\n",
              "  np.float64(0.9812624255396147),\n",
              "  np.float64(0.977602386637912),\n",
              "  np.float64(0.9754867083067064),\n",
              "  np.float64(0.9739359796296361),\n",
              "  np.float64(0.9733897849669881),\n",
              "  np.float64(0.9714967245596274),\n",
              "  np.float64(0.9687464490430199),\n",
              "  np.float64(0.9675207135123466),\n",
              "  np.float64(0.9724547406996101),\n",
              "  np.float64(0.9682665950602847),\n",
              "  np.float64(0.9634415868662933),\n",
              "  np.float64(0.9666053131735165),\n",
              "  np.float64(0.9649353527652191),\n",
              "  np.float64(0.9641641535323175),\n",
              "  np.float64(0.9638092685749537),\n",
              "  np.float64(0.9607341085924664),\n",
              "  np.float64(0.9647232644890867),\n",
              "  np.float64(0.9695885760042101),\n",
              "  np.float64(0.9590732871643047),\n",
              "  np.float64(0.9568687736587742),\n",
              "  np.float64(0.9507523323869675),\n",
              "  np.float64(0.9564131724048358),\n",
              "  np.float64(0.9561492094792736),\n",
              "  np.float64(0.952786134800699),\n",
              "  np.float64(0.9554712889349509),\n",
              "  np.float64(0.954315240839431),\n",
              "  np.float64(0.9520476440380687),\n",
              "  np.float64(0.9451105873629928),\n",
              "  np.float64(0.9438314852795101),\n",
              "  np.float64(0.9525833700864114),\n",
              "  np.float64(0.9465171579437679),\n",
              "  np.float64(0.942595150595273),\n",
              "  np.float64(0.9448468390038404),\n",
              "  np.float64(0.9447236825703795),\n",
              "  np.float64(0.9416219230213395),\n",
              "  np.float64(0.9396566318331718),\n",
              "  np.float64(0.9421117126278145),\n",
              "  np.float64(0.9444952970845528),\n",
              "  np.float64(0.9347761052416744),\n",
              "  np.float64(0.9391529207458551),\n",
              "  np.float64(0.9367633418734309),\n",
              "  np.float64(0.9355534170164717),\n",
              "  np.float64(0.9368090293675461),\n",
              "  np.float64(0.9338300192652639),\n",
              "  np.float64(0.9368415045062056),\n",
              "  np.float64(0.9324877515214856),\n",
              "  np.float64(0.9324871326039087),\n",
              "  np.float64(0.932045391253745),\n",
              "  np.float64(0.9286058654560793),\n",
              "  np.float64(0.936668588862639),\n",
              "  np.float64(0.930730332107528),\n",
              "  np.float64(0.9229533905711206),\n",
              "  np.float64(0.9272419093105022),\n",
              "  np.float64(0.9268978141037126),\n",
              "  np.float64(0.9223109530926343),\n",
              "  np.float64(0.922776249066881),\n",
              "  np.float64(0.9239849106282955),\n",
              "  np.float64(0.9217481278049661),\n",
              "  np.float64(0.9235829723129406),\n",
              "  np.float64(0.9217208400641957),\n",
              "  np.float64(0.9225537566919365),\n",
              "  np.float64(0.9200765889948038),\n",
              "  np.float64(0.9152444102074252),\n",
              "  np.float64(0.9197894298231919),\n",
              "  np.float64(0.9186624874586558),\n",
              "  np.float64(0.9208779952246396),\n",
              "  np.float64(0.9139532036053704),\n",
              "  np.float64(0.9148077175305772),\n",
              "  np.float64(0.9209301566029329),\n",
              "  np.float64(0.9157554638943763),\n",
              "  np.float64(0.916555197108018),\n",
              "  np.float64(0.9130837367132254),\n",
              "  np.float64(0.9113925684587308),\n",
              "  np.float64(0.9093169688906771),\n",
              "  np.float64(0.9088854855064433),\n",
              "  np.float64(0.9153847957137105),\n",
              "  np.float64(0.9176218231167736),\n",
              "  np.float64(0.9153777253847887),\n",
              "  np.float64(0.9058832602462504),\n",
              "  np.float64(0.9090608989584996),\n",
              "  np.float64(0.9075780805246967),\n",
              "  np.float64(0.9097259410466463),\n",
              "  np.float64(0.9040219702442139),\n",
              "  np.float64(0.9040339172292926),\n",
              "  np.float64(0.9067426198622827),\n",
              "  np.float64(0.9021100691281603),\n",
              "  np.float64(0.9012033885444908),\n",
              "  np.float64(0.9106029572643762),\n",
              "  np.float64(0.8960755043584023),\n",
              "  np.float64(0.9037739936746654),\n",
              "  np.float64(0.9011978966329464),\n",
              "  np.float64(0.9060788615733627),\n",
              "  np.float64(0.9023423978537171),\n",
              "  np.float64(0.8998259496482847),\n",
              "  np.float64(0.897266111227457),\n",
              "  np.float64(0.9066706765356962),\n",
              "  np.float64(0.8926838057813254),\n",
              "  np.float64(0.9009454536842314),\n",
              "  np.float64(0.8957796595993239),\n",
              "  np.float64(0.8997191519756288),\n",
              "  np.float64(0.8923445295529425),\n",
              "  np.float64(0.8950432939234149),\n",
              "  np.float64(0.8929655042292008),\n",
              "  np.float64(0.8969849118586438),\n",
              "  np.float64(0.8901799875690688),\n",
              "  np.float64(0.887504652850806),\n",
              "  np.float64(0.9000609761327453),\n",
              "  np.float64(0.8885516727445999),\n",
              "  np.float64(0.8938940423564085),\n",
              "  np.float64(0.8935941211347652),\n",
              "  np.float64(0.8967627265333417),\n",
              "  np.float64(0.8903488959019646),\n",
              "  np.float64(0.8904512048841041),\n",
              "  np.float64(0.8928622548662912),\n",
              "  np.float64(0.8905449983334723),\n",
              "  np.float64(0.8863922223593935),\n",
              "  np.float64(0.8882790885842334),\n",
              "  np.float64(0.8889727813120496),\n",
              "  np.float64(0.8868456897070973),\n",
              "  np.float64(0.889817295548033),\n",
              "  np.float64(0.8877566832220003),\n",
              "  np.float64(0.8904449267856758),\n",
              "  np.float64(0.8835416749826585),\n",
              "  np.float64(0.8843010216987335),\n",
              "  np.float64(0.8875806086811292),\n",
              "  np.float64(0.8872866050969048),\n",
              "  np.float64(0.884518886425309),\n",
              "  np.float64(0.8848996557018436),\n",
              "  np.float64(0.8873884212063083),\n",
              "  np.float64(0.8861815322556235),\n",
              "  np.float64(0.888038626806599),\n",
              "  np.float64(0.887455529857132),\n",
              "  np.float64(0.8856751026045152),\n",
              "  np.float64(0.8836085133513047),\n",
              "  np.float64(0.8852995089200784),\n",
              "  np.float64(0.8800458882305999),\n",
              "  np.float64(0.8810525904646841),\n",
              "  np.float64(0.8827436241404913),\n",
              "  np.float64(0.8821323758515287),\n",
              "  np.float64(0.878947996071029),\n",
              "  np.float64(0.8840907090933265),\n",
              "  np.float64(0.8809627248672378),\n",
              "  np.float64(0.8743790550173449),\n",
              "  np.float64(0.8780292625230506),\n",
              "  np.float64(0.8767703144068512),\n",
              "  np.float64(0.8880339947754528),\n",
              "  np.float64(0.8761722121367611),\n",
              "  np.float64(0.881160983709882),\n",
              "  np.float64(0.8787979103254909),\n",
              "  np.float64(0.8797475111781463),\n",
              "  np.float64(0.8807546330300018),\n",
              "  np.float64(0.8801649562964035),\n",
              "  np.float64(0.8778032191661741),\n",
              "  np.float64(0.874323416032101),\n",
              "  np.float64(0.881849410557475),\n",
              "  np.float64(0.8878350898792754),\n",
              "  np.float64(0.8849624201844091),\n",
              "  np.float64(0.8772263571888262),\n",
              "  np.float64(0.8826914790040006),\n",
              "  np.float64(0.8777354410361909),\n",
              "  np.float64(0.8716791955030383),\n",
              "  np.float64(0.8660216657022154),\n",
              "  np.float64(0.8762717343739943),\n",
              "  np.float64(0.871495440558483),\n",
              "  np.float64(0.8759471306538724),\n",
              "  np.float64(0.8753556243121077),\n",
              "  np.float64(0.8676565266462263),\n",
              "  np.float64(0.872297148941598),\n",
              "  np.float64(0.8785688412518107),\n",
              "  np.float64(0.8696796044370793),\n",
              "  np.float64(0.8731186774412398),\n",
              "  np.float64(0.8699003219548253),\n",
              "  np.float64(0.873786469046789),\n",
              "  np.float64(0.8799914870929781),\n",
              "  np.float64(0.8804488667336351),\n",
              "  np.float64(0.8660253804971724),\n",
              "  np.float64(0.8697656431482822),\n",
              "  np.float64(0.8695743907122104),\n",
              "  np.float64(0.8741939715153293),\n",
              "  np.float64(0.8720882103826171),\n",
              "  np.float64(0.8790648649668835),\n",
              "  np.float64(0.8685395585584702),\n",
              "  np.float64(0.8708248192781415),\n",
              "  np.float64(0.8762439491615728),\n",
              "  np.float64(0.8712621589028465),\n",
              "  np.float64(0.8726817382420112),\n",
              "  np.float64(0.869026521756376),\n",
              "  np.float64(0.8792570807543136),\n",
              "  np.float64(0.8673503741365367),\n",
              "  np.float64(0.8690035254085654),\n",
              "  np.float64(0.8723680462954649),\n",
              "  np.float64(0.8622487258945718),\n",
              "  np.float64(0.8627320009868769),\n",
              "  np.float64(0.8673700628447971),\n",
              "  np.float64(0.8652274711816027),\n",
              "  np.float64(0.8661198519537379),\n",
              "  np.float64(0.8656932824377225),\n",
              "  np.float64(0.8716763346657407),\n",
              "  np.float64(0.8706149362922291),\n",
              "  np.float64(0.8714489028519743),\n",
              "  np.float64(0.8700629027183296),\n",
              "  np.float64(0.8654534963629152),\n",
              "  np.float64(0.8642376353649103),\n",
              "  np.float64(0.8627179199628516),\n",
              "  np.float64(0.8641339869929316),\n",
              "  np.float64(0.8660175015392962),\n",
              "  np.float64(0.8695973459917651),\n",
              "  np.float64(0.8677736558675561),\n",
              "  np.float64(0.8631894466117618),\n",
              "  np.float64(0.8627297817892551),\n",
              "  np.float64(0.8676002053382836),\n",
              "  np.float64(0.8664044365762389),\n",
              "  np.float64(0.867745810316793),\n",
              "  np.float64(0.8718483208298586)],\n",
              " [5791,\n",
              "  6688,\n",
              "  7131,\n",
              "  7503,\n",
              "  7571,\n",
              "  7656,\n",
              "  7735,\n",
              "  7991,\n",
              "  7886,\n",
              "  7973,\n",
              "  8040,\n",
              "  8049,\n",
              "  8014,\n",
              "  8150,\n",
              "  8169,\n",
              "  8199,\n",
              "  8185,\n",
              "  8211,\n",
              "  8171,\n",
              "  8186,\n",
              "  8239,\n",
              "  8242,\n",
              "  8244,\n",
              "  8271,\n",
              "  8243,\n",
              "  8269,\n",
              "  8290,\n",
              "  8265,\n",
              "  8304,\n",
              "  8306,\n",
              "  8292,\n",
              "  8292,\n",
              "  8327,\n",
              "  8326,\n",
              "  8327,\n",
              "  8325,\n",
              "  8305,\n",
              "  8335,\n",
              "  8334,\n",
              "  8324,\n",
              "  8335,\n",
              "  8339,\n",
              "  8361,\n",
              "  8354,\n",
              "  8362,\n",
              "  8378,\n",
              "  8356,\n",
              "  8366,\n",
              "  8395,\n",
              "  8376,\n",
              "  8406,\n",
              "  8392,\n",
              "  8410,\n",
              "  8408,\n",
              "  8401,\n",
              "  8403,\n",
              "  8426,\n",
              "  8439,\n",
              "  8443,\n",
              "  8439,\n",
              "  8423,\n",
              "  8448,\n",
              "  8440,\n",
              "  8429,\n",
              "  8459,\n",
              "  8442,\n",
              "  8441,\n",
              "  8454,\n",
              "  8439,\n",
              "  8462,\n",
              "  8454,\n",
              "  8452,\n",
              "  8450,\n",
              "  8453,\n",
              "  8450,\n",
              "  8465,\n",
              "  8467,\n",
              "  8470,\n",
              "  8473,\n",
              "  8474,\n",
              "  8480,\n",
              "  8480,\n",
              "  8481,\n",
              "  8483,\n",
              "  8482,\n",
              "  8482,\n",
              "  8495,\n",
              "  8488,\n",
              "  8496,\n",
              "  8498,\n",
              "  8494,\n",
              "  8510,\n",
              "  8498,\n",
              "  8507,\n",
              "  8505,\n",
              "  8502,\n",
              "  8518,\n",
              "  8505,\n",
              "  8519,\n",
              "  8521,\n",
              "  8500,\n",
              "  8518,\n",
              "  8524,\n",
              "  8527,\n",
              "  8526,\n",
              "  8523,\n",
              "  8520,\n",
              "  8526,\n",
              "  8540,\n",
              "  8536,\n",
              "  8533,\n",
              "  8538,\n",
              "  8533,\n",
              "  8536,\n",
              "  8546,\n",
              "  8543,\n",
              "  8540,\n",
              "  8544,\n",
              "  8539,\n",
              "  8551,\n",
              "  8561,\n",
              "  8549,\n",
              "  8554,\n",
              "  8553,\n",
              "  8545,\n",
              "  8574,\n",
              "  8550,\n",
              "  8563,\n",
              "  8556,\n",
              "  8575,\n",
              "  8570,\n",
              "  8572,\n",
              "  8567,\n",
              "  8583,\n",
              "  8556,\n",
              "  8573,\n",
              "  8582,\n",
              "  8567,\n",
              "  8564,\n",
              "  8580,\n",
              "  8581,\n",
              "  8570,\n",
              "  8576,\n",
              "  8574,\n",
              "  8587,\n",
              "  8585,\n",
              "  8588,\n",
              "  8598,\n",
              "  8595,\n",
              "  8597,\n",
              "  8599,\n",
              "  8586,\n",
              "  8587,\n",
              "  8602,\n",
              "  8584,\n",
              "  8606,\n",
              "  8591,\n",
              "  8605,\n",
              "  8597,\n",
              "  8581,\n",
              "  8605,\n",
              "  8598,\n",
              "  8613,\n",
              "  8605,\n",
              "  8614,\n",
              "  8613,\n",
              "  8617,\n",
              "  8620,\n",
              "  8619,\n",
              "  8624,\n",
              "  8626,\n",
              "  8627,\n",
              "  8631,\n",
              "  8634,\n",
              "  8628,\n",
              "  8627,\n",
              "  8639,\n",
              "  8638,\n",
              "  8632,\n",
              "  8642,\n",
              "  8646,\n",
              "  8651,\n",
              "  8625,\n",
              "  8635,\n",
              "  8643,\n",
              "  8656,\n",
              "  8635,\n",
              "  8647,\n",
              "  8652,\n",
              "  8651,\n",
              "  8649,\n",
              "  8651,\n",
              "  8659,\n",
              "  8667,\n",
              "  8648,\n",
              "  8653,\n",
              "  8675,\n",
              "  8647,\n",
              "  8652,\n",
              "  8664,\n",
              "  8661,\n",
              "  8669,\n",
              "  8651,\n",
              "  8635,\n",
              "  8680,\n",
              "  8671,\n",
              "  8685,\n",
              "  8673,\n",
              "  8674,\n",
              "  8676,\n",
              "  8667,\n",
              "  8676,\n",
              "  8672,\n",
              "  8689,\n",
              "  8679,\n",
              "  8667,\n",
              "  8691,\n",
              "  8690,\n",
              "  8682,\n",
              "  8674,\n",
              "  8684,\n",
              "  8690,\n",
              "  8684,\n",
              "  8682,\n",
              "  8698,\n",
              "  8685,\n",
              "  8698,\n",
              "  8684,\n",
              "  8682,\n",
              "  8692,\n",
              "  8687,\n",
              "  8694,\n",
              "  8691,\n",
              "  8700,\n",
              "  8693,\n",
              "  8675,\n",
              "  8688,\n",
              "  8703,\n",
              "  8704,\n",
              "  8704,\n",
              "  8692,\n",
              "  8696,\n",
              "  8704,\n",
              "  8712,\n",
              "  8713,\n",
              "  8708,\n",
              "  8700,\n",
              "  8705,\n",
              "  8713,\n",
              "  8701,\n",
              "  8714,\n",
              "  8714,\n",
              "  8709,\n",
              "  8717,\n",
              "  8701,\n",
              "  8705,\n",
              "  8706,\n",
              "  8717,\n",
              "  8721,\n",
              "  8719,\n",
              "  8725,\n",
              "  8700,\n",
              "  8696,\n",
              "  8711,\n",
              "  8731,\n",
              "  8708,\n",
              "  8713,\n",
              "  8710,\n",
              "  8719,\n",
              "  8712,\n",
              "  8721,\n",
              "  8729,\n",
              "  8734,\n",
              "  8711,\n",
              "  8732,\n",
              "  8715,\n",
              "  8725,\n",
              "  8728,\n",
              "  8724,\n",
              "  8741,\n",
              "  8718,\n",
              "  8725,\n",
              "  8740,\n",
              "  8718,\n",
              "  8742,\n",
              "  8745,\n",
              "  8735,\n",
              "  8738,\n",
              "  8725,\n",
              "  8732,\n",
              "  8730,\n",
              "  8755,\n",
              "  8723,\n",
              "  8745,\n",
              "  8724,\n",
              "  8735,\n",
              "  8734,\n",
              "  8734,\n",
              "  8734,\n",
              "  8731,\n",
              "  8741,\n",
              "  8739,\n",
              "  8744,\n",
              "  8739,\n",
              "  8729,\n",
              "  8743,\n",
              "  8746,\n",
              "  8733,\n",
              "  8747,\n",
              "  8741,\n",
              "  8743,\n",
              "  8752,\n",
              "  8756,\n",
              "  8754,\n",
              "  8744,\n",
              "  8755,\n",
              "  8740,\n",
              "  8748,\n",
              "  8758,\n",
              "  8730,\n",
              "  8745,\n",
              "  8762,\n",
              "  8755,\n",
              "  8761,\n",
              "  8741,\n",
              "  8752,\n",
              "  8748,\n",
              "  8757,\n",
              "  8763,\n",
              "  8753,\n",
              "  8768,\n",
              "  8738,\n",
              "  8756,\n",
              "  8755,\n",
              "  8746,\n",
              "  8743,\n",
              "  8759,\n",
              "  8763,\n",
              "  8759,\n",
              "  8763,\n",
              "  8753,\n",
              "  8756,\n",
              "  8749,\n",
              "  8748,\n",
              "  8751,\n",
              "  8746,\n",
              "  8766,\n",
              "  8774,\n",
              "  8756,\n",
              "  8760,\n",
              "  8762,\n",
              "  8750,\n",
              "  8776,\n",
              "  8762,\n",
              "  8752,\n",
              "  8757,\n",
              "  8763,\n",
              "  8767,\n",
              "  8763,\n",
              "  8753,\n",
              "  8758,\n",
              "  8757,\n",
              "  8770,\n",
              "  8763,\n",
              "  8756,\n",
              "  8769,\n",
              "  8750,\n",
              "  8767,\n",
              "  8762,\n",
              "  8748,\n",
              "  8766,\n",
              "  8757,\n",
              "  8758,\n",
              "  8753,\n",
              "  8759,\n",
              "  8754,\n",
              "  8755,\n",
              "  8768,\n",
              "  8772,\n",
              "  8760,\n",
              "  8765,\n",
              "  8767,\n",
              "  8772,\n",
              "  8768,\n",
              "  8761,\n",
              "  8754,\n",
              "  8761,\n",
              "  8765,\n",
              "  8778,\n",
              "  8778,\n",
              "  8770,\n",
              "  8766,\n",
              "  8763,\n",
              "  8750,\n",
              "  8775,\n",
              "  8776,\n",
              "  8756,\n",
              "  8756,\n",
              "  8765,\n",
              "  8751],\n",
              " [np.float64(2.8953866641040014),\n",
              "  np.float64(2.4922795980388854),\n",
              "  np.float64(2.1619762221974304),\n",
              "  np.float64(1.970081800136849),\n",
              "  np.float64(1.8894258463009241),\n",
              "  np.float64(1.7757738480417256),\n",
              "  np.float64(1.7167127969120508),\n",
              "  np.float64(1.612746052058902),\n",
              "  np.float64(1.587489337582136),\n",
              "  np.float64(1.4891893170593409),\n",
              "  np.float64(1.4468596871280575),\n",
              "  np.float64(1.4020809763627835),\n",
              "  np.float64(1.3915461161834075),\n",
              "  np.float64(1.3327333964793393),\n",
              "  np.float64(1.2975474465743582),\n",
              "  np.float64(1.2722061043374315),\n",
              "  np.float64(1.244678319920999),\n",
              "  np.float64(1.2202467172033646),\n",
              "  np.float64(1.201370851175033),\n",
              "  np.float64(1.1831046792817421),\n",
              "  np.float64(1.1660863718308905),\n",
              "  np.float64(1.1480681805592567),\n",
              "  np.float64(1.1292540464561043),\n",
              "  np.float64(1.1120620110022292),\n",
              "  np.float64(1.0976202272440376),\n",
              "  np.float64(1.0821428597991998),\n",
              "  np.float64(1.0694519158063187),\n",
              "  np.float64(1.0565032708377886),\n",
              "  np.float64(1.0447427895251276),\n",
              "  np.float64(1.029675772775532),\n",
              "  np.float64(1.0185447207957388),\n",
              "  np.float64(1.006074712548049),\n",
              "  np.float64(0.9949598187162297),\n",
              "  np.float64(0.9831159374837539),\n",
              "  np.float64(0.9726771591569425),\n",
              "  np.float64(0.9625003482351202),\n",
              "  np.float64(0.9525127364166042),\n",
              "  np.float64(0.941925762664297),\n",
              "  np.float64(0.9322107819868515),\n",
              "  np.float64(0.9224050765502984),\n",
              "  np.float64(0.9130212514483664),\n",
              "  np.float64(0.9043530032504251),\n",
              "  np.float64(0.8941349533905794),\n",
              "  np.float64(0.8857395968903103),\n",
              "  np.float64(0.876358356967299),\n",
              "  np.float64(0.8680782118866132),\n",
              "  np.float64(0.8602531708369168),\n",
              "  np.float64(0.8511864942715922),\n",
              "  np.float64(0.84315435635119),\n",
              "  np.float64(0.8357657621364689),\n",
              "  np.float64(0.8275871290114507),\n",
              "  np.float64(0.8200678835240804),\n",
              "  np.float64(0.8119760594653428),\n",
              "  np.float64(0.8044806539309369),\n",
              "  np.float64(0.7968924656048118),\n",
              "  np.float64(0.7895217999659317),\n",
              "  np.float64(0.7822315809644625),\n",
              "  np.float64(0.7752288767634681),\n",
              "  np.float64(0.7681811724172749),\n",
              "  np.float64(0.761366092748753),\n",
              "  np.float64(0.7543436401844064),\n",
              "  np.float64(0.7475271440657031),\n",
              "  np.float64(0.7409142600501277),\n",
              "  np.float64(0.7345238102571497),\n",
              "  np.float64(0.7279861834301439),\n",
              "  np.float64(0.7214063212213135),\n",
              "  np.float64(0.7152469280027501),\n",
              "  np.float64(0.7088139463567456),\n",
              "  np.float64(0.7025684288466086),\n",
              "  np.float64(0.6964123982593315),\n",
              "  np.float64(0.6901711691488606),\n",
              "  np.float64(0.6842086298661838),\n",
              "  np.float64(0.6783614964510328),\n",
              "  np.float64(0.6723020908628643),\n",
              "  np.float64(0.6664292874281913),\n",
              "  np.float64(0.6608231151690457),\n",
              "  np.float64(0.6550662450623262),\n",
              "  np.float64(0.649330608692605),\n",
              "  np.float64(0.6437095200537232),\n",
              "  np.float64(0.6381620926463144),\n",
              "  np.float64(0.6327169255625497),\n",
              "  np.float64(0.6272816969186982),\n",
              "  np.float64(0.6219948221528354),\n",
              "  np.float64(0.6165195897525153),\n",
              "  np.float64(0.6115908761200228),\n",
              "  np.float64(0.6062867763377636),\n",
              "  np.float64(0.600924087864093),\n",
              "  np.float64(0.5958996538104421),\n",
              "  np.float64(0.5909290016168103),\n",
              "  np.float64(0.5859286202220314),\n",
              "  np.float64(0.5811317598650122),\n",
              "  np.float64(0.5761159480117922),\n",
              "  np.float64(0.571368948388),\n",
              "  np.float64(0.5666285000074224),\n",
              "  np.float64(0.5617647964489523),\n",
              "  np.float64(0.5571519398944765),\n",
              "  np.float64(0.5525638844992286),\n",
              "  np.float64(0.5479407528900918),\n",
              "  np.float64(0.543507495993037),\n",
              "  np.float64(0.539014003619169),\n",
              "  np.float64(0.5347205715423975),\n",
              "  np.float64(0.5301275572568821),\n",
              "  np.float64(0.525860292562485),\n",
              "  np.float64(0.5214907746502984),\n",
              "  np.float64(0.5172405712349242),\n",
              "  np.float64(0.5130647050088408),\n",
              "  np.float64(0.5088971133248387),\n",
              "  np.float64(0.5047852283037267),\n",
              "  np.float64(0.500604425940707),\n",
              "  np.float64(0.4966253792434686),\n",
              "  np.float64(0.4925968061915725),\n",
              "  np.float64(0.4885863514705011),\n",
              "  np.float64(0.48461123029108943),\n",
              "  np.float64(0.480724447890328),\n",
              "  np.float64(0.4769394503863549),\n",
              "  np.float64(0.47312063525173687),\n",
              "  np.float64(0.46941700902433864),\n",
              "  np.float64(0.465619545009392),\n",
              "  np.float64(0.4619320917226021),\n",
              "  np.float64(0.4582756164413213),\n",
              "  np.float64(0.45459438532671553),\n",
              "  np.float64(0.4511285145524544),\n",
              "  np.float64(0.44747304023125456),\n",
              "  np.float64(0.4440889413612969),\n",
              "  np.float64(0.4405456361476836),\n",
              "  np.float64(0.4370602530529241),\n",
              "  np.float64(0.43369599469057357),\n",
              "  np.float64(0.4302965015069004),\n",
              "  np.float64(0.4269507681647259),\n",
              "  np.float64(0.4238265000552506),\n",
              "  np.float64(0.4204597967497097),\n",
              "  np.float64(0.4170590385735745),\n",
              "  np.float64(0.4140070207315619),\n",
              "  np.float64(0.4107872065764531),\n",
              "  np.float64(0.4077688268300945),\n",
              "  np.float64(0.4045345680196393),\n",
              "  np.float64(0.4014822156454012),\n",
              "  np.float64(0.3983431123561391),\n",
              "  np.float64(0.39534568925196545),\n",
              "  np.float64(0.3925585801810093),\n",
              "  np.float64(0.38932889308940216),\n",
              "  np.float64(0.3864245361382763),\n",
              "  np.float64(0.38352201844085043),\n",
              "  np.float64(0.38084426427829554),\n",
              "  np.float64(0.3778002167026503),\n",
              "  np.float64(0.3750011295700585),\n",
              "  np.float64(0.3723309940522808),\n",
              "  np.float64(0.36958783089156433),\n",
              "  np.float64(0.3667131112201757),\n",
              "  np.float64(0.3639675926076163),\n",
              "  np.float64(0.361261603927798),\n",
              "  np.float64(0.35859856705009246),\n",
              "  np.float64(0.3559977063831139),\n",
              "  np.float64(0.3534517356575809),\n",
              "  np.float64(0.3508627769016752),\n",
              "  np.float64(0.348322384560445),\n",
              "  np.float64(0.34583268391464667),\n",
              "  np.float64(0.34324145668635214),\n",
              "  np.float64(0.34081161309782504),\n",
              "  np.float64(0.33835728955699024),\n",
              "  np.float64(0.3359738867755381),\n",
              "  np.float64(0.3335583575956708),\n",
              "  np.float64(0.33099467199821403),\n",
              "  np.float64(0.32880524981694265),\n",
              "  np.float64(0.32633271572783),\n",
              "  np.float64(0.32408301648580845),\n",
              "  np.float64(0.3217857196940037),\n",
              "  np.float64(0.31948225021237636),\n",
              "  np.float64(0.31719796563289554),\n",
              "  np.float64(0.3150347032642422),\n",
              "  np.float64(0.3126964605057805),\n",
              "  np.float64(0.31053281268813726),\n",
              "  np.float64(0.30852295231757315),\n",
              "  np.float64(0.3064196052270224),\n",
              "  np.float64(0.30405802105584356),\n",
              "  np.float64(0.3023623556947479),\n",
              "  np.float64(0.2999679219808967),\n",
              "  np.float64(0.29796470446991574),\n",
              "  np.float64(0.2957611548168559),\n",
              "  np.float64(0.29378409168662856),\n",
              "  np.float64(0.29182689246553106),\n",
              "  np.float64(0.28976679704325203),\n",
              "  np.float64(0.2879534973809882),\n",
              "  np.float64(0.28587190444301136),\n",
              "  np.float64(0.2838623459055237),\n",
              "  np.float64(0.28225624832406193),\n",
              "  np.float64(0.2801312476181282),\n",
              "  np.float64(0.278274193273612),\n",
              "  np.float64(0.2764420086712167),\n",
              "  np.float64(0.27448829485897047),\n",
              "  np.float64(0.2726872678525152),\n",
              "  np.float64(0.2709002970571852),\n",
              "  np.float64(0.2693291031789611),\n",
              "  np.float64(0.26742209204369427),\n",
              "  np.float64(0.26572867489148855),\n",
              "  np.float64(0.26393886936363276),\n",
              "  np.float64(0.2622163253577332),\n",
              "  np.float64(0.2606605106341803),\n",
              "  np.float64(0.25899769460972744),\n",
              "  np.float64(0.25720694908236436),\n",
              "  np.float64(0.2555168616475448),\n",
              "  np.float64(0.25387921020858767),\n",
              "  np.float64(0.2524666693644133),\n",
              "  np.float64(0.2510680937687893),\n",
              "  np.float64(0.24910307442199006),\n",
              "  np.float64(0.247488365140716),\n",
              "  np.float64(0.24602966839016635),\n",
              "  np.float64(0.24443905967084026),\n",
              "  np.float64(0.2428842477941346),\n",
              "  np.float64(0.24149864829321055),\n",
              "  np.float64(0.24003181090818057),\n",
              "  np.float64(0.23844733550222247),\n",
              "  np.float64(0.2369816314351822),\n",
              "  np.float64(0.23556746014499413),\n",
              "  np.float64(0.23425000056336692),\n",
              "  np.float64(0.2329269676893306),\n",
              "  np.float64(0.23120627907216837),\n",
              "  np.float64(0.22986437132790646),\n",
              "  np.float64(0.22846062112091023),\n",
              "  np.float64(0.22715013091031855),\n",
              "  np.float64(0.22579461101110082),\n",
              "  np.float64(0.22450615892207587),\n",
              "  np.float64(0.22315715239558542),\n",
              "  np.float64(0.2218680887284607),\n",
              "  np.float64(0.22058074975126696),\n",
              "  np.float64(0.21927649348701583),\n",
              "  np.float64(0.2179584876897378),\n",
              "  np.float64(0.21678505027401054),\n",
              "  np.float64(0.21560596481550798),\n",
              "  np.float64(0.2143159034190677),\n",
              "  np.float64(0.21310156108383937),\n",
              "  np.float64(0.21181415042268972),\n",
              "  np.float64(0.2107209469672005),\n",
              "  np.float64(0.2094106545086832),\n",
              "  np.float64(0.20830126723879652),\n",
              "  np.float64(0.20735162228524945),\n",
              "  np.float64(0.2060904101423051),\n",
              "  np.float64(0.20476384082069152),\n",
              "  np.float64(0.20384004026728453),\n",
              "  np.float64(0.20257047024934072),\n",
              "  np.float64(0.20149972554306553),\n",
              "  np.float64(0.20036846908559894),\n",
              "  np.float64(0.19960621601537784),\n",
              "  np.float64(0.19818111118771226),\n",
              "  np.float64(0.19722728702008757),\n",
              "  np.float64(0.19618707528171572),\n",
              "  np.float64(0.19509104252358378),\n",
              "  np.float64(0.19410312567323582),\n",
              "  np.float64(0.19309525291998234),\n",
              "  np.float64(0.1921139586178192),\n",
              "  np.float64(0.19111117110283238),\n",
              "  np.float64(0.1899998588426611),\n",
              "  np.float64(0.18909519758260854),\n",
              "  np.float64(0.1881494985341675),\n",
              "  np.float64(0.18731962344611378),\n",
              "  np.float64(0.1862563565509251),\n",
              "  np.float64(0.1853702911627573),\n",
              "  np.float64(0.18434745542766223),\n",
              "  np.float64(0.183373311075825),\n",
              "  np.float64(0.18263297972251527),\n",
              "  np.float64(0.1816670713571194),\n",
              "  np.float64(0.18102637366026422),\n",
              "  np.float64(0.18003899920990726),\n",
              "  np.float64(0.17893545386631005),\n",
              "  np.float64(0.17803800835695086),\n",
              "  np.float64(0.17724292571562006),\n",
              "  np.float64(0.17638555353850627),\n",
              "  np.float64(0.17552490813473035),\n",
              "  np.float64(0.1746431354240359),\n",
              "  np.float64(0.17395874181646967),\n",
              "  np.float64(0.1731586279630932),\n",
              "  np.float64(0.17220596080228118),\n",
              "  np.float64(0.17146012490666374),\n",
              "  np.float64(0.17074425049979083),\n",
              "  np.float64(0.16986747593682888),\n",
              "  np.float64(0.1693415713592),\n",
              "  np.float64(0.1682543117648992),\n",
              "  np.float64(0.1676521326296229),\n",
              "  np.float64(0.16679025088594954),\n",
              "  np.float64(0.16618340688657873),\n",
              "  np.float64(0.16547488618641462),\n",
              "  np.float64(0.16479353786406675),\n",
              "  np.float64(0.16393897403958727),\n",
              "  np.float64(0.16317068219200734),\n",
              "  np.float64(0.16247045404312552),\n",
              "  np.float64(0.16184721714632191),\n",
              "  np.float64(0.1611014454064717),\n",
              "  np.float64(0.1603705501664093),\n",
              "  np.float64(0.15957017230071685),\n",
              "  np.float64(0.15899652869629038),\n",
              "  np.float64(0.15827431095880581),\n",
              "  np.float64(0.15771844882790104),\n",
              "  np.float64(0.15714057181140872),\n",
              "  np.float64(0.15635788638922118),\n",
              "  np.float64(0.15578808676125983),\n",
              "  np.float64(0.15504110869591697),\n",
              "  np.float64(0.15446760894345218),\n",
              "  np.float64(0.1539187913523163),\n",
              "  np.float64(0.15317458380346863),\n",
              "  np.float64(0.15249511996120754),\n",
              "  np.float64(0.15197376255709838),\n",
              "  np.float64(0.1514035025527707),\n",
              "  np.float64(0.15078898822798853),\n",
              "  np.float64(0.15015663282691896),\n",
              "  np.float64(0.14959297167608976),\n",
              "  np.float64(0.14904104461510365),\n",
              "  np.float64(0.14846072539597982),\n",
              "  np.float64(0.14788130933142052),\n",
              "  np.float64(0.1473613770428636),\n",
              "  np.float64(0.1467681823012253),\n",
              "  np.float64(0.14621272630607138),\n",
              "  np.float64(0.14570945769157584),\n",
              "  np.float64(0.14530525592030852),\n",
              "  np.float64(0.14450084650698497),\n",
              "  np.float64(0.14404784438245832),\n",
              "  np.float64(0.1436904628741349),\n",
              "  np.float64(0.1429888259091831),\n",
              "  np.float64(0.14258487668585915),\n",
              "  np.float64(0.14210575506563802),\n",
              "  np.float64(0.14148760743629696),\n",
              "  np.float64(0.1409217416533782),\n",
              "  np.float64(0.14063774563436976),\n",
              "  np.float64(0.13999776429841881),\n",
              "  np.float64(0.13955784369604635),\n",
              "  np.float64(0.13916726960002254),\n",
              "  np.float64(0.1384830198648262),\n",
              "  np.float64(0.13807924267995728),\n",
              "  np.float64(0.13753234941333412),\n",
              "  np.float64(0.13714345392455046),\n",
              "  np.float64(0.1365821930936538),\n",
              "  np.float64(0.13650611849242428),\n",
              "  np.float64(0.13602439712227998),\n",
              "  np.float64(0.13534342648897937),\n",
              "  np.float64(0.13492021904318013),\n",
              "  np.float64(0.13445905527825117),\n",
              "  np.float64(0.13419008964860973),\n",
              "  np.float64(0.133595966407324),\n",
              "  np.float64(0.133286248915948),\n",
              "  np.float64(0.1328930383505031),\n",
              "  np.float64(0.13238377702147947),\n",
              "  np.float64(0.13200130104335064),\n",
              "  np.float64(0.1317691096190335),\n",
              "  np.float64(0.1313062488518219),\n",
              "  np.float64(0.1305919569529912),\n",
              "  np.float64(0.1303450990054995),\n",
              "  np.float64(0.13017110181395902),\n",
              "  np.float64(0.12958206795589902),\n",
              "  np.float64(0.12923499961142795),\n",
              "  np.float64(0.1286614593712025),\n",
              "  np.float64(0.12840392001672427),\n",
              "  np.float64(0.12802908909372052),\n",
              "  np.float64(0.12764319700250296),\n",
              "  np.float64(0.12731440867552737),\n",
              "  np.float64(0.1269286981774702),\n",
              "  np.float64(0.126664811460222),\n",
              "  np.float64(0.1261416310091203),\n",
              "  np.float64(0.12584376090067198),\n",
              "  np.float64(0.12547050051091135),\n",
              "  np.float64(0.12500243992470006),\n",
              "  np.float64(0.12481653280729418),\n",
              "  np.float64(0.12441124883461827),\n",
              "  np.float64(0.12425166617339078),\n",
              "  np.float64(0.12371988912701948),\n",
              "  np.float64(0.12342494583138576),\n",
              "  np.float64(0.12305648915161826),\n",
              "  np.float64(0.12280993828390915),\n",
              "  np.float64(0.12250275175593191),\n",
              "  np.float64(0.1220809014471368),\n",
              "  np.float64(0.12191867442529838),\n",
              "  np.float64(0.12155036677322173),\n",
              "  np.float64(0.12112334517630874),\n",
              "  np.float64(0.12080859133299672),\n",
              "  np.float64(0.12056184762099578),\n",
              "  np.float64(0.12019686454694711),\n",
              "  np.float64(0.12004947977229463),\n",
              "  np.float64(0.11956530562774197),\n",
              "  np.float64(0.11962488561042144),\n",
              "  np.float64(0.11897183651571769),\n",
              "  np.float64(0.1188103249908498),\n",
              "  np.float64(0.11837554802040431),\n",
              "  np.float64(0.11810257287532662),\n",
              "  np.float64(0.11794277026153316),\n",
              "  np.float64(0.11756183690136782),\n",
              "  np.float64(0.11742372499492787),\n",
              "  np.float64(0.11709198244151678),\n",
              "  np.float64(0.11676530290504777),\n",
              "  np.float64(0.11646139948919121),\n",
              "  np.float64(0.11615172904435606),\n",
              "  np.float64(0.11596824637856618),\n",
              "  np.float64(0.11564881513276175),\n",
              "  np.float64(0.11544102146238532),\n",
              "  np.float64(0.11508397664579054),\n",
              "  np.float64(0.11488024212269121),\n",
              "  np.float64(0.11466869915302236),\n",
              "  np.float64(0.11433591063041842),\n",
              "  np.float64(0.11411593006876543),\n",
              "  np.float64(0.11395159819262317),\n",
              "  np.float64(0.11363176487528329),\n",
              "  np.float64(0.11347625460700184),\n",
              "  np.float64(0.11346989760277112)],\n",
              " [689,\n",
              "  781,\n",
              "  848,\n",
              "  881,\n",
              "  899,\n",
              "  914,\n",
              "  928,\n",
              "  945,\n",
              "  953,\n",
              "  962,\n",
              "  960,\n",
              "  973,\n",
              "  969,\n",
              "  979,\n",
              "  981,\n",
              "  982,\n",
              "  986,\n",
              "  989,\n",
              "  990,\n",
              "  993,\n",
              "  992,\n",
              "  996,\n",
              "  994,\n",
              "  996,\n",
              "  996,\n",
              "  996,\n",
              "  996,\n",
              "  997,\n",
              "  997,\n",
              "  998,\n",
              "  997,\n",
              "  998,\n",
              "  998,\n",
              "  998,\n",
              "  998,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  999,\n",
              "  1000,\n",
              "  999,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000,\n",
              "  1000])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 5a: Full MNIST without regularization\n"
      ],
      "metadata": {
        "id": "-BtCvstWvLR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_data, validation_data, test_data =load_data_wrapper()\n",
        "\n",
        "net = Network([784, 30, 10], cost=CrossEntropyCost)\n",
        "net.large_weight_initializer()\n",
        "\n",
        "net.SGD(training_data, 30, 10, 0.5,\n",
        "        evaluation_data=test_data,\n",
        "        monitor_evaluation_accuracy=True,\n",
        "        monitor_training_accuracy=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phfml1uFvLbQ",
        "outputId": "c55068d5-3b64-4641-88ed-7cca543f5c2f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Accuracy on training data: 45790 / 50000\n",
            "Accuracy on evaluation data: 9157 / 10000\n",
            "Epoch 1 training complete\n",
            "Accuracy on training data: 46518 / 50000\n",
            "Accuracy on evaluation data: 9276 / 10000\n",
            "Epoch 2 training complete\n",
            "Accuracy on training data: 47076 / 50000\n",
            "Accuracy on evaluation data: 9338 / 10000\n",
            "Epoch 3 training complete\n",
            "Accuracy on training data: 47357 / 50000\n",
            "Accuracy on evaluation data: 9413 / 10000\n",
            "Epoch 4 training complete\n",
            "Accuracy on training data: 47524 / 50000\n",
            "Accuracy on evaluation data: 9426 / 10000\n",
            "Epoch 5 training complete\n",
            "Accuracy on training data: 47704 / 50000\n",
            "Accuracy on evaluation data: 9442 / 10000\n",
            "Epoch 6 training complete\n",
            "Accuracy on training data: 47860 / 50000\n",
            "Accuracy on evaluation data: 9486 / 10000\n",
            "Epoch 7 training complete\n",
            "Accuracy on training data: 47893 / 50000\n",
            "Accuracy on evaluation data: 9469 / 10000\n",
            "Epoch 8 training complete\n",
            "Accuracy on training data: 48138 / 50000\n",
            "Accuracy on evaluation data: 9514 / 10000\n",
            "Epoch 9 training complete\n",
            "Accuracy on training data: 48045 / 50000\n",
            "Accuracy on evaluation data: 9492 / 10000\n",
            "Epoch 10 training complete\n",
            "Accuracy on training data: 48204 / 50000\n",
            "Accuracy on evaluation data: 9494 / 10000\n",
            "Epoch 11 training complete\n",
            "Accuracy on training data: 48262 / 50000\n",
            "Accuracy on evaluation data: 9499 / 10000\n",
            "Epoch 12 training complete\n",
            "Accuracy on training data: 48346 / 50000\n",
            "Accuracy on evaluation data: 9502 / 10000\n",
            "Epoch 13 training complete\n",
            "Accuracy on training data: 48364 / 50000\n",
            "Accuracy on evaluation data: 9502 / 10000\n",
            "Epoch 14 training complete\n",
            "Accuracy on training data: 48446 / 50000\n",
            "Accuracy on evaluation data: 9510 / 10000\n",
            "Epoch 15 training complete\n",
            "Accuracy on training data: 48574 / 50000\n",
            "Accuracy on evaluation data: 9540 / 10000\n",
            "Epoch 16 training complete\n",
            "Accuracy on training data: 48611 / 50000\n",
            "Accuracy on evaluation data: 9529 / 10000\n",
            "Epoch 17 training complete\n",
            "Accuracy on training data: 48504 / 50000\n",
            "Accuracy on evaluation data: 9506 / 10000\n",
            "Epoch 18 training complete\n",
            "Accuracy on training data: 48622 / 50000\n",
            "Accuracy on evaluation data: 9510 / 10000\n",
            "Epoch 19 training complete\n",
            "Accuracy on training data: 48628 / 50000\n",
            "Accuracy on evaluation data: 9525 / 10000\n",
            "Epoch 20 training complete\n",
            "Accuracy on training data: 48635 / 50000\n",
            "Accuracy on evaluation data: 9538 / 10000\n",
            "Epoch 21 training complete\n",
            "Accuracy on training data: 48751 / 50000\n",
            "Accuracy on evaluation data: 9533 / 10000\n",
            "Epoch 22 training complete\n",
            "Accuracy on training data: 48740 / 50000\n",
            "Accuracy on evaluation data: 9539 / 10000\n",
            "Epoch 23 training complete\n",
            "Accuracy on training data: 48818 / 50000\n",
            "Accuracy on evaluation data: 9516 / 10000\n",
            "Epoch 24 training complete\n",
            "Accuracy on training data: 48789 / 50000\n",
            "Accuracy on evaluation data: 9518 / 10000\n",
            "Epoch 25 training complete\n",
            "Accuracy on training data: 48792 / 50000\n",
            "Accuracy on evaluation data: 9516 / 10000\n",
            "Epoch 26 training complete\n",
            "Accuracy on training data: 48852 / 50000\n",
            "Accuracy on evaluation data: 9521 / 10000\n",
            "Epoch 27 training complete\n",
            "Accuracy on training data: 48882 / 50000\n",
            "Accuracy on evaluation data: 9509 / 10000\n",
            "Epoch 28 training complete\n",
            "Accuracy on training data: 48848 / 50000\n",
            "Accuracy on evaluation data: 9521 / 10000\n",
            "Epoch 29 training complete\n",
            "Accuracy on training data: 48959 / 50000\n",
            "Accuracy on evaluation data: 9528 / 10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([],\n",
              " [9157,\n",
              "  9276,\n",
              "  9338,\n",
              "  9413,\n",
              "  9426,\n",
              "  9442,\n",
              "  9486,\n",
              "  9469,\n",
              "  9514,\n",
              "  9492,\n",
              "  9494,\n",
              "  9499,\n",
              "  9502,\n",
              "  9502,\n",
              "  9510,\n",
              "  9540,\n",
              "  9529,\n",
              "  9506,\n",
              "  9510,\n",
              "  9525,\n",
              "  9538,\n",
              "  9533,\n",
              "  9539,\n",
              "  9516,\n",
              "  9518,\n",
              "  9516,\n",
              "  9521,\n",
              "  9509,\n",
              "  9521,\n",
              "  9528],\n",
              " [],\n",
              " [45790,\n",
              "  46518,\n",
              "  47076,\n",
              "  47357,\n",
              "  47524,\n",
              "  47704,\n",
              "  47860,\n",
              "  47893,\n",
              "  48138,\n",
              "  48045,\n",
              "  48204,\n",
              "  48262,\n",
              "  48346,\n",
              "  48364,\n",
              "  48446,\n",
              "  48574,\n",
              "  48611,\n",
              "  48504,\n",
              "  48622,\n",
              "  48628,\n",
              "  48635,\n",
              "  48751,\n",
              "  48740,\n",
              "  48818,\n",
              "  48789,\n",
              "  48792,\n",
              "  48852,\n",
              "  48882,\n",
              "  48848,\n",
              "  48959])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 5b: Full MNIST with L2 regularization\n"
      ],
      "metadata": {
        "id": "98XqtwWqvLv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "training_data, validation_data, test_data = load_data_wrapper()\n",
        "\n",
        "net =Network([784, 30, 10], cost=CrossEntropyCost)\n",
        "net.large_weight_initializer()\n",
        "\n",
        "net.SGD(training_data, 30, 10, 0.5,\n",
        "        lmbda=5.0,\n",
        "        evaluation_data=test_data,\n",
        "        monitor_evaluation_accuracy=True,\n",
        "        monitor_training_accuracy=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKvtjM5yvL94",
        "outputId": "56379583-7d2d-466e-ace5-61262bd022d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Accuracy on training data: 45638 / 50000\n",
            "Accuracy on evaluation data: 9108 / 10000\n",
            "Epoch 1 training complete\n",
            "Accuracy on training data: 46631 / 50000\n",
            "Accuracy on evaluation data: 9298 / 10000\n",
            "Epoch 2 training complete\n",
            "Accuracy on training data: 47378 / 50000\n",
            "Accuracy on evaluation data: 9444 / 10000\n",
            "Epoch 3 training complete\n",
            "Accuracy on training data: 47640 / 50000\n",
            "Accuracy on evaluation data: 9426 / 10000\n",
            "Epoch 4 training complete\n",
            "Accuracy on training data: 47987 / 50000\n",
            "Accuracy on evaluation data: 9540 / 10000\n",
            "Epoch 5 training complete\n",
            "Accuracy on training data: 47931 / 50000\n",
            "Accuracy on evaluation data: 9523 / 10000\n",
            "Epoch 6 training complete\n",
            "Accuracy on training data: 48100 / 50000\n",
            "Accuracy on evaluation data: 9532 / 10000\n",
            "Epoch 7 training complete\n",
            "Accuracy on training data: 48104 / 50000\n",
            "Accuracy on evaluation data: 9550 / 10000\n",
            "Epoch 8 training complete\n",
            "Accuracy on training data: 48023 / 50000\n",
            "Accuracy on evaluation data: 9519 / 10000\n",
            "Epoch 9 training complete\n",
            "Accuracy on training data: 48065 / 50000\n",
            "Accuracy on evaluation data: 9536 / 10000\n",
            "Epoch 10 training complete\n",
            "Accuracy on training data: 48093 / 50000\n",
            "Accuracy on evaluation data: 9548 / 10000\n",
            "Epoch 11 training complete\n",
            "Accuracy on training data: 48077 / 50000\n",
            "Accuracy on evaluation data: 9517 / 10000\n",
            "Epoch 12 training complete\n",
            "Accuracy on training data: 48319 / 50000\n",
            "Accuracy on evaluation data: 9597 / 10000\n",
            "Epoch 13 training complete\n",
            "Accuracy on training data: 48195 / 50000\n",
            "Accuracy on evaluation data: 9554 / 10000\n",
            "Epoch 14 training complete\n",
            "Accuracy on training data: 48310 / 50000\n",
            "Accuracy on evaluation data: 9592 / 10000\n",
            "Epoch 15 training complete\n",
            "Accuracy on training data: 48289 / 50000\n",
            "Accuracy on evaluation data: 9580 / 10000\n",
            "Epoch 16 training complete\n",
            "Accuracy on training data: 48316 / 50000\n",
            "Accuracy on evaluation data: 9574 / 10000\n",
            "Epoch 17 training complete\n",
            "Accuracy on training data: 48160 / 50000\n",
            "Accuracy on evaluation data: 9575 / 10000\n",
            "Epoch 18 training complete\n",
            "Accuracy on training data: 48233 / 50000\n",
            "Accuracy on evaluation data: 9572 / 10000\n",
            "Epoch 19 training complete\n",
            "Accuracy on training data: 48368 / 50000\n",
            "Accuracy on evaluation data: 9615 / 10000\n",
            "Epoch 20 training complete\n",
            "Accuracy on training data: 48090 / 50000\n",
            "Accuracy on evaluation data: 9552 / 10000\n",
            "Epoch 21 training complete\n",
            "Accuracy on training data: 48353 / 50000\n",
            "Accuracy on evaluation data: 9595 / 10000\n",
            "Epoch 22 training complete\n",
            "Accuracy on training data: 48163 / 50000\n",
            "Accuracy on evaluation data: 9563 / 10000\n",
            "Epoch 23 training complete\n",
            "Accuracy on training data: 48464 / 50000\n",
            "Accuracy on evaluation data: 9622 / 10000\n",
            "Epoch 24 training complete\n",
            "Accuracy on training data: 48365 / 50000\n",
            "Accuracy on evaluation data: 9570 / 10000\n",
            "Epoch 25 training complete\n",
            "Accuracy on training data: 48425 / 50000\n",
            "Accuracy on evaluation data: 9616 / 10000\n",
            "Epoch 26 training complete\n",
            "Accuracy on training data: 48270 / 50000\n",
            "Accuracy on evaluation data: 9563 / 10000\n",
            "Epoch 27 training complete\n",
            "Accuracy on training data: 48528 / 50000\n",
            "Accuracy on evaluation data: 9647 / 10000\n",
            "Epoch 28 training complete\n",
            "Accuracy on training data: 48473 / 50000\n",
            "Accuracy on evaluation data: 9619 / 10000\n",
            "Epoch 29 training complete\n",
            "Accuracy on training data: 48365 / 50000\n",
            "Accuracy on evaluation data: 9613 / 10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([],\n",
              " [9108,\n",
              "  9298,\n",
              "  9444,\n",
              "  9426,\n",
              "  9540,\n",
              "  9523,\n",
              "  9532,\n",
              "  9550,\n",
              "  9519,\n",
              "  9536,\n",
              "  9548,\n",
              "  9517,\n",
              "  9597,\n",
              "  9554,\n",
              "  9592,\n",
              "  9580,\n",
              "  9574,\n",
              "  9575,\n",
              "  9572,\n",
              "  9615,\n",
              "  9552,\n",
              "  9595,\n",
              "  9563,\n",
              "  9622,\n",
              "  9570,\n",
              "  9616,\n",
              "  9563,\n",
              "  9647,\n",
              "  9619,\n",
              "  9613],\n",
              " [],\n",
              " [45638,\n",
              "  46631,\n",
              "  47378,\n",
              "  47640,\n",
              "  47987,\n",
              "  47931,\n",
              "  48100,\n",
              "  48104,\n",
              "  48023,\n",
              "  48065,\n",
              "  48093,\n",
              "  48077,\n",
              "  48319,\n",
              "  48195,\n",
              "  48310,\n",
              "  48289,\n",
              "  48316,\n",
              "  48160,\n",
              "  48233,\n",
              "  48368,\n",
              "  48090,\n",
              "  48353,\n",
              "  48163,\n",
              "  48464,\n",
              "  48365,\n",
              "  48425,\n",
              "  48270,\n",
              "  48528,\n",
              "  48473,\n",
              "  48365])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 5c: 100 Hidden Neurons, L2 regularization\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gk4rYBQgvMG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "training_data, validation_data, test_data = load_data_wrapper()\n",
        "\n",
        "net = Network([784, 100, 10], cost=CrossEntropyCost)\n",
        "net.large_weight_initializer()\n",
        "\n",
        "net.SGD(training_data, 30, 10, 0.5,\n",
        "        lmbda=5.0,\n",
        "        evaluation_data=validation_data,\n",
        "        monitor_evaluation_accuracy=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw6UWiLzvMQf",
        "outputId": "5f47e4c4-1d2c-453c-844a-780931a2b369"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 9389 / 10000\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 9488 / 10000\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 9608 / 10000\n",
            "Epoch 3 training complete\n",
            "Accuracy on evaluation data: 9649 / 10000\n",
            "Epoch 4 training complete\n",
            "Accuracy on evaluation data: 9699 / 10000\n",
            "Epoch 5 training complete\n",
            "Accuracy on evaluation data: 9703 / 10000\n",
            "Epoch 6 training complete\n",
            "Accuracy on evaluation data: 9713 / 10000\n",
            "Epoch 7 training complete\n",
            "Accuracy on evaluation data: 9718 / 10000\n",
            "Epoch 8 training complete\n",
            "Accuracy on evaluation data: 9768 / 10000\n",
            "Epoch 9 training complete\n",
            "Accuracy on evaluation data: 9736 / 10000\n",
            "Epoch 10 training complete\n",
            "Accuracy on evaluation data: 9652 / 10000\n",
            "Epoch 11 training complete\n",
            "Accuracy on evaluation data: 9734 / 10000\n",
            "Epoch 12 training complete\n",
            "Accuracy on evaluation data: 9735 / 10000\n",
            "Epoch 13 training complete\n",
            "Accuracy on evaluation data: 9652 / 10000\n",
            "Epoch 14 training complete\n",
            "Accuracy on evaluation data: 9755 / 10000\n",
            "Epoch 15 training complete\n",
            "Accuracy on evaluation data: 9723 / 10000\n",
            "Epoch 16 training complete\n",
            "Accuracy on evaluation data: 9743 / 10000\n",
            "Epoch 17 training complete\n",
            "Accuracy on evaluation data: 9729 / 10000\n",
            "Epoch 18 training complete\n",
            "Accuracy on evaluation data: 9773 / 10000\n",
            "Epoch 19 training complete\n",
            "Accuracy on evaluation data: 9770 / 10000\n",
            "Epoch 20 training complete\n",
            "Accuracy on evaluation data: 9769 / 10000\n",
            "Epoch 21 training complete\n",
            "Accuracy on evaluation data: 9764 / 10000\n",
            "Epoch 22 training complete\n",
            "Accuracy on evaluation data: 9771 / 10000\n",
            "Epoch 23 training complete\n",
            "Accuracy on evaluation data: 9662 / 10000\n",
            "Epoch 24 training complete\n",
            "Accuracy on evaluation data: 9769 / 10000\n",
            "Epoch 25 training complete\n",
            "Accuracy on evaluation data: 9766 / 10000\n",
            "Epoch 26 training complete\n",
            "Accuracy on evaluation data: 9740 / 10000\n",
            "Epoch 27 training complete\n",
            "Accuracy on evaluation data: 9759 / 10000\n",
            "Epoch 28 training complete\n",
            "Accuracy on evaluation data: 9747 / 10000\n",
            "Epoch 29 training complete\n",
            "Accuracy on evaluation data: 9754 / 10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([],\n",
              " [9389,\n",
              "  9488,\n",
              "  9608,\n",
              "  9649,\n",
              "  9699,\n",
              "  9703,\n",
              "  9713,\n",
              "  9718,\n",
              "  9768,\n",
              "  9736,\n",
              "  9652,\n",
              "  9734,\n",
              "  9735,\n",
              "  9652,\n",
              "  9755,\n",
              "  9723,\n",
              "  9743,\n",
              "  9729,\n",
              "  9773,\n",
              "  9770,\n",
              "  9769,\n",
              "  9764,\n",
              "  9771,\n",
              "  9662,\n",
              "  9769,\n",
              "  9766,\n",
              "  9740,\n",
              "  9759,\n",
              "  9747,\n",
              "  9754],\n",
              " [],\n",
              " [])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VNQHLHMYvMjL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gqzHpoFRvMqh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h1xE1oLGvMyw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PQdbiZDgvM6K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rXWZ55apvNBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w5qwqXt3vNKH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N8KhR7NHvNXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LjqeE3waGVza"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}