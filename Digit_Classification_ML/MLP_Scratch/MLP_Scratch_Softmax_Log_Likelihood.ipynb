{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H3S-oU98oyWT"
      },
      "outputs": [],
      "source": [
        "# --- Data Loading ---\n",
        "import gzip\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "def load_data():\n",
        "    f = gzip.open('/content/mnist.pkl.gz', 'rb')  # adjust path if necessary\n",
        "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
        "    f.close()\n",
        "    return training_data, validation_data, test_data\n",
        "\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = list(zip(training_inputs, training_results))\n",
        "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
        "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
        "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
        "    test_data = list(zip(test_inputs, te_d[1]))\n",
        "    return training_data, validation_data, test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# --- Cost Function ---\n",
        "class CrossEntropyCost:\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a) - (1-y)*np.log(1-a)))\n",
        "\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a - y)\n",
        "\n",
        "class LogLikelihoodCost:\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return -np.sum(np.nan_to_num(y * np.log(a)))\n",
        "\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return a - y  # same as cross-entropy, with softmax\n",
        "\n",
        "\n",
        "\n",
        "# --- Network Class ---\n",
        "class Network:\n",
        "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.default_weight_initializer()\n",
        "        self.cost = cost\n",
        "\n",
        "    def large_weight_initializer(self):\n",
        "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def default_weight_initializer(self):\n",
        "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) / np.sqrt(x)\n",
        "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "      for i in range(len(self.biases) - 1):\n",
        "          a = sigmoid(np.dot(self.weights[i], a) + self.biases[i])\n",
        "      z = np.dot(self.weights[-1], a) + self.biases[-1]\n",
        "      a = softmax(z)  # softmax only at the output layer\n",
        "      return a\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            lmbda=0.0,\n",
        "            evaluation_data=None,\n",
        "            monitor_evaluation_cost=False,\n",
        "            monitor_evaluation_accuracy=False,\n",
        "            monitor_training_cost=False,\n",
        "            monitor_training_accuracy=False):\n",
        "\n",
        "        if evaluation_data: n_data = len(evaluation_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        evaluation_cost, evaluation_accuracy = [], []\n",
        "        training_cost, training_accuracy = [], []\n",
        "\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta, lmbda, n)\n",
        "\n",
        "            print(f\"Epoch {j} training complete\")\n",
        "\n",
        "            if monitor_training_cost:\n",
        "                cost = self.total_cost(training_data, lmbda)\n",
        "                training_cost.append(cost)\n",
        "                print(f\"Cost on training data: {cost}\")\n",
        "\n",
        "            if monitor_training_accuracy:\n",
        "                accuracy = self.accuracy(training_data, convert=True)\n",
        "                training_accuracy.append(accuracy)\n",
        "                print(f\"Accuracy on training data: {accuracy} / {n}\")\n",
        "\n",
        "            if monitor_evaluation_cost:\n",
        "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                evaluation_cost.append(cost)\n",
        "                print(f\"Cost on evaluation data: {cost}\")\n",
        "\n",
        "            if monitor_evaluation_accuracy:\n",
        "                accuracy = self.accuracy(evaluation_data)\n",
        "                evaluation_accuracy.append(accuracy)\n",
        "                print(f\"Accuracy on evaluation data: {accuracy} / {n_data}\")\n",
        "\n",
        "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "\n",
        "        self.weights = [(1 - eta * (lmbda / n)) * w - (eta / len(mini_batch)) * nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b - (eta / len(mini_batch)) * nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        # Initialize gradients\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # Forward pass\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "\n",
        "        for i in range(len(self.biases) - 1):\n",
        "            z = np.dot(self.weights[i], activation) + self.biases[i]\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Output layer (softmax)\n",
        "        z = np.dot(self.weights[-1], activation) + self.biases[-1]\n",
        "        zs.append(z)\n",
        "        activation = softmax(z)\n",
        "        activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = self.cost.delta(zs[-1], activations[-1], y)\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].T)\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].T, delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].T)\n",
        "\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "\n",
        "\n",
        "    def accuracy(self, data, convert=False):\n",
        "        if convert:\n",
        "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
        "        else:\n",
        "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
        "        return sum(int(x == y) for (x, y) in results)\n",
        "\n",
        "    def total_cost(self, data, lmbda, convert=False):\n",
        "        cost = 0.0\n",
        "        for x, y in data:\n",
        "            a = self.feedforward(x)\n",
        "            if convert: y = vectorized_result(y)\n",
        "            cost += self.cost.fn(a, y) / len(data)\n",
        "        cost += 0.5 * (lmbda / len(data)) * sum(np.linalg.norm(w)**2 for w in self.weights)\n",
        "        return cost\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z))  # subtract max for numerical stability\n",
        "    return exp_z / np.sum(exp_z, axis=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "b0kFgZRJpNrX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()\n",
        "\n",
        "net = Network([784, 30, 10], cost=LogLikelihoodCost)\n",
        "net.large_weight_initializer()\n",
        "\n",
        "net.SGD(training_data, 30, 10, 0.5,\n",
        "        evaluation_data=test_data,\n",
        "        monitor_evaluation_accuracy=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klLEz7Nqqff1",
        "outputId": "03f9fab2-f150-4ee2-b2fd-558fb795540e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 9037 / 10000\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 9212 / 10000\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 9280 / 10000\n",
            "Epoch 3 training complete\n",
            "Accuracy on evaluation data: 9318 / 10000\n",
            "Epoch 4 training complete\n",
            "Accuracy on evaluation data: 9377 / 10000\n",
            "Epoch 5 training complete\n",
            "Accuracy on evaluation data: 9455 / 10000\n",
            "Epoch 6 training complete\n",
            "Accuracy on evaluation data: 9474 / 10000\n",
            "Epoch 7 training complete\n",
            "Accuracy on evaluation data: 9444 / 10000\n",
            "Epoch 8 training complete\n",
            "Accuracy on evaluation data: 9430 / 10000\n",
            "Epoch 9 training complete\n",
            "Accuracy on evaluation data: 9473 / 10000\n",
            "Epoch 10 training complete\n",
            "Accuracy on evaluation data: 9446 / 10000\n",
            "Epoch 11 training complete\n",
            "Accuracy on evaluation data: 9470 / 10000\n",
            "Epoch 12 training complete\n",
            "Accuracy on evaluation data: 9512 / 10000\n",
            "Epoch 13 training complete\n",
            "Accuracy on evaluation data: 9458 / 10000\n",
            "Epoch 14 training complete\n",
            "Accuracy on evaluation data: 9499 / 10000\n",
            "Epoch 15 training complete\n",
            "Accuracy on evaluation data: 9507 / 10000\n",
            "Epoch 16 training complete\n",
            "Accuracy on evaluation data: 9470 / 10000\n",
            "Epoch 17 training complete\n",
            "Accuracy on evaluation data: 9525 / 10000\n",
            "Epoch 18 training complete\n",
            "Accuracy on evaluation data: 9532 / 10000\n",
            "Epoch 19 training complete\n",
            "Accuracy on evaluation data: 9525 / 10000\n",
            "Epoch 20 training complete\n",
            "Accuracy on evaluation data: 9504 / 10000\n",
            "Epoch 21 training complete\n",
            "Accuracy on evaluation data: 9493 / 10000\n",
            "Epoch 22 training complete\n",
            "Accuracy on evaluation data: 9528 / 10000\n",
            "Epoch 23 training complete\n",
            "Accuracy on evaluation data: 9553 / 10000\n",
            "Epoch 24 training complete\n",
            "Accuracy on evaluation data: 9496 / 10000\n",
            "Epoch 25 training complete\n",
            "Accuracy on evaluation data: 9517 / 10000\n",
            "Epoch 26 training complete\n",
            "Accuracy on evaluation data: 9483 / 10000\n",
            "Epoch 27 training complete\n",
            "Accuracy on evaluation data: 9500 / 10000\n",
            "Epoch 28 training complete\n",
            "Accuracy on evaluation data: 9527 / 10000\n",
            "Epoch 29 training complete\n",
            "Accuracy on evaluation data: 9515 / 10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([],\n",
              " [9037,\n",
              "  9212,\n",
              "  9280,\n",
              "  9318,\n",
              "  9377,\n",
              "  9455,\n",
              "  9474,\n",
              "  9444,\n",
              "  9430,\n",
              "  9473,\n",
              "  9446,\n",
              "  9470,\n",
              "  9512,\n",
              "  9458,\n",
              "  9499,\n",
              "  9507,\n",
              "  9470,\n",
              "  9525,\n",
              "  9532,\n",
              "  9525,\n",
              "  9504,\n",
              "  9493,\n",
              "  9528,\n",
              "  9553,\n",
              "  9496,\n",
              "  9517,\n",
              "  9483,\n",
              "  9500,\n",
              "  9527,\n",
              "  9515],\n",
              " [],\n",
              " [])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N8KhR7NHvNXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LjqeE3waGVza"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}