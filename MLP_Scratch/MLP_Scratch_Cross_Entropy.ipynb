{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H3S-oU98oyWT"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "def load_data():\n",
        "    f = gzip.open('/content/mnist.pkl.gz', 'rb')\n",
        "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
        "    f.close()\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = list(zip(training_inputs, training_results))\n",
        "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
        "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
        "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
        "    test_data = list(zip(test_inputs, te_d[1]))\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class CrossEntropyCost:\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a) - (1-y)*np.log(1-a)))\n",
        "\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a - y)\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.default_weight_initializer()\n",
        "        self.cost = cost\n",
        "\n",
        "    def large_weight_initializer(self):\n",
        "      self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "      self.weights = [np.random.randn(y, x)\n",
        "                      for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "\n",
        "\n",
        "    def default_weight_initializer(self):\n",
        "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) / np.sqrt(x)\n",
        "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            lmbda=0.0,\n",
        "            evaluation_data=None,\n",
        "            monitor_evaluation_cost=False,\n",
        "            monitor_evaluation_accuracy=False,\n",
        "            monitor_training_cost=False,\n",
        "            monitor_training_accuracy=False):\n",
        "        \"\"\"Train the neural network using mini-batch SGD with optional evaluation and monitoring.\"\"\"\n",
        "\n",
        "        if evaluation_data: n_data = len(evaluation_data)\n",
        "        n = len(training_data)\n",
        "\n",
        "        evaluation_cost, evaluation_accuracy = [], []\n",
        "        training_cost, training_accuracy = [], []\n",
        "\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta, lmbda, n)\n",
        "\n",
        "            print(f\"Epoch {j} training complete\")\n",
        "\n",
        "            if monitor_training_cost:\n",
        "                cost = self.total_cost(training_data, lmbda)\n",
        "                training_cost.append(cost)\n",
        "                print(f\"Cost on training data: {cost}\")\n",
        "\n",
        "            if monitor_training_accuracy:\n",
        "                accuracy = self.accuracy(training_data, convert=True)\n",
        "                training_accuracy.append(accuracy)\n",
        "                print(f\"Accuracy on training data: {accuracy} / {n}\")\n",
        "\n",
        "            if monitor_evaluation_cost:\n",
        "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                evaluation_cost.append(cost)\n",
        "                print(f\"Cost on evaluation data: {cost}\")\n",
        "\n",
        "            if monitor_evaluation_accuracy:\n",
        "                accuracy = self.accuracy(evaluation_data)\n",
        "                evaluation_accuracy.append(accuracy)\n",
        "                print(f\"Accuracy on evaluation data: {accuracy} / {n_data}\")\n",
        "\n",
        "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "\n",
        "        self.weights = [(1 - eta * (lmbda / n)) * w - (eta / len(mini_batch)) * nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b - (eta / len(mini_batch)) * nb\n",
        "                      for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        delta = self.cost.delta(zs[-1], activations[-1], y)\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].T)\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].T, delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].T)\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "    def accuracy(self, data, convert=False):\n",
        "        if convert:\n",
        "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
        "                       for (x, y) in data]\n",
        "        else:\n",
        "            results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in data]\n",
        "        return sum(int(x == y) for (x, y) in results)\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n"
      ],
      "metadata": {
        "id": "b0kFgZRJpNrX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()\n",
        "\n",
        "# Basic network\n",
        "net = Network([784, 30, 10], cost=CrossEntropyCost)\n",
        "net.large_weight_initializer()\n",
        "net.SGD(training_data, 30, 10, 0.5, evaluation_data=test_data,monitor_evaluation_accuracy=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klLEz7Nqqff1",
        "outputId": "38db2b39-97b2-4c1e-dc24-0fa462a447bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 9125 / 10000\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 9322 / 10000\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 9329 / 10000\n",
            "Epoch 3 training complete\n",
            "Accuracy on evaluation data: 9391 / 10000\n",
            "Epoch 4 training complete\n",
            "Accuracy on evaluation data: 9421 / 10000\n",
            "Epoch 5 training complete\n",
            "Accuracy on evaluation data: 9448 / 10000\n",
            "Epoch 6 training complete\n",
            "Accuracy on evaluation data: 9458 / 10000\n",
            "Epoch 7 training complete\n",
            "Accuracy on evaluation data: 9433 / 10000\n",
            "Epoch 8 training complete\n",
            "Accuracy on evaluation data: 9478 / 10000\n",
            "Epoch 9 training complete\n",
            "Accuracy on evaluation data: 9455 / 10000\n",
            "Epoch 10 training complete\n",
            "Accuracy on evaluation data: 9480 / 10000\n",
            "Epoch 11 training complete\n",
            "Accuracy on evaluation data: 9498 / 10000\n",
            "Epoch 12 training complete\n",
            "Accuracy on evaluation data: 9498 / 10000\n",
            "Epoch 13 training complete\n",
            "Accuracy on evaluation data: 9494 / 10000\n",
            "Epoch 14 training complete\n",
            "Accuracy on evaluation data: 9520 / 10000\n",
            "Epoch 15 training complete\n",
            "Accuracy on evaluation data: 9531 / 10000\n",
            "Epoch 16 training complete\n",
            "Accuracy on evaluation data: 9541 / 10000\n",
            "Epoch 17 training complete\n",
            "Accuracy on evaluation data: 9532 / 10000\n",
            "Epoch 18 training complete\n",
            "Accuracy on evaluation data: 9511 / 10000\n",
            "Epoch 19 training complete\n",
            "Accuracy on evaluation data: 9544 / 10000\n",
            "Epoch 20 training complete\n",
            "Accuracy on evaluation data: 9492 / 10000\n",
            "Epoch 21 training complete\n",
            "Accuracy on evaluation data: 9544 / 10000\n",
            "Epoch 22 training complete\n",
            "Accuracy on evaluation data: 9557 / 10000\n",
            "Epoch 23 training complete\n",
            "Accuracy on evaluation data: 9531 / 10000\n",
            "Epoch 24 training complete\n",
            "Accuracy on evaluation data: 9510 / 10000\n",
            "Epoch 25 training complete\n",
            "Accuracy on evaluation data: 9521 / 10000\n",
            "Epoch 26 training complete\n",
            "Accuracy on evaluation data: 9532 / 10000\n",
            "Epoch 27 training complete\n",
            "Accuracy on evaluation data: 9521 / 10000\n",
            "Epoch 28 training complete\n",
            "Accuracy on evaluation data: 9543 / 10000\n",
            "Epoch 29 training complete\n",
            "Accuracy on evaluation data: 9523 / 10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([],\n",
              " [9125,\n",
              "  9322,\n",
              "  9329,\n",
              "  9391,\n",
              "  9421,\n",
              "  9448,\n",
              "  9458,\n",
              "  9433,\n",
              "  9478,\n",
              "  9455,\n",
              "  9480,\n",
              "  9498,\n",
              "  9498,\n",
              "  9494,\n",
              "  9520,\n",
              "  9531,\n",
              "  9541,\n",
              "  9532,\n",
              "  9511,\n",
              "  9544,\n",
              "  9492,\n",
              "  9544,\n",
              "  9557,\n",
              "  9531,\n",
              "  9510,\n",
              "  9521,\n",
              "  9532,\n",
              "  9521,\n",
              "  9543,\n",
              "  9523],\n",
              " [],\n",
              " [])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LjqeE3waGVza"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}